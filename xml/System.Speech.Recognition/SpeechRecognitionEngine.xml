<Type Name="SpeechRecognitionEngine" FullName="System.Speech.Recognition.SpeechRecognitionEngine">
  <Metadata>
    <Meta Name="ms.openlocfilehash" Value="64d271fac7ee774099403cbb7bec467f30d51b3f" />
    <Meta Name="ms.sourcegitcommit" Value="d31dc2ede16f6f7bc64e90d9f897ff54c4e3869b" />
    <Meta Name="ms.translationtype" Value="HT" />
    <Meta Name="ms.contentlocale" Value="it-IT" />
    <Meta Name="ms.lasthandoff" Value="04/03/2018" />
    <Meta Name="ms.locfileid" Value="30531675" />
  </Metadata>
  <TypeSignature Language="C#" Value="public class SpeechRecognitionEngine : IDisposable" />
  <TypeSignature Language="ILAsm" Value=".class public auto ansi beforefieldinit SpeechRecognitionEngine extends System.Object implements class System.IDisposable" />
  <TypeSignature Language="DocId" Value="T:System.Speech.Recognition.SpeechRecognitionEngine" />
  <TypeSignature Language="VB.NET" Value="Public Class SpeechRecognitionEngine&#xA;Implements IDisposable" />
  <TypeSignature Language="C++ CLI" Value="public ref class SpeechRecognitionEngine : IDisposable" />
  <AssemblyInfo>
    <AssemblyName>System.Speech</AssemblyName>
    <AssemblyVersion>4.0.0.0</AssemblyVersion>
  </AssemblyInfo>
  <Base>
    <BaseTypeName>System.Object</BaseTypeName>
  </Base>
  <Interfaces>
    <Interface>
      <InterfaceName>System.IDisposable</InterfaceName>
    </Interface>
  </Interfaces>
  <Docs>
    <summary>Fornisce i mezzi per accedere e gestire un motore di riconoscimento vocale in-process.</summary>
    <remarks>
      <format type="text/markdown"><![CDATA[  
  
## Remarks  
 È possibile creare un'istanza di questa classe per uno qualsiasi del riconoscimento vocale installata. Per ottenere informazioni sui tipi di riconoscimento vengono installati, usare il metodo statico <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> metodo.  
  
 Questa classe è per l'esecuzione di comandi vocali riconoscimento motori in-process e consente di controllare vari aspetti del riconoscimento vocale, come indicato di seguito:  
  
-   Per creare un riconoscimento vocale in-process, utilizzare uno del <xref:System.Speech.Recognition.SpeechRecognitionEngine.%23ctor%2A> costruttori.  
  
-   Per gestire le grammatiche riconoscimento vocale, utilizzare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar%2A>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars%2A> metodi e <xref:System.Speech.Recognition.SpeechRecognitionEngine.Grammars%2A> proprietà.  
  
-   Per configurare l'input per il riconoscimento, utilizzare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>, o <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A> metodo.  
  
-   Per eseguire il riconoscimento vocale, utilizzare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> metodo.  
  
-   Per modificare la modalità di gestione di input imprevisto o inattività riconoscimento, utilizzare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> proprietà.  
  
-   Per modificare il numero di alternative restituisce il riconoscimento, utilizzare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates%2A> proprietà. Il riconoscimento restituisce i risultati del riconoscimento in un <xref:System.Speech.Recognition.RecognitionResult> oggetto.  
  
-   Per sincronizzare le modifiche al sistema di riconoscimento, utilizzare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> metodo. Il riconoscimento utilizza più di un thread per eseguire attività.  
  
-   Per emulare l'input per il riconoscimento, utilizzare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> e <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> metodi.  
  
 Il <xref:System.Speech.Recognition.SpeechRecognitionEngine> oggetto è l'unico Usa del processo di cui creare un'istanza dell'oggetto. Al contrario, il <xref:System.Speech.Recognition.SpeechRecognizer> condivide un singolo sistema di riconoscimento con qualsiasi applicazione che si desidera utilizzarlo.  
  
> [!NOTE]
>  Chiamare sempre il metodo <xref:System.Speech.Recognition.SpeechRecognitionEngine.Dispose%2A> prima di rilasciare l'ultimo riferimento a riconoscimento vocale. In caso contrario, le risorse utilizzate non vengono liberate finché il garbage collector chiama l'oggetto di riconoscimento `Finalize` metodo.  
  
   
  
## Examples  
 Nell'esempio seguente viene illustrata parte di un'applicazione console che illustra il riconoscimento vocale base. Poiché questo esempio viene utilizzato il `Multiple` modalità del <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> (metodo), esegue il riconoscimento fino a quando non si chiude la finestra della console o interrompere il debug.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
    </remarks>
    <altmember cref="T:System.Speech.Recognition.Grammar" />
    <altmember cref="T:System.Speech.Recognition.SpeechRecognizer" />
  </Docs>
  <Members>
    <MemberGroup MemberName=".ctor">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Inizializza una nuova istanza della classe <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 È possibile costruire un <xref:System.Speech.Recognition.SpeechRecognitionEngine> istanza da uno qualsiasi dei seguenti:  
  
-   Il motore di riconoscimento vocale predefinito per il sistema  
  
-   Un motore di riconoscimento vocale specifico che si specifica il nome  
  
-   Il motore di riconoscimento vocale predefinito per impostazioni locali specificate  
  
-   Un motore di riconoscimento specifico che soddisfa i criteri specificati in un <xref:System.Speech.Recognition.RecognizerInfo> oggetto.  
  
 Prima di iniziarne il riconoscimento vocale ha il riconoscimento, è necessario caricare la grammatica di riconoscimento almeno vocale e configurare l'input per il riconoscimento.  
  
 Per caricare una grammatica, chiamare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> metodo.  
  
 Per configurare l'input audio, usare uno dei metodi seguenti:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor" />
      <MemberSignature Language="VB.NET" Value="Public Sub New ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine();" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters />
      <Docs>
        <summary>Inizializza una nuova istanza della classe <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> utilizzando il riconoscimento vocale predefinito per il sistema.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Prima di iniziarne il riconoscimento vocale ha il riconoscimento vocale, è necessario caricare almeno una grammatica di riconoscimento e configurare l'input per il riconoscimento.  
  
 Per caricare una grammatica, chiamare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> metodo.  
  
 Per configurare l'input audio, usare uno dei metodi seguenti:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (System.Globalization.CultureInfo culture);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(class System.Globalization.CultureInfo culture) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Globalization.CultureInfo)" />
      <MemberSignature Language="VB.NET" Value="Public Sub New (culture As CultureInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine(System::Globalization::CultureInfo ^ culture);" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="culture" Type="System.Globalization.CultureInfo" />
      </Parameters>
      <Docs>
        <param name="culture">Le impostazioni locali che il riconoscimento vocale deve supportare.</param>
        <summary>Inizializza una nuova istanza della classe <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> utilizzando il riconoscimento vocale predefinito per le impostazioni locali specificate.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Microsoft Windows e l'API Speech accettare tutti i codici paese di lingua validi. Per eseguire il riconoscimento vocale utilizzando la lingua specificata nel `CultureInfo` argomento, un motore di riconoscimento vocale che supporta che deve essere installato il codice paese di linguaggio. I riconoscimento vocale fornita con Microsoft Windows 7 funzionano con i seguenti codici di lingua, paese.  
  
-   en-GB. Inglese (Regno Unito)  
  
-   en-US. Inglese (Stati Uniti)  
  
-   de-DE. Tedesco (Germania)  
  
-   es-ES. Spagnolo (Spagna)  
  
-   fr-FR. Francese (Francia)  
  
-   ja-JP. Giapponese (Giappone)  
  
-   zh-CN. Cinese (Cina)  
  
-   zh-TW. Cinese (Taiwan)  
  
 Codici di lingua di due lettere, ad esempio "en", "fr", o "es" sono inoltre consentiti.  
  
 Prima di iniziarne il riconoscimento vocale ha il riconoscimento, è necessario caricare la grammatica di riconoscimento almeno vocale e configurare l'input per il riconoscimento.  
  
 Per caricare una grammatica, chiamare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> metodo.  
  
 Per configurare l'input audio, usare uno dei metodi seguenti:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 Nell'esempio seguente viene illustrata parte di un'applicazione console che illustra il riconoscimento vocale base e Inizializza un riconoscimento vocale per le impostazioni locali en-US.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentException">Nessuno dei sistemi di riconoscimento di input vocali installato supporta le impostazioni locali specificate oppure il parametro <paramref name="culture" /> è la lingua inglese.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="Culture" /> è <see langword="null" />.</exception>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (System.Speech.Recognition.RecognizerInfo recognizerInfo);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(class System.Speech.Recognition.RecognizerInfo recognizerInfo) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine(System::Speech::Recognition::RecognizerInfo ^ recognizerInfo);" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="recognizerInfo" Type="System.Speech.Recognition.RecognizerInfo" />
      </Parameters>
      <Docs>
        <param name="recognizerInfo">Le informazioni per il riconoscimento vocale specifico.</param>
        <summary>Inizializza una nuova istanza dell'oggetto <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />  utilizzando le informazioni disponibili in un oggetto <see cref="T:System.Speech.Recognition.RecognizerInfo" /> per specificare il riconoscimento da utilizzare.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 È possibile creare un'istanza di questa classe per uno qualsiasi del riconoscimento vocale installata. Per ottenere informazioni sui tipi di riconoscimento vengono installati, usare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> metodo.  
  
 Prima di iniziarne il riconoscimento vocale ha il riconoscimento, è necessario caricare la grammatica di riconoscimento almeno vocale e configurare l'input per il riconoscimento.  
  
 Per caricare una grammatica, chiamare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> metodo.  
  
 Per configurare l'input audio, usare uno dei metodi seguenti:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 Nell'esempio seguente viene illustrata parte di un'applicazione console che illustra il riconoscimento vocale base e Inizializza un riconoscimento vocale che supporta la lingua inglese.  
  
```csharp  
 using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Select a speech recognizer that supports English.  
      RecognizerInfo info = null;  
      foreach (RecognizerInfo ri in SpeechRecognitionEngine.InstalledRecognizers())  
      {  
        if (ri.Culture.TwoLetterISOLanguageName.Equals("en"))  
        {  
          info = ri;  
          break;  
        }  
      }  
      if (info == null) return;  
  
      // Create the selected recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(info))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (string recognizerId);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(string recognizerId) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub New (recognizerId As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine(System::String ^ recognizerId);" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="recognizerId" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="recognizerId">Il nome del token del riconoscimento vocale da utilizzare.</param>
        <summary>Inizializza una nuova istanza della classe <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> con una parametro di stringa che specifica il nome del riconoscimento da utilizzare.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il nome del token di riconoscimento è il valore della <xref:System.Speech.Recognition.RecognizerInfo.Id%2A> proprietà del <xref:System.Speech.Recognition.RecognizerInfo> oggetto restituito dal <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo%2A> proprietà di riconoscimento. Per ottenere una raccolta di tutti i riconoscitori installati, usare il metodo statico <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> metodo.  
  
 Prima di iniziarne il riconoscimento vocale ha il riconoscimento, è necessario caricare la grammatica di riconoscimento almeno vocale e configurare l'input per il riconoscimento.  
  
 Per caricare una grammatica, chiamare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> metodo.  
  
 Per configurare l'input audio, usare uno dei metodi seguenti:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 Nell'esempio seguente viene illustrata parte di un'applicazione console che illustra il riconoscimento vocale base e crea un'istanza di 8.0 di riconoscimento vocale per Windows (in lingua inglese - Stati Uniti).  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an instance of the Microsoft Speech Recognizer 8.0 for  
      // Windows (English - US).  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine("MS-1033-80-DESK"))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized += new EventHandler(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentException">Non è installato alcun riconoscimento di input vocale con quel nome di token oppure il parametro <paramref name="recognizerId" /> è la stringa vuota ("").</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="recognizerId" /> è <see langword="null" />.</exception>
      </Docs>
    </Member>
    <Member MemberName="AudioFormat">
      <MemberSignature Language="C#" Value="public System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioFormat As SpeechAudioFormatInfo" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::AudioFormat::SpeechAudioFormatInfo ^ AudioFormat { System::Speech::AudioFormat::SpeechAudioFormatInfo ^ get(); };" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.AudioFormat.SpeechAudioFormatInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ottiene il formato dell'audio ricevuto da <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>Il formato audio dell'input all'istanza di <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> o <see langword="null" /> se l'input non è configurato o non impostato su input null.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Per configurare l'input audio, usare uno dei metodi seguenti:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 Nell'esempio seguente viene utilizzato <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat%2A> per ottenere e visualizzare i dati in formato audio.  
  
```  
static void DisplayAudioDeviceFormat(Label label, SpeechRecognitionEngine recognitionEngine)   
{  
  
  if (recognitionEngine != null && label != null)   
  {  
    label.Text = String.Format("Encoding Format:         {0}\n" +  
          "AverageBytesPerSecond    {1}\n" +  
          "BitsPerSample            {2}\n" +  
          "BlockAlign               {3}\n" +  
          "ChannelCount             {4}\n" +  
          "SamplesPerSecond         {5}",  
          recognitionEngine.AudioFormat.EncodingFormat.ToString(),  
          recognitionEngine.AudioFormat.AverageBytesPerSecond,  
          recognitionEngine.AudioFormat.BitsPerSample,  
          recognitionEngine.AudioFormat.BlockAlign,  
          recognitionEngine.AudioFormat.ChannelCount,  
          recognitionEngine.AudioFormat.SamplesPerSecond);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.AudioFormat.SpeechAudioFormatInfo" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioFormat" />
      </Docs>
    </Member>
    <Member MemberName="AudioLevel">
      <MemberSignature Language="C#" Value="public int AudioLevel { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance int32 AudioLevel" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioLevel As Integer" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property int AudioLevel { int get(); };" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Int32</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ottiene il livello dell'audio ricevuto da <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>Livello audio dell'input del riconoscimento vocale, da 0 a 100.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il valore 0 rappresenta inattività 100 rappresenta il volume di input massimo.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioLevel" />
      </Docs>
    </Member>
    <Member MemberName="AudioLevelUpdated">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; AudioLevelUpdated;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; AudioLevelUpdated" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioLevelUpdated As EventHandler(Of AudioLevelUpdatedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioLevelUpdatedEventArgs ^&gt; ^ AudioLevelUpdated;" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Generato quando <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> segnala il livello del relativo input audio.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il <xref:System.Speech.Recognition.SpeechRecognitionEngine> genera questo evento più volte al secondo. La frequenza con cui l'evento viene generato varia a seconda del computer in cui è in esecuzione l'applicazione.  
  
 Per ottenere il livello audio al momento dell'evento, utilizzare il <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs.AudioLevel%2A> proprietà dell'oggetto associato <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs>. Per ottenere il livello corrente audio di input per il riconoscimento, usare il riconoscimento <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel%2A> proprietà.  
  
 Quando si crea un delegato di <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated>, si identifica il metodo con cui gestire l'evento. Per associare l'evento al gestore eventi in uso, aggiungere all'evento un'istanza del delegato. Il gestore eventi viene chiamato ogni volta che si verifica l'evento, a meno che non venga rimosso il delegato. Per ulteriori informazioni sui delegati del gestore eventi, vedere [eventi e delegati](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Nell'esempio seguente aggiunge un gestore per il <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated> evento da un <xref:System.Speech.Recognition.SpeechRecognitionEngine> oggetto. Il gestore restituisce il nuovo livello audio nella console.  
  
```  
private SpeechRecognitionEngine recognizer;  
  
// Initialize the SpeechRecognitionEngine object.   
private void Initialize()  
{  
  recognizer = new SpeechRecognitionEngine();  
  
  // Add an event handler for the AudioLevelUpdated event.  
  recognizer.AudioLevelUpdated +=   
   new EventHandler<AudioLevelUpdatedEventArgs>(recognizer_AudioLevelUpdated);  
  
  // Add other initialization code here.  
  
}  
  
// Write the audio level to the console when the AudioLevelUpdated event is raised.  
void recognizer_AudioLevelUpdated(object sender, AudioLevelUpdatedEventArgs e)  
{  
  Console.WriteLine("The audio level is now: {0}.", e.AudioLevel);  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioLevelUpdatedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel" />
      </Docs>
    </Member>
    <Member MemberName="AudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan AudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan AudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioPosition As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan AudioPosition { TimeSpan get(); };" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ottiene la posizione corrente nel flusso audio generato dal dispositivo che fornisce l'input a <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>La posizione corrente nel flusso audio generato dal dispositivo di input.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> proprietà fa riferimento la posizione del dispositivo di input nel proprio flusso audio generato. Al contrario, il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> proprietà fa riferimento la posizione del riconoscimento all'interno di input audio. Queste posizioni possono essere diverse. Ad esempio, se ha ricevuto il riconoscimento di input per i quali non ha ancora generato un risultato di riconoscimento, il valore del <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> proprietà è minore del valore del <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> proprietà.  
  
   
  
## Examples  
 Nell'esempio seguente, il riconoscimento vocale in-process Usa una grammatica dettatura per associare input vocale. Un gestore per il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> evento scrive nella console di <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel%2A> quando il riconoscimento vocale ha rilevato vocale relativi input.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine for US English.  
      using (recognizer = new SpeechRecognitionEngine(  
        new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create a grammar for finding services in different cities.  
        Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
        Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
        GrammarBuilder findServices = new GrammarBuilder("Find");  
        findServices.Append(services);  
        findServices.Append("near");  
        findServices.Append(cities);  
  
        // Create a Grammar object from the GrammarBuilder and load it to the recognizer.  
        Grammar servicesGrammar = new Grammar(findServices);  
        recognizer.LoadGrammarAsync(servicesGrammar);  
  
        // Add handlers for events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  
  
        // Start asynchronous recognition.  
        recognizer.RecognizeAsync();  
        Console.WriteLine("Starting asynchronous recognition...");  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Gather information about detected speech and write it to the console.  
    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Speech detected:");  
      Console.WriteLine("  Audio level: " + recognizer.AudioLevel);  
      Console.WriteLine("  Audio position at the event: " + e.AudioPosition);  
      Console.WriteLine("  Current audio position: " + recognizer.AudioPosition);  
      Console.WriteLine("  Current recognizer audio position: " +   
        recognizer.RecognizerAudioPosition);  
    }  
  
    // Write the text of the recognition result to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("\nSpeech recognized: " + e.Result.Text);  
  
      // Add event handler code here.  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="AudioSignalProblemOccurred">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; AudioSignalProblemOccurred;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; AudioSignalProblemOccurred" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioSignalProblemOccurred As EventHandler(Of AudioSignalProblemOccurredEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioSignalProblemOccurredEventArgs ^&gt; ^ AudioSignalProblemOccurred;" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Generato quando <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> rileva un problema nel segnale audio.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Per ottenere si è verificato il problema, utilizzare il <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs.AudioSignalProblem%2A> proprietà dell'oggetto associato <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs>.  
  
 Quando si crea un delegato di <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred>, si identifica il metodo con cui gestire l'evento. Per associare l'evento al gestore eventi in uso, aggiungere all'evento un'istanza del delegato. Il gestore eventi viene chiamato ogni volta che si verifica l'evento, a meno che non venga rimosso il delegato. Per ulteriori informazioni sui delegati del gestore eventi, vedere [eventi e delegati](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 L'esempio seguente definisce un gestore eventi che raccoglie informazioni su un <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred> evento.  
  
```  
private SpeechRecognitionEngine recognizer;  
  
// Initialize the speech recognition engine.  
private void Initialize()  
{  
  recognizer = new SpeechRecognitionEngine();  
  
  // Add a handler for the AudioSignalProblemOccurred event.  
  recognizer.AudioSignalProblemOccurred +=   
    new EventHandler<AudioSignalProblemOccurredEventArgs>(  
      recognizer_AudioSignalProblemOccurred);  
}  
  
// Gather information when the AudioSignalProblemOccurred event is raised.  
void recognizer_AudioSignalProblemOccurred(object sender, AudioSignalProblemOccurredEventArgs e)  
{  
  StringBuilder details = new StringBuilder();  
  
  details.AppendLine("Audio signal problem information:");  
  details.AppendFormat(  
    " Audio level:               {0}" + Environment.NewLine +  
    " Audio position:            {1}" + Environment.NewLine +  
    " Audio signal problem:      {2}" + Environment.NewLine +  
    " Recognition engine audio position: {3}" + Environment.NewLine,  
    e.AudioLevel, e.AudioPosition,  e.AudioSignalProblem,  
    e.recoEngineAudioPosition);  
  
  // Insert additional event handler code here.  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioSignalProblem" />
        <altmember cref="T:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="AudioState">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.AudioState AudioState { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.Speech.Recognition.AudioState AudioState" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioState" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioState As AudioState" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::AudioState AudioState { System::Speech::Recognition::AudioState get(); };" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.AudioState</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ottiene lo stato dell'audio ricevuto da <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>Lo stato dell'input audio per il riconoscimento vocale.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> proprietà rappresenta lo stato audio con un membro con il <xref:System.Speech.Recognition.AudioState> enumerazione.  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioState" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioState" />
      </Docs>
    </Member>
    <Member MemberName="AudioStateChanged">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; AudioStateChanged;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioStateChangedEventArgs&gt; AudioStateChanged" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioStateChanged As EventHandler(Of AudioStateChangedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioStateChangedEventArgs ^&gt; ^ AudioStateChanged;" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Generato in seguito alla modifica dello stato nell'audio ricevuto da <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Per ottenere lo stato audio al momento dell'evento, utilizzare il <xref:System.Speech.Recognition.AudioStateChangedEventArgs.AudioState%2A> proprietà dell'oggetto associato <xref:System.Speech.Recognition.AudioStateChangedEventArgs>. Per ottenere lo stato corrente di audio di input per il riconoscimento, usare il riconoscimento <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> proprietà. Per ulteriori informazioni sullo stato audio, vedere il <xref:System.Speech.Recognition.AudioState> enumerazione.  
  
 Quando si crea un delegato di <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged>, si identifica il metodo con cui gestire l'evento. Per associare l'evento al gestore eventi in uso, aggiungere all'evento un'istanza del delegato. Il gestore eventi viene chiamato ogni volta che si verifica l'evento, a meno che non venga rimosso il delegato. Per ulteriori informazioni sui delegati del gestore eventi, vedere [eventi e delegati](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Nell'esempio seguente viene utilizzato un gestore per il <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> evento da scrivere il riconoscimento del nuovo <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> nella console ogni volta che le modifiche, l'utilizzo di un membro del <xref:System.Speech.Recognition.AudioState> enumerazione.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a grammar.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder("On this farm he had a");  
        farm.Append(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Attach event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(recognizer_AudioStateChanged);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine();  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
        Console.WriteLine();  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Done.");  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the AudioStateChanged event.  
    static void recognizer_AudioStateChanged(object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("The new audio state is: " + e.AudioState);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioState" />
        <altmember cref="T:System.Speech.Recognition.AudioStateChangedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioState" />
      </Docs>
    </Member>
    <Member MemberName="BabbleTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan BabbleTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan BabbleTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
      <MemberSignature Language="VB.NET" Value="Public Property BabbleTimeout As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan BabbleTimeout { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ottiene o imposta l'intervallo di tempo durante il quale <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> accetta input contenente solo il rumore di fondo, prima di completare il riconoscimento.</summary>
        <value>La durata dell'intervallo di tempo.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ogni riconoscimento vocale è disponibile un algoritmo per distinguere tra vocale e di inattività. Il riconoscimento classifica come qualsiasi inattività-non di input che non corrispondono alla regola iniziale di uno qualsiasi del riconoscimento del rumore caricati e abilitato le grammatiche riconoscimento vocale. Se il riconoscimento riceve solo rumore di fondo e inattività entro l'intervallo di timeout babble, il riconoscimento completa l'operazione di riconoscimento.  
  
-   Per le operazioni di riconoscimento asincrona, genera il riconoscimento di <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> evento, in cui il <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.BabbleTimeout%2A?displayProperty=nameWithType> proprietà `true`e il <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A?displayProperty=nameWithType> proprietà è `null`.  
  
-   Per le operazioni di riconoscimento sincrono e l'emulazione, restituisce il riconoscimento `null`, anziché un valore valido <xref:System.Speech.Recognition.RecognitionResult>.  
  
 Se il periodo di timeout babble è impostato su 0, il riconoscimento non in grado di eseguire un controllo di timeout babble. L'intervallo di timeout può essere qualsiasi valore non negativo. Il valore predefinito è 0 secondi.  
  
   
  
## Examples  
 Nell'esempio seguente viene illustrata parte di un'applicazione console che illustra il riconoscimento vocale base che imposta il <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> e <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> le proprietà di un <xref:System.Speech.Recognition.SpeechRecognitionEngine> prima di avviare il riconoscimento vocale. Gestori per il riconoscimento vocale <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> e <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> gli eventi di output delle informazioni di evento nella console per dimostrare come <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> le proprietà di un <xref:System.Speech.Recognition.SpeechRecognitionEngine> influiscono sulle operazioni di riconoscimento.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Load a Grammar object.  
        recognizer.LoadGrammar(CreateServicesGrammar("FindServices"));  
  
        // Add event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(  
            AudioStateChangedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(3);  
        recognizer.BabbleTimeout = TimeSpan.FromSeconds(2);  
        recognizer.EndSilenceTimeout = TimeSpan.FromSeconds(1);  
        recognizer.EndSilenceTimeoutAmbiguous = TimeSpan.FromSeconds(1.5);  
  
        Console.WriteLine("BabbleTimeout: {0}", recognizer.BabbleTimeout);  
        Console.WriteLine("InitialSilenceTimeout: {0}", recognizer.InitialSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeout: {0}", recognizer.EndSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeoutAmbiguous: {0}", recognizer.EndSilenceTimeoutAmbiguous);  
        Console.WriteLine();  
  
        // Start asynchronous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Single);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Create a grammar and build it into a Grammar object.   
    static Grammar CreateServicesGrammar(string grammarName)  
    {  
  
      // Create a grammar for finding services in different cities.  
      Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
      Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
      GrammarBuilder findServices = new GrammarBuilder("Find");  
      findServices.Append(services);  
      findServices.Append("near");  
      findServices.Append(cities);  
  
      // Create a Grammar object from the GrammarBuilder..  
      Grammar servicesGrammar = new Grammar(findServices);  
      servicesGrammar.Name = ("FindServices");  
      return servicesGrammar;  
    }  
  
    // Handle the AudioStateChanged event.  
    static void AudioStateChangedHandler(  
      object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("AudioStateChanged ({0}): {1}",  
        DateTime.Now.ToString("mm:ss.f"), e.AudioState);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("RecognizeCompleted ({0}):",  
        DateTime.Now.ToString("mm:ss.f"));  
  
      string resultText;  
      if (e.Result != null) { resultText = e.Result.Text; }  
      else { resultText = "<null>"; }  
  
      Console.WriteLine(  
        " BabbleTimeout: {0}; InitialSilenceTimeout: {1}; Result text: {2}",  
        e.BabbleTimeout, e.InitialSilenceTimeout, resultText);  
      if (e.Error != null)  
      {  
        Console.WriteLine(" Exception message: ", e.Error.Message);  
      }  
  
      // Start the next asynchronous recognition operation.  
      ((SpeechRecognitionEngine)sender).RecognizeAsync(RecognizeMode.Single);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Questa proprietà è impostata su un valore minore di 0 secondi.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <MemberGroup MemberName="Dispose">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Elimina l'oggetto <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
      </Docs>
    </MemberGroup>
    <Member MemberName="Dispose">
      <MemberSignature Language="C#" Value="public void Dispose ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig newslot virtual instance void Dispose() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Dispose" />
      <MemberSignature Language="VB.NET" Value="Public Sub Dispose ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; virtual void Dispose();" />
      <MemberType>Method</MemberType>
      <Implements>
        <InterfaceMember>M:System.IDisposable.Dispose</InterfaceMember>
      </Implements>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Elimina l'oggetto <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <Member MemberName="Dispose">
      <MemberSignature Language="C#" Value="protected virtual void Dispose (bool disposing);" />
      <MemberSignature Language="ILAsm" Value=".method familyhidebysig newslot virtual instance void Dispose(bool disposing) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Dispose(System.Boolean)" />
      <MemberSignature Language="VB.NET" Value="Protected Overridable Sub Dispose (disposing As Boolean)" />
      <MemberSignature Language="C++ CLI" Value="protected:&#xA; virtual void Dispose(bool disposing);" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="disposing" Type="System.Boolean" />
      </Parameters>
      <Docs>
        <param name="disposing">
          <see langword="true" /> per rilasciare sia le risorse gestite sia quelle non gestite; <see langword="false" /> per rilasciare solo le risorse non gestite.</param>
        <summary>Elimina l'oggetto <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> e rilascia le risorse usate durante la sessione.</summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <MemberGroup MemberName="EmulateRecognize">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Emula l'input al riconoscimento vocale, utilizzando il testo anziché l'audio per il riconoscimento vocale sincrono.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Questi metodi di ignorare l'input audio di sistema e fornire il testo per il riconoscimento come <xref:System.String> oggetti o come una matrice di <xref:System.Speech.Recognition.RecognizedWordUnit> oggetti. Può essere utile quando si desidera testare o il debug di un'applicazione o grammatica. Ad esempio, è possibile utilizzare l'emulazione per determinare se una parola in una grammatica e quali semantica viene restituita quando la parola viene riconosciuta. Utilizzare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A> per disattivare l'input audio per il motore di riconoscimento vocale durante le operazioni di emulazione.  
  
 Il controllo di riconoscimento vocale genera il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventi come se l'operazione di riconoscimento non è emulata. Il riconoscimento delle nuove righe e gli spazi vuoti aggiuntivi vengono ignorati e considera i segni di punteggiatura come valore letterale input.  
  
> [!NOTE]
>  Il <xref:System.Speech.Recognition.RecognitionResult> oggetto generato dal riconoscimento vocale in risposta all'input emulata ha un valore di `null` per relativo <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> proprietà.  
  
 Per emulare un riconoscimento asincrono, utilizzare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> metodo.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(string inputText) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Function EmulateRecognize (inputText As String) As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(System::String ^ inputText);" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="inputText">Input per l'operazione di riconoscimento.</param>
        <summary>Emula l'input di una frase al riconoscimento vocale, utilizzando il testo anziché l'audio per il riconoscimento vocale sincrono.</summary>
        <returns>Il risultato dell'operazione di riconoscimento o <see langword="null" /> se l'operazione non riesce o il riconoscimento non è abilitato.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il controllo di riconoscimento vocale genera il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventi come se l'operazione di riconoscimento non è emulata.  
  
 Il riconoscimento forniti con Vista e Windows 7 Ignora maiuscole / minuscole e caratteri di larghezza quando l'applicazione delle regole di sintassi per la frase di input. Per ulteriori informazioni su questo tipo di confronto, vedere il <xref:System.Globalization.CompareOptions> valori di enumerazione <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> e <xref:System.Globalization.CompareOptions.IgnoreWidth>. Il riconoscimento anche Ignora le nuove righe e lo spazio vuoto aggiuntivo e considera la punteggiatura come input letterale.  
  
   
  
## Examples  
 Esempio di codice seguente fa parte di un'applicazione console in cui viene emulato input, i risultati di riconoscimento associati e gli eventi associati generati dal riconoscimento vocale. L'esempio genera l'output seguente.  
  
```  
TestRecognize("Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = Smith  
...Recognition result text = Smith  
  
TestRecognize("Jones")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Jones; Text = Jones  
...Recognition result text = Jones  
  
TestRecognize("Mister")...  
 SpeechDetected event raised.  
 SpeechHypothesized event raised.  
  Grammar = Smith; Text = mister  
 SpeechRecognitionRejected event raised.  
  Grammar = <not available>; Text =  
...No recognition result.  
  
TestRecognize("Mister Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = mister Smith  
...Recognition result text = mister Smith  
  
press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace Sre_EmulateRecognize  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Load grammars.  
        recognizer.LoadGrammar(CreateNameGrammar("Smith"));  
        recognizer.LoadGrammar(CreateNameGrammar("Jones"));  
  
        // Disable audio input to the recognizer.  
        recognizer.SetInputToNull();  
  
        // Add handlers for events raised by the EmulateRecognize method.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
  
        // Start four synchronous emulated recognition operations.  
        TestRecognize(recognizer, "Smith");  
        TestRecognize(recognizer, "Jones");  
        TestRecognize(recognizer, "Mister");  
        TestRecognize(recognizer, "Mister Smith");  
      }  
  
      Console.WriteLine("press any key to exit...");  
      Console.ReadKey(true);  
    }  
  
    // Create a simple name grammar.  
    // Set the grammar name to the surname.  
    private static Grammar CreateNameGrammar(string surname)  
    {  
      GrammarBuilder builder = new GrammarBuilder("mister", 0, 1);  
      builder.Append(surname);  
  
      Grammar nameGrammar = new Grammar(builder);  
      nameGrammar.Name = surname;  
  
      return nameGrammar;  
    }  
  
    // Send emulated input to the recognizer for synchronous recognition.  
    private static void TestRecognize(  
      SpeechRecognitionEngine recognizer, string input)  
    {  
      Console.WriteLine("TestRecognize(\"{0}\")...", input);  
      RecognitionResult result =  
        recognizer.EmulateRecognize(input,CompareOptions.IgnoreCase);  
      if (result != null)  
      {  
        Console.WriteLine("...Recognition result text = {0}",  
          result.Text ?? "<null>");  
      }  
      else  
      {  
        Console.WriteLine("...No recognition result.");  
      }  
      Console.WriteLine();  
    }  
  
    static void SpeechDetectedHandler(  
      object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechDetected event raised.");  
    }  
  
    // Handle events.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechHypothesized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognitionRejected event raised.");  
      if (e.Result != null)  
      {  
        string grammarName;  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name ?? "<none>";  
        }  
        else  
        {  
          grammarName = "<not available>";  
        }  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          grammarName, e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Nel riconoscimento non sono caricate grammatiche di riconoscimento vocale.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="inputText" /> è <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="inputText" /> è la stringa vuota ("").</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(class System.Speech.Recognition.RecognizedWordUnit[] wordUnits, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(cli::array &lt;System::Speech::Recognition::RecognizedWordUnit ^&gt; ^ wordUnits, System::Globalization::CompareOptions compareOptions);" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="wordUnits" Type="System.Speech.Recognition.RecognizedWordUnit[]" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="wordUnits">Matrice di unità di parole che contiene l'input per l'operazione di riconoscimento.</param>
        <param name="compareOptions">Combinazione bit per bit dei valori di enumerazione che descrivono il tipo di confronto da utilizzare per l'operazione di riconoscimento emulato.</param>
        <summary>Emula l'input di parole specifiche al riconoscimento vocale, utilizzando il testo anziché l'audio per il riconoscimento vocale sincrono, e specifica come il riconoscimento gestisce il confronto Unicode tra le parole e le grammatiche di riconoscimento vocale caricate.</summary>
        <returns>Il risultato dell'operazione di riconoscimento o <see langword="null" /> se l'operazione non riesce o il riconoscimento non è abilitato.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il controllo di riconoscimento vocale genera il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventi come se l'operazione di riconoscimento non è emulata.  
  
 Il riconoscimento utilizza `compareOptions` quando si applica regole grammaticali per la frase di input. Il riconoscimento forniti con Vista e Windows 7 Ignora maiuscole / minuscole se il <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> o <xref:System.Globalization.CompareOptions.IgnoreCase> valore è presente. Il riconoscimento Ignora sempre la larghezza del carattere e mai Ignora Katakana / Hiragana. Il riconoscimento inoltre nuove righe e gli spazi vuoti aggiuntivi vengono ignorati e considera i segni di punteggiatura come input letterale. Per ulteriori informazioni sulla larghezza del carattere e il tipo Kana, vedere il <xref:System.Globalization.CompareOptions> enumerazione.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Nel riconoscimento non sono caricate grammatiche di riconoscimento vocale.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="wordUnits" /> è <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="wordUnits" /> contiene uno o più elementi <see langword="null" />.</exception>
        <exception cref="T:System.NotSupportedException">
          <paramref name="compareOptions" /> contiene il flag <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" /> o <see cref="F:System.Globalization.CompareOptions.StringSort" />.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(string inputText, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String,System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(System::String ^ inputText, System::Globalization::CompareOptions compareOptions);" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="inputText">Frase di Input per l'operazione di riconoscimento.</param>
        <param name="compareOptions">Combinazione bit per bit dei valori di enumerazione che descrivono il tipo di confronto da utilizzare per l'operazione di riconoscimento emulato.</param>
        <summary>Emula l'input di una frase al riconoscimento vocale, utilizzando il testo anziché l'audio per il riconoscimento vocale sincrono, e specifica come il riconoscimento gestisce il confronto Unicode tra la frase e le grammatiche di riconoscimento vocale caricate.</summary>
        <returns>Il risultato dell'operazione di riconoscimento o <see langword="null" /> se l'operazione non riesce o il riconoscimento non è abilitato.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il controllo di riconoscimento vocale genera il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventi come se l'operazione di riconoscimento non è emulata.  
  
 Il riconoscimento utilizza `compareOptions` quando si applica regole grammaticali per la frase di input. Il riconoscimento forniti con Vista e Windows 7 Ignora maiuscole / minuscole se il <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> o <xref:System.Globalization.CompareOptions.IgnoreCase> valore è presente. Il riconoscimento Ignora sempre la larghezza del carattere e mai Ignora Katakana / Hiragana. Il riconoscimento inoltre nuove righe e gli spazi vuoti aggiuntivi vengono ignorati e considera i segni di punteggiatura come input letterale. Per ulteriori informazioni sulla larghezza del carattere e il tipo Kana, vedere il <xref:System.Globalization.CompareOptions> enumerazione.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Nel riconoscimento non sono caricate grammatiche di riconoscimento vocale.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="inputText" /> è <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="inputText" /> è la stringa vuota ("").</exception>
        <exception cref="T:System.NotSupportedException">
          <paramref name="compareOptions" /> contiene il flag <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" /> o <see cref="F:System.Globalization.CompareOptions.StringSort" />.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <MemberGroup MemberName="EmulateRecognizeAsync">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Emula l'input al riconoscimento vocale, utilizzando il testo anziché l'audio per il riconoscimento vocale asincrono.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Questi metodi di ignorare l'input audio di sistema e fornire il testo per il riconoscimento come <xref:System.String> oggetti o come una matrice di <xref:System.Speech.Recognition.RecognizedWordUnit> oggetti. Può essere utile quando si desidera testare o il debug di un'applicazione o grammatica. Ad esempio, è possibile utilizzare l'emulazione per determinare se una parola in una grammatica e quali semantica viene restituita quando la parola viene riconosciuta. Utilizzare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A> per disattivare l'input audio per il motore di riconoscimento vocale durante le operazioni di emulazione.  
  
 Il controllo di riconoscimento vocale genera il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventi come se l'operazione di riconoscimento non è emulata. Al termine dell'operazione di riconoscimento asincrono il riconoscimento, genera il <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> evento. Il riconoscimento delle nuove righe e gli spazi vuoti aggiuntivi vengono ignorati e considera i segni di punteggiatura come valore letterale input.  
  
> [!NOTE]
>  Il <xref:System.Speech.Recognition.RecognitionResult> oggetto generato dal riconoscimento vocale in risposta all'input emulata ha un valore di `null` per relativo <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> proprietà.  
  
 Per emulare un riconoscimento sincrono, utilizzare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> metodo.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (string inputText);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(string inputText) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub EmulateRecognizeAsync (inputText As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(System::String ^ inputText);" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="inputText">Input per l'operazione di riconoscimento.</param>
        <summary>Emula l'input di una frase al riconoscimento vocale, utilizzando il testo anziché l'audio per il riconoscimento vocale asincrono.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il controllo di riconoscimento vocale genera il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventi come se l'operazione di riconoscimento non è emulata. Al termine dell'operazione di riconoscimento asincrono il riconoscimento, genera il <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> evento.  
  
 Il riconoscimento forniti con Vista e Windows 7 Ignora maiuscole / minuscole e caratteri di larghezza quando l'applicazione delle regole di sintassi per la frase di input. Per ulteriori informazioni su questo tipo di confronto, vedere il <xref:System.Globalization.CompareOptions> valori di enumerazione <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> e <xref:System.Globalization.CompareOptions.IgnoreWidth>. Il riconoscimento anche Ignora le nuove righe e lo spazio vuoto aggiuntivo e considera la punteggiatura come input letterale.  
  
   
  
## Examples  
 Esempio di codice seguente fa parte di un'applicazione console che illustra l'input emulata asincrona, i risultati di riconoscimento associati e gli eventi associati generati dal riconoscimento vocale. L'esempio genera l'output seguente.  
  
```  
  
TestRecognizeAsync("Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = Smith  
 EmulateRecognizeCompleted event raised.  
  Grammar = Smith; Text = Smith  
 Done.  
  
TestRecognizeAsync("Jones")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Jones; Text = Jones  
 EmulateRecognizeCompleted event raised.  
  Grammar = Jones; Text = Jones  
 Done.  
  
TestRecognizeAsync("Mister")...  
 SpeechDetected event raised.  
 SpeechHypothesized event raised.  
  Grammar = Smith; Text = mister  
 SpeechRecognitionRejected event raised.  
  Grammar = <not available>; Text =  
 EmulateRecognizeCompleted event raised.  
  No recognition result available.  
 Done.  
  
TestRecognizeAsync("Mister Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = mister Smith  
 EmulateRecognizeCompleted event raised.  
  Grammar = Smith; Text = mister Smith  
 Done.  
  
press any key to exit...  
```  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SreEmulateRecognizeAsync  
{  
  class Program  
  {  
    // Indicate when an asynchronous operation is finished.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Load grammars.  
        recognizer.LoadGrammar(CreateNameGrammar("Smith"));  
        recognizer.LoadGrammar(CreateNameGrammar("Jones"));  
  
        // Configure the audio input.  
        recognizer.SetInputToNull();  
  
        // Add event handlers for the events raised by the  
        // EmulateRecognizeAsync method.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHander);  
  
        // Start four asynchronous emulated recognition operations.  
        TestRecognizeAsync(recognizer, "Smith");  
        TestRecognizeAsync(recognizer, "Jones");  
        TestRecognizeAsync(recognizer, "Mister");  
        TestRecognizeAsync(recognizer, "Mister Smith");  
      }  
  
      Console.WriteLine("press any key to exit...");  
      Console.ReadKey(true);  
    }  
  
    // Create a simple name grammar.  
    // Set the grammar name to the surname.  
    private static Grammar CreateNameGrammar(string surname)  
    {  
      GrammarBuilder builder = new GrammarBuilder("mister", 0, 1);  
      builder.Append(surname);  
  
      Grammar nameGrammar = new Grammar(builder);  
      nameGrammar.Name = surname;  
  
      return nameGrammar;  
    }  
  
    // Send emulated input to the recognizer for asynchronous  
    // recognition.  
    private static void TestRecognizeAsync(  
      SpeechRecognitionEngine recognizer, string input)  
    {  
      completed = false;  
  
      Console.WriteLine("TestRecognizeAsync(\"{0}\")...", input);  
      recognizer.EmulateRecognizeAsync(input);  
  
      // Wait for the operation to complete.  
      while (!completed)  
      {  
        Thread.Sleep(333);  
      }  
  
      Console.WriteLine(" Done.");  
      Console.WriteLine();  
    }  
  
    static void SpeechDetectedHandler(  
      object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechDetected event raised.");  
    }  
  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechHypothesized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    // Handle events.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognitionRejected event raised.");  
      if (e.Result != null)  
      {  
        string grammarName;  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name ?? "<none>";  
        }  
        else  
        {  
          grammarName = "<not available>";  
        }  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          grammarName, e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text );  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void EmulateRecognizeCompletedHander(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" EmulateRecognizeCompleted event raised.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine("  {0} exception encountered: {1}:",  
          e.Error.GetType().Name, e.Error.Message);  
      }  
      else if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      else if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Nel riconoscimento non sono caricate grammatiche di riconoscimento vocale, oppure ci sono operazioni di riconoscimento asincrone non ancora complete.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="inputText" /> è <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="inputText" /> è la stringa vuota ("").</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(class System.Speech.Recognition.RecognizedWordUnit[] wordUnits, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(cli::array &lt;System::Speech::Recognition::RecognizedWordUnit ^&gt; ^ wordUnits, System::Globalization::CompareOptions compareOptions);" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="wordUnits" Type="System.Speech.Recognition.RecognizedWordUnit[]" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="wordUnits">Matrice di unità di parole che contiene l'input per l'operazione di riconoscimento.</param>
        <param name="compareOptions">Combinazione bit per bit dei valori di enumerazione che descrivono il tipo di confronto da utilizzare per l'operazione di riconoscimento emulato.</param>
        <summary>Emula l'input di parole specifiche al riconoscimento vocale, utilizzando una matrice di oggetti <see cref="T:System.Speech.Recognition.RecognizedWordUnit" /> anziché l'audio per il riconoscimento vocale asincrono, e specifica come il riconoscimento gestisce il confronto Unicode tra le parole e le grammatiche di riconoscimento vocale caricate.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il controllo di riconoscimento vocale genera il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventi come se l'operazione di riconoscimento non è emulata. Al termine dell'operazione di riconoscimento asincrono il riconoscimento, genera il <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> evento.  
  
 Il riconoscimento utilizza `compareOptions` quando si applica regole grammaticali per la frase di input. Il riconoscimento forniti con Vista e Windows 7 Ignora maiuscole / minuscole se il <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> o <xref:System.Globalization.CompareOptions.IgnoreCase> valore è presente. Il riconoscimento Ignora sempre la larghezza del carattere e mai Ignora Katakana / Hiragana. Il riconoscimento anche Ignora le nuove righe e lo spazio vuoto aggiuntivo e considera la punteggiatura come input letterale. Per ulteriori informazioni sulla larghezza del carattere e il tipo Kana, vedere il <xref:System.Globalization.CompareOptions> enumerazione.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Nel riconoscimento non sono caricate grammatiche di riconoscimento vocale, oppure ci sono operazioni di riconoscimento asincrone non ancora complete.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="wordUnits" /> è <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="wordUnits" /> contiene uno o più elementi <see langword="null" />.</exception>
        <exception cref="T:System.NotSupportedException">
          <paramref name="compareOptions" /> contiene il flag <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" /> o <see cref="F:System.Globalization.CompareOptions.StringSort" />.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (string inputText, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(string inputText, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String,System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(System::String ^ inputText, System::Globalization::CompareOptions compareOptions);" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="inputText">Frase di Input per l'operazione di riconoscimento.</param>
        <param name="compareOptions">Combinazione bit per bit dei valori di enumerazione che descrivono il tipo di confronto da utilizzare per l'operazione di riconoscimento emulato.</param>
        <summary>Emula l'input di una frase al riconoscimento vocale, utilizzando il testo anziché l'audio per il riconoscimento vocale asincrono, e specifica come il riconoscimento gestisce il confronto Unicode tra la frase e le grammatiche di riconoscimento vocale caricate.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il controllo di riconoscimento vocale genera il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventi come se l'operazione di riconoscimento non è emulata. Al termine dell'operazione di riconoscimento asincrono il riconoscimento, genera il <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> evento.  
  
 Il riconoscimento utilizza `compareOptions` quando si applica regole grammaticali per la frase di input. Il riconoscimento forniti con Vista e Windows 7 Ignora maiuscole / minuscole se il <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> o <xref:System.Globalization.CompareOptions.IgnoreCase> valore è presente. Il riconoscimento Ignora sempre la larghezza del carattere e mai Ignora Katakana / Hiragana. Il riconoscimento anche Ignora le nuove righe e lo spazio vuoto aggiuntivo e considera la punteggiatura come input letterale. Per ulteriori informazioni sulla larghezza del carattere e il tipo Kana, vedere il <xref:System.Globalization.CompareOptions> enumerazione.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Nel riconoscimento non sono caricate grammatiche di riconoscimento vocale, oppure ci sono operazioni di riconoscimento asincrone non ancora complete.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="inputText" /> è <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="inputText" /> è la stringa vuota ("").</exception>
        <exception cref="T:System.NotSupportedException">
          <paramref name="compareOptions" /> contiene il flag <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" /> o <see cref="F:System.Globalization.CompareOptions.StringSort" />.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; EmulateRecognizeCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; EmulateRecognizeCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event EmulateRecognizeCompleted As EventHandler(Of EmulateRecognizeCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::EmulateRecognizeCompletedEventArgs ^&gt; ^ EmulateRecognizeCompleted;" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Generato quando <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> completa un'operazione di riconoscimento asincrona di input emulato.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ogni <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> metodo inizia un'operazione asincrona di riconoscimento. Il <xref:System.Speech.Recognition.SpeechRecognitionEngine> genera il <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> evento quando completa l'operazione asincrona.  
  
 Il <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> operazione può generare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventi. Il <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> evento è l'ultimo evento di questo tipo che il riconoscimento genera per l'operazione specificata.  
  
 Se il riconoscimento emulato ha avuto esito positivo, è possibile accedere al risultato del riconoscimento utilizzando uno dei seguenti:  
  
-   Il <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs.Result%2A> proprietà il <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs> oggetto nel gestore per il <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> evento.  
  
-   <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> proprietà di <xref:System.Speech.Recognition.SpeechRecognizedEventArgs> oggetto nel gestore per il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> evento.  
  
 Se emulato riconoscimento non è stato completato la <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> non viene generato e <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs.Result%2A> sarà null.  
  
 <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs> deriva da <xref:System.ComponentModel.AsyncCompletedEventArgs>.  
  
 <xref:System.Speech.Recognition.SpeechRecognizedEventArgs> deriva da <xref:System.Speech.Recognition.RecognitionEventArgs>.  
  
 Quando si crea un delegato di <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted>, si identifica il metodo con cui gestire l'evento. Per associare l'evento al gestore eventi in uso, aggiungere all'evento un'istanza del delegato. Il gestore eventi viene chiamato ogni volta che si verifica l'evento, a meno che non venga rimosso il delegato. Per ulteriori informazioni sui delegati del gestore eventi, vedere [eventi e delegati](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Nell'esempio seguente fa parte di un'applicazione console che viene caricata una grammatica di riconoscimento vocale e input emulata asincrona, i risultati di riconoscimento associati e gli eventi associati generati dal riconoscimento vocale.  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace InProcessRecognizer  
{  
  class Program  
  {  
    // Indicate whether the asynchronous emulate recognition  
    // operation has completed.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
  
      // Initialize an instance of an in-process recognizer.  
      using (SpeechRecognitionEngine recognizer =   
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
        recognizer.LoadGrammar(testGrammar);  
  
        // Attach event handlers for recognition events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call mathches the grammar  
        // and generates a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing testing");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call does not match the grammar  
        // or generate a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing one two three");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null)  
      {  
        Console.WriteLine("Result of 1st call to EmulateRecognizeAsync = {0}",  
          e.Result.Text ?? "<no text>");  
        Console.WriteLine();  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }  
  
    // Handle the EmulateRecognizeCompleted event.  
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      if (e.Result == null)  
      {  
        Console.WriteLine("Result of 2nd call to EmulateRecognizeAsync = No result generated.");  
      }  
  
      // Indicate the asynchronous operation is complete.  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EndSilenceTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan EndSilenceTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan EndSilenceTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
      <MemberSignature Language="VB.NET" Value="Public Property EndSilenceTimeout As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan EndSilenceTimeout { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ottiene o imposta l'intervallo di silenzio che <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> accetterà alla fine dell'input non ambiguo prima di completare un'operazione di riconoscimento.</summary>
        <value>La durata dell'intervallo di silenzio.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il riconoscimento vocale utilizza questo intervallo di timeout quando l'input di riconoscimento non è ambiguo. Ad esempio, per una grammatica di riconoscimento vocale che supporta il riconoscimento di uno "nuovo gioco." o "nuova partita", "nuovo gioco." è un input non ambiguo, e "nuova partita" è un input ambiguo.  
  
 Questa proprietà determina il tempo di attesa di riconoscimento vocale per l'input aggiuntivi prima di completare un'operazione di riconoscimento. L'intervallo di timeout può essere da 0 a 10 secondi. Il valore predefinito è 150 millisecondi.  
  
 Per impostare l'intervallo di timeout per l'input ambiguo, utilizzare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> proprietà.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Questa proprietà è impostata su un valore inferiore a 0 secondi o maggiore di 10 secondi.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <Member MemberName="EndSilenceTimeoutAmbiguous">
      <MemberSignature Language="C#" Value="public TimeSpan EndSilenceTimeoutAmbiguous { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan EndSilenceTimeoutAmbiguous" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
      <MemberSignature Language="VB.NET" Value="Public Property EndSilenceTimeoutAmbiguous As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan EndSilenceTimeoutAmbiguous { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ottiene o imposta l'intervallo di silenzio che <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> accetterà alla fine dell'input ambiguo prima di completare un'operazione di riconoscimento.</summary>
        <value>La durata dell'intervallo di silenzio.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il riconoscimento vocale utilizza questo intervallo di timeout quando l'input di riconoscimento è ambiguo. Ad esempio, per una grammatica di riconoscimento vocale che supporta il riconoscimento di uno "nuovo gioco." o "nuova partita", "nuovo gioco." è un input non ambiguo, e "nuova partita" è un input ambiguo.  
  
 Questa proprietà determina il tempo di attesa di riconoscimento vocale per l'input aggiuntivi prima di completare un'operazione di riconoscimento. L'intervallo di timeout può essere da 0 a 10 secondi. Il valore predefinito è 500 millisecondi.  
  
 Per impostare l'intervallo di timeout per l'input non ambiguo, utilizzare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> proprietà.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Questa proprietà è impostata su un valore inferiore a 0 secondi o maggiore di 10 secondi.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <Member MemberName="Grammars">
      <MemberSignature Language="C#" Value="public System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt; Grammars { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Collections.ObjectModel.ReadOnlyCollection`1&lt;class System.Speech.Recognition.Grammar&gt; Grammars" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.Grammars" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property Grammars As ReadOnlyCollection(Of Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::Grammar ^&gt; ^ Grammars { System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::Grammar ^&gt; ^ get(); };" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ottiene una raccolta di oggetti <see cref="T:System.Speech.Recognition.Grammar" /> caricati in questa istanza di <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>Raccolta di oggetti <see cref="T:System.Speech.Recognition.Grammar" />.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Examples  
 Nell'esempio seguente genera informazioni nella console per ogni grammatica di riconoscimento vocale attualmente caricata da un riconoscimento vocale.  
  
> [!IMPORTANT]
>  Copiare la raccolta di grammatica per evitare errori se la raccolta viene modificata mentre questo metodo enumera gli elementi della raccolta.  
  
```csharp  
  
private static void ListGrammars(SpeechRecognitionEngine recognizer)  
{  
  string qualifier;  
  List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
  foreach (Grammar g in grammars)  
  {  
    qualifier = (g.Enabled) ? "enabled" : "disabled";  
  
    Console.WriteLine("Grammar {0} is loaded and is {1}.",  
      g.Name, qualifier);  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.Grammar" />
      </Docs>
    </Member>
    <Member MemberName="InitialSilenceTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan InitialSilenceTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan InitialSilenceTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
      <MemberSignature Language="VB.NET" Value="Public Property InitialSilenceTimeout As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan InitialSilenceTimeout { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ottiene o imposta l'intervallo di tempo durante il quale <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> accetta input contenente solo silenzio, prima di completare il riconoscimento.</summary>
        <value>La durata dell'intervallo di silenzio.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ogni riconoscimento vocale è disponibile un algoritmo per distinguere tra vocale e di inattività. Se l'input per il riconoscimento è inattività durante il periodo di timeout di inattività iniziale, il riconoscimento completa l'operazione di riconoscimento.  
  
-   Per le operazioni asincrone di riconoscimento e l'emulazione, genera il riconoscimento di <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> evento, in cui il <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InitialSilenceTimeout%2A?displayProperty=nameWithType> proprietà è `true`e <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A?displayProperty=nameWithType> proprietà è `null`.  
  
-   Per le operazioni di riconoscimento sincrono e l'emulazione, restituisce il riconoscimento `null`, anziché un valore valido <xref:System.Speech.Recognition.RecognitionResult>.  
  
 Se l'intervallo di timeout di inattività iniziale è impostato su 0, il riconoscimento non in grado di eseguire un controllo di timeout di inattività iniziale. L'intervallo di timeout può essere qualsiasi valore non negativo. Il valore predefinito è 0 secondi.  
  
   
  
## Examples  
 Nell'esempio seguente viene illustrata parte di un'applicazione console che illustra il riconoscimento vocale base. Nell'esempio viene impostata la <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> e <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> le proprietà di un <xref:System.Speech.Recognition.SpeechRecognitionEngine> prima di avviare il riconoscimento vocale. Gestori per il riconoscimento vocale <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> e <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> gli eventi di output delle informazioni di evento nella console per dimostrare come <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> le proprietà di un <xref:System.Speech.Recognition.SpeechRecognitionEngine> tali proprietà influiscono sulle operazioni di riconoscimento.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Load a Grammar object.  
        recognizer.LoadGrammar(CreateServicesGrammar("FindServices"));  
  
        // Add event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(  
            AudioStateChangedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(3);  
        recognizer.BabbleTimeout = TimeSpan.FromSeconds(2);  
        recognizer.EndSilenceTimeout = TimeSpan.FromSeconds(1);  
        recognizer.EndSilenceTimeoutAmbiguous = TimeSpan.FromSeconds(1.5);  
  
        Console.WriteLine("BabbleTimeout: {0}", recognizer.BabbleTimeout);  
        Console.WriteLine("InitialSilenceTimeout: {0}", recognizer.InitialSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeout: {0}", recognizer.EndSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeoutAmbiguous: {0}", recognizer.EndSilenceTimeoutAmbiguous);  
        Console.WriteLine();  
  
        // Start asynchronous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Single);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Create a grammar and build it into a Grammar object.   
    static Grammar CreateServicesGrammar(string grammarName)  
    {  
  
      // Create a grammar for finding services in different cities.  
      Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
      Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
      GrammarBuilder findServices = new GrammarBuilder("Find");  
      findServices.Append(services);  
      findServices.Append("near");  
      findServices.Append(cities);  
  
      // Create a Grammar object from the GrammarBuilder..  
      Grammar servicesGrammar = new Grammar(findServices);  
      servicesGrammar.Name = ("FindServices");  
      return servicesGrammar;  
    }  
  
    // Handle the AudioStateChanged event.  
    static void AudioStateChangedHandler(  
      object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("AudioStateChanged ({0}): {1}",  
        DateTime.Now.ToString("mm:ss.f"), e.AudioState);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("RecognizeCompleted ({0}):",  
        DateTime.Now.ToString("mm:ss.f"));  
  
      string resultText;  
      if (e.Result != null) { resultText = e.Result.Text; }  
      else { resultText = "<null>"; }  
  
      Console.WriteLine(  
        " BabbleTimeout: {0}; InitialSilenceTimeout: {1}; Result text: {2}",  
        e.BabbleTimeout, e.InitialSilenceTimeout, resultText);  
      if (e.Error != null)  
      {  
        Console.WriteLine(" Exception message: ", e.Error.Message);  
      }  
  
      // Start the next asynchronous recognition operation.  
      ((SpeechRecognitionEngine)sender).RecognizeAsync(RecognizeMode.Single);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Questa proprietà è impostata su un valore minore di 0 secondi.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <Member MemberName="InstalledRecognizers">
      <MemberSignature Language="C#" Value="public static System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt; InstalledRecognizers ();" />
      <MemberSignature Language="ILAsm" Value=".method public static hidebysig class System.Collections.ObjectModel.ReadOnlyCollection`1&lt;class System.Speech.Recognition.RecognizerInfo&gt; InstalledRecognizers() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers" />
      <MemberSignature Language="VB.NET" Value="Public Shared Function InstalledRecognizers () As ReadOnlyCollection(Of RecognizerInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; static System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::RecognizerInfo ^&gt; ^ InstalledRecognizers();" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt;</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Restituisce informazioni per tutti sistemi di riconoscimento vocale installati nel sistema corrente.</summary>
        <returns>Raccolta di sola lettura di oggetti <see cref="T:System.Speech.Recognition.RecognizerInfo" /> che descrive i riconoscitori installati.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Per ottenere informazioni sul riconoscimento corrente, utilizzare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo%2A> proprietà.  
  
   
  
## Examples  
 Nell'esempio seguente viene illustrata parte di un'applicazione console che illustra il riconoscimento vocale base. Nell'esempio viene utilizzata la raccolta restituita dal <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> metodo per trovare un riconoscimento vocale che supporta la lingua inglese.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Select a speech recognizer that supports English.  
      RecognizerInfo info = null;  
      foreach (RecognizerInfo ri in SpeechRecognitionEngine.InstalledRecognizers())  
      {  
        if (ri.Culture.TwoLetterISOLanguageName.Equals("en"))  
        {  
          info = ri;  
          break;  
        }  
      }  
      if (info == null) return;  
  
      // Create the selected recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(info))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammar">
      <MemberSignature Language="C#" Value="public void LoadGrammar (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void LoadGrammar(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void LoadGrammar(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">L'oggetto di grammatica da caricare.</param>
        <summary>Carica un oggetto <see cref="T:System.Speech.Recognition.Grammar" /> in modo sincrono.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il riconoscimento genera un'eccezione se il <xref:System.Speech.Recognition.Grammar> oggetto è già stato caricato, viene caricato in modo asincrono o non è riuscito a caricare qualsiasi tipo di riconoscimento. Non è possibile caricare lo stesso <xref:System.Speech.Recognition.Grammar> oggetto in più istanze di <xref:System.Speech.Recognition.SpeechRecognitionEngine>. In alternativa, creare un nuovo <xref:System.Speech.Recognition.Grammar> per ciascun <xref:System.Speech.Recognition.SpeechRecognitionEngine> istanza.  
  
 Se il riconoscimento è in esecuzione, le applicazioni devono utilizzare <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> per sospendere il motore di riconoscimento vocale prima di caricamento, scaricamento, abilitazione o disabilitazione di una grammatica.  
  
 Quando si carica una grammatica, viene abilitato per impostazione predefinita. Per disabilitare una grammatica caricata, utilizzare il <xref:System.Speech.Recognition.Grammar.Enabled%2A> proprietà.  
  
 Per caricare un <xref:System.Speech.Recognition.Grammar> in modo asincrono, utilizzare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> metodo.  
  
   
  
## Examples  
 Nell'esempio seguente viene illustrata parte di un'applicazione console che illustra il riconoscimento vocale base. Nell'esempio viene creato un <xref:System.Speech.Recognition.DictationGrammar> e li carica in un riconoscimento vocale.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="Grammar" /> è <see langword="null" />.</exception>
        <exception cref="T:System.InvalidOperationException">
          <paramref name="Grammar" /> non è uno stato valido.</exception>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammarAsync">
      <MemberSignature Language="C#" Value="public void LoadGrammarAsync (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void LoadGrammarAsync(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void LoadGrammarAsync(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">La grammatica di riconoscimento vocale da caricare.</param>
        <summary>Carica in modo asincrono una grammatica di riconoscimento vocale.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Quando il riconoscimento viene completato il caricamento un <xref:System.Speech.Recognition.Grammar> dell'oggetto, viene generato un <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> evento. Il riconoscimento genera un'eccezione se il <xref:System.Speech.Recognition.Grammar> oggetto è già stato caricato, viene caricato in modo asincrono o non è riuscito a caricare qualsiasi tipo di riconoscimento. Non è possibile caricare lo stesso <xref:System.Speech.Recognition.Grammar> oggetto in più istanze di <xref:System.Speech.Recognition.SpeechRecognitionEngine>. In alternativa, creare un nuovo <xref:System.Speech.Recognition.Grammar> per ciascun <xref:System.Speech.Recognition.SpeechRecognitionEngine> istanza.  
  
 Se il riconoscimento è in esecuzione, le applicazioni devono utilizzare <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> per sospendere il motore di riconoscimento vocale prima di caricamento, scaricamento, abilitazione o disabilitazione di una grammatica.  
  
 Quando si carica una grammatica, viene abilitato per impostazione predefinita. Per disabilitare una grammatica caricata, utilizzare il <xref:System.Speech.Recognition.Grammar.Enabled%2A> proprietà.  
  
 Per caricare una grammatica di riconoscimento vocale in modo sincrono, utilizzare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> metodo.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="Grammar" /> è <see langword="null" />.</exception>
        <exception cref="T:System.InvalidOperationException">
          <paramref name="Grammar" /> non è uno stato valido.</exception>
        <exception cref="T:System.OperationCanceledException">Operazione asincrona annullata.</exception>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammarCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; LoadGrammarCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; LoadGrammarCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event LoadGrammarCompleted As EventHandler(Of LoadGrammarCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::LoadGrammarCompletedEventArgs ^&gt; ^ LoadGrammarCompleted;" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Generato quando <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> termina il caricamento asincrono di un oggetto <see cref="T:System.Speech.Recognition.Grammar" />.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il riconoscimento <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> metodo inizia un'operazione asincrona. Il <xref:System.Speech.Recognition.SpeechRecognitionEngine> genera questo evento quando viene completato l'operazione. Per ottenere il <xref:System.Speech.Recognition.Grammar> oggetto caricato il riconoscimento, usare il <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs.Grammar%2A> proprietà dell'oggetto associato <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs>. Per ottenere l'oggetto corrente <xref:System.Speech.Recognition.Grammar> oggetti ha caricato il riconoscimento, usare il riconoscimento <xref:System.Speech.Recognition.SpeechRecognitionEngine.Grammars%2A> proprietà.  
  
 Se il riconoscimento è in esecuzione, le applicazioni devono utilizzare <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> per sospendere il motore di riconoscimento vocale prima di caricamento, scaricamento, abilitazione o disabilitazione di una grammatica.  
  
 Quando si crea un delegato <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted>, si identifica il metodo che gestirà l'evento. Per associare l'evento al gestore eventi in uso, aggiungere all'evento un'istanza del delegato. Il gestore eventi viene chiamato ogni volta che si verifica l'evento, a meno che non venga rimosso il delegato. Per ulteriori informazioni sui delegati del gestore eventi, vedere [eventi e delegati](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Nell'esempio seguente crea un riconoscimento vocale in-process e quindi crea due tipi di grammatiche per il riconoscimento delle parole specifiche e per l'accettazione di dettatura disponibile. Nell'esempio si costruisce un <xref:System.Speech.Recognition.Grammar> oggetto da ogni le grammatiche di riconoscimento vocale completata, quindi carica in modo asincrono il <xref:System.Speech.Recognition.Grammar> oggetti per il <xref:System.Speech.Recognition.SpeechRecognitionEngine> istanza. Gestori per il riconoscimento <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> e <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventi scrive nella console il nome del <xref:System.Speech.Recognition.Grammar> oggetto utilizzato per eseguire il riconoscimento e il testo del risultato del riconoscimento, rispettivamente.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and set its input.  
      recognizer = new SpeechRecognitionEngine();  
      recognizer.SetInputToDefaultAudioDevice();  
  
      // Add a handler for the LoadGrammarCompleted event.  
      recognizer.LoadGrammarCompleted +=  
        new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
      // Add a handler for the SpeechRecognized event.  
      recognizer.SpeechRecognized +=  
        new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
      // Create the "yesno" grammar.  
      Choices yesChoices = new Choices(new string[] { "yes", "yup", "yeah" });  
      SemanticResultValue yesValue =  
          new SemanticResultValue(yesChoices, (bool)true);  
      Choices noChoices = new Choices(new string[] { "no", "nope", "neah" });  
      SemanticResultValue noValue =  
          new SemanticResultValue(noChoices, (bool)false);  
      SemanticResultKey yesNoKey =  
          new SemanticResultKey("yesno", new Choices(new GrammarBuilder[] { yesValue, noValue }));  
      Grammar yesnoGrammar = new Grammar(yesNoKey);  
      yesnoGrammar.Name = "yesNo";  
  
      // Create the "done" grammar.  
      Grammar doneGrammar =  
        new Grammar(new Choices(new string[] { "done", "exit", "quit", "stop" }));  
      doneGrammar.Name = "Done";  
  
      // Create a dictation grammar.  
      Grammar dictation = new DictationGrammar();  
      dictation.Name = "Dictation";  
  
      // Load grammars to the recognizer.  
      recognizer.LoadGrammarAsync(yesnoGrammar);  
      recognizer.LoadGrammarAsync(doneGrammar);  
      recognizer.LoadGrammarAsync(dictation);  
  
      // Start asynchronous, continuous recognition.  
      recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
      // Keep the console window open.  
      Console.ReadLine();  
    }  
  
    // Handle the LoadGrammarCompleted event.   
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      string grammarName = e.Grammar.Name;  
      bool grammarLoaded = e.Grammar.Loaded;  
  
      if (e.Error != null)  
      {  
        Console.WriteLine("LoadGrammar for {0} failed with a {1}.",  
        grammarName, e.Error.GetType().Name);  
  
        // Add exception handling code here.  
      }  
  
      Console.WriteLine("Grammar {0} {1} loaded.",  
      grammarName, (grammarLoaded) ? "is" : "is not");  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Grammar({0}): {1}", e.Result.Grammar.Name, e.Result.Text);  
  
      // Add event handler code here.  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.LoadGrammarCompletedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.Grammars" />
      </Docs>
    </Member>
    <Member MemberName="MaxAlternates">
      <MemberSignature Language="C#" Value="public int MaxAlternates { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance int32 MaxAlternates" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" />
      <MemberSignature Language="VB.NET" Value="Public Property MaxAlternates As Integer" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property int MaxAlternates { int get(); void set(int value); };" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Int32</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ottiene o imposta il numero massimo di risultati del riconoscimento alternativi che <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> restituisce per ogni operazione di riconoscimento.</summary>
        <value>Il numero di risultati alternativi da restituire.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> proprietà del <xref:System.Speech.Recognition.RecognitionResult> classe contiene la raccolta di <xref:System.Speech.Recognition.RecognizedPhrase> gli oggetti che rappresentano le interpretazioni possibili dell'input.  
  
 Il valore predefinito per <xref:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates%2A> è 10.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">
          La proprietà <see cref="P:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" /> è impostata su un valore minore di 0.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
      </Docs>
    </Member>
    <Member MemberName="QueryRecognizerSetting">
      <MemberSignature Language="C#" Value="public object QueryRecognizerSetting (string settingName);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance object QueryRecognizerSetting(string settingName) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Function QueryRecognizerSetting (settingName As String) As Object" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Object ^ QueryRecognizerSetting(System::String ^ settingName);" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Object</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="settingName">Nome dell'impostazione da restituire.</param>
        <summary>Restituisce i valori delle impostazioni per il riconoscimento.</summary>
        <returns>Valore dell'impostazione.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Le impostazioni per il riconoscimento possono contenere una stringa, intero a 64 bit o dati di indirizzo di memoria. Nella tabella seguente vengono descritte le impostazioni definite per un riconoscimento vocale API di Microsoft (SAPI)-riconoscimento conformo. Le seguenti impostazioni devono avere lo stesso intervallo per ogni riconoscimento che supporta l'impostazione. Un riconoscimento conformi SAPI non è necessario per supportare queste impostazioni e può supportare altre impostazioni.  
  
|nome|Descrizione|  
|----------|-----------------|  
|`ResourceUsage`|Specifica l'utilizzo della CPU del riconoscimento. L'intervallo è compreso tra 0 e 100. Il valore predefinito è 50.|  
|`ResponseSpeed`|Indica la lunghezza di inattività alla fine dell'input non ambiguo prima che il riconoscimento vocale viene completata un'operazione di riconoscimento. L'intervallo è compreso tra 0 e 10.000 millisecondi (ms). Questa impostazione corrisponde al riconoscimento <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> proprietà.  Predefinito = 150 ms.|  
|`ComplexResponseSpeed`|Indica la lunghezza di inattività alla fine dell'input ambiguo prima che il riconoscimento vocale viene completata un'operazione di riconoscimento. L'intervallo è compreso tra 0 e 10.000 ms. Questa impostazione corrisponde al riconoscimento <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> proprietà. Predefinito = 500ms.|  
|`AdaptationOn`|Indica se l'adattamento del modello acustico è ON (valore = `1`) o OFF (valore = `0`). Il valore predefinito è `1` (ON).|  
|`PersistedBackgroundAdaptation`|Indica se l'adattamento è ON (valore = `1`) o OFF (valore = `0`), e viene mantenuta l'impostazione del Registro di sistema. Il valore predefinito è `1` (ON).|  
  
 Per aggiornare un'impostazione per il riconoscimento, utilizzare uno del <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> metodi.  
  
   
  
## Examples  
 Nell'esempio seguente fa parte di un'applicazione console che restituisce i valori per un numero di impostazioni definite per il riconoscimento che supporta le impostazioni locali en-US. L'esempio genera l'output seguente.  
  
```  
Settings for recognizer MS-1033-80-DESK:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 150  
  ComplexResponseSpeed           = 500  
  AdaptationOn                   = 1  
  PersistedBackgroundAdaptation  = 1  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace RecognizerSettings  
{  
  class Program  
  {  
    static readonly string[] settings = new string[] {  
      "ResourceUsage",  
      "ResponseSpeed",  
      "ComplexResponseSpeed",  
      "AdaptationOn",  
      "PersistedBackgroundAdaptation"  
    };  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        Console.WriteLine("Settings for recognizer {0}:",  
          recognizer.RecognizerInfo.Name);  
        Console.WriteLine();  
  
        foreach (string setting in settings)  
        {  
          try  
          {  
            object value = recognizer.QueryRecognizerSetting(setting);  
            Console.WriteLine("  {0,-30} = {1}", setting, value);  
          }  
          catch  
          {  
            Console.WriteLine("  {0,-30} is not supported by this recognizer.",  
              setting);  
          }  
        }  
      }  
      Console.WriteLine();  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="settingName" /> è <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="settingName" /> è la stringa vuota ("").</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">Il riconoscimento non ha un'impostazione con tale nome.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.Int32)" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
      </Docs>
    </Member>
    <MemberGroup MemberName="Recognize">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Avvia un'operazione sincrona di riconoscimento vocale.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Questi metodi eseguono un'operazione di riconoscimento sincrono, singolo. Il riconoscimento esegue questa operazione su relativo grammatiche di riconoscimento vocale caricati e abilitati.  
  
 Durante una chiamata a questo metodo, il riconoscimento può generare gli eventi seguenti:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Generato quando il riconoscimento rileva l'input che è possibile identificare come riconoscimento vocale.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Generato quando l'input viene creata una corrispondenza ambigua con una delle grammatiche attive.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Generato quando il riconoscimento completa un'operazione di riconoscimento.  
  
 Il riconoscimento non genera il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> evento quando si utilizza uno del <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> metodi.  
  
 Il <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> i metodi restituiscono un <xref:System.Speech.Recognition.RecognitionResult> oggetto, o `null` se l'operazione non è riuscita o non è abilitato il riconoscimento.  
  
 Un'operazione di riconoscimento sincrono può non riuscire per i motivi seguenti:  
  
-   Riconoscimento vocale non viene rilevato prima della scadono gli intervalli di timeout per il <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> , proprietà o per il `initialSilenceTimeout` parametro del <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> (metodo).  
  
-   Il motore di riconoscimento vocale rileva, ma non trova corrispondenze in uno dei relativi caricati e abilitati <xref:System.Speech.Recognition.Grammar> oggetti.  
  
 Per modificare la modalità di gestione dei tempi di inattività rispetto al riconoscimento o di riconoscimento vocale il riconoscimento, utilizzare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> proprietà.  
  
 Il <xref:System.Speech.Recognition.SpeechRecognitionEngine> deve avere almeno un <xref:System.Speech.Recognition.Grammar> oggetto caricato prima di eseguire il riconoscimento. Per caricare una grammatica di riconoscimento vocale, utilizzare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> metodo.  
  
 Per eseguire il riconoscimento asincrono, utilizzare uno del <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> metodi.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="Recognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult Recognize ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult Recognize() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
      <MemberSignature Language="VB.NET" Value="Public Function Recognize () As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ Recognize();" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Effettua un'operazione sincrona di riconoscimento vocale.</summary>
        <returns>Il risultato di riconoscimento per l'input, o <see langword="null" /> se l'operazione non riesce o il riconoscimento non è abilitato.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Questo metodo esegue un'operazione di riconoscimento singolo. Il riconoscimento esegue questa operazione su relativo grammatiche di riconoscimento vocale caricati e abilitati.  
  
 Durante una chiamata a questo metodo, il riconoscimento può generare gli eventi seguenti:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Generato quando il riconoscimento rileva l'input che è possibile identificare come riconoscimento vocale.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Generato quando l'input viene creata una corrispondenza ambigua con una delle grammatiche attive.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Generato quando il riconoscimento completa un'operazione di riconoscimento.  
  
 Il riconoscimento non genera il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> evento quando si utilizza questo metodo.  
  
 Il <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize> metodo restituisce un <xref:System.Speech.Recognition.RecognitionResult> oggetto, o `null` se l'operazione non è riuscita.  
  
 Un'operazione di riconoscimento sincrono può non riuscire per i motivi seguenti:  
  
-   Riconoscimento vocale non viene rilevato prima della scadono gli intervalli di timeout per il <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> proprietà.  
  
-   Il motore di riconoscimento vocale rileva, ma non trova corrispondenze in uno dei relativi caricati e abilitati <xref:System.Speech.Recognition.Grammar> oggetti.  
  
 Per eseguire il riconoscimento asincrono, utilizzare uno del <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> metodi.  
  
   
  
## Examples  
 Nell'esempio seguente viene illustrata parte di un'applicazione console che illustra il riconoscimento vocale base. Nell'esempio viene creato un <xref:System.Speech.Recognition.DictationGrammar>, li carica in un riconoscimento vocale in-process ed esegue un'operazione di riconoscimento.  
  
```  
  
using System;  
using System.Speech.Recognition;  
  
namespace SynchronousRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Modify the initial silence time-out value.  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(5);  
  
        // Start synchronous speech recognition.  
        RecognitionResult result = recognizer.Recognize();  
  
        if (result != null)  
        {  
          Console.WriteLine("Recognized text = {0}", result.Text);  
        }  
        else  
        {  
          Console.WriteLine("No recognition result available.");  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to continue...");  
      Console.ReadKey();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="Recognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult Recognize (TimeSpan initialSilenceTimeout);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult Recognize(valuetype System.TimeSpan initialSilenceTimeout) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize(System.TimeSpan)" />
      <MemberSignature Language="VB.NET" Value="Public Function Recognize (initialSilenceTimeout As TimeSpan) As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ Recognize(TimeSpan initialSilenceTimeout);" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="initialSilenceTimeout" Type="System.TimeSpan" />
      </Parameters>
      <Docs>
        <param name="initialSilenceTimeout">L'intervallo di tempo in cui un riconoscimento vocale accetta l'input contenente solo il silenzio prima di finalizzare il riconoscimento.</param>
        <summary>Effettua un'operazione sincrona di riconoscimento vocale con un periodo di timeout di inattività iniziale specificato.</summary>
        <returns>Il risultato di riconoscimento per l'input, o <see langword="null" /> se l'operazione non riesce o il riconoscimento non è abilitato.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Se il motore di riconoscimento vocale rileva vocale entro l'intervallo di tempo specificato da `initialSilenceTimeout` argomento, <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%28System.TimeSpan%29> esegue un'operazione di riconoscimento singolo e quindi termina.  Il `initialSilenceTimeout` parametro sostituisce il riconoscimento <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> proprietà.  
  
 Durante una chiamata a questo metodo, il riconoscimento può generare gli eventi seguenti:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Generato quando il riconoscimento rileva l'input che è possibile identificare come riconoscimento vocale.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Generato quando l'input viene creata una corrispondenza ambigua con una delle grammatiche attive.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Generato quando il riconoscimento completa un'operazione di riconoscimento.  
  
 Il riconoscimento non genera il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> evento quando si utilizza questo metodo.  
  
 Il <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize> metodo restituisce un <xref:System.Speech.Recognition.RecognitionResult> oggetto, o `null` se l'operazione non è riuscita.  
  
 Un'operazione di riconoscimento sincrono può non riuscire per i motivi seguenti:  
  
-   Riconoscimento vocale non viene rilevato prima della scadono gli intervalli di timeout per il <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> o per il `initialSilenceTimeout` parametro.  
  
-   Il motore di riconoscimento vocale rileva, ma non trova corrispondenze in uno dei relativi caricati e abilitati <xref:System.Speech.Recognition.Grammar> oggetti.  
  
 Per eseguire il riconoscimento asincrono, utilizzare uno del <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> metodi.  
  
   
  
## Examples  
 Nell'esempio seguente viene illustrata parte di un'applicazione console che illustra il riconoscimento vocale base. Nell'esempio viene creato un <xref:System.Speech.Recognition.DictationGrammar>, li carica in un riconoscimento vocale in-process ed esegue un'operazione di riconoscimento.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SynchronousRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start synchronous speech recognition.  
        RecognitionResult result = recognizer.Recognize(TimeSpan.FromSeconds(5));  
  
        if (result != null)  
        {  
          Console.WriteLine("Recognized text = {0}", result.Text);  
        }  
        else  
        {  
          Console.WriteLine("No recognition result available.");  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to continue...");  
      Console.ReadKey();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      </Docs>
    </Member>
    <MemberGroup MemberName="RecognizeAsync">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Avvia un'operazione asincrona di riconoscimento vocale.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Questi metodi eseguono uno o più operazioni asincrone di riconoscimento. Il riconoscimento esegue ogni operazione su relativo grammatiche di riconoscimento vocale caricati e abilitati.  
  
 Durante una chiamata a questo metodo, il riconoscimento può generare gli eventi seguenti:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Generato quando il riconoscimento rileva l'input che è possibile identificare come riconoscimento vocale.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Generato quando l'input viene creata una corrispondenza ambigua con una delle grammatiche attive.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Generato quando il riconoscimento completa un'operazione di riconoscimento.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. Eccezione generata quando un <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> al termine dell'operazione.  
  
 Per recuperare il risultato di un'operazione asincrona di riconoscimento, collegare un gestore eventi per il riconoscimento <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> evento. Il riconoscimento ha generato l'evento ogni volta che completata correttamente un'operazione di riconoscimento sincrono o asincrono. Se il riconoscimento non è stato completato la <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> proprietà <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> oggetto, che è possibile accedere nel gestore per il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> evento, saranno `null`.  
  
 Un'operazione asincrona di riconoscimento può non riuscire per i motivi seguenti:  
  
-   Riconoscimento vocale non viene rilevato prima della scadono gli intervalli di timeout per il <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> proprietà.  
  
-   Il motore di riconoscimento vocale rileva, ma non trova corrispondenze in uno dei relativi caricati e abilitati <xref:System.Speech.Recognition.Grammar> oggetti.  
  
-   Il <xref:System.Speech.Recognition.SpeechRecognitionEngine> deve avere almeno un <xref:System.Speech.Recognition.Grammar> oggetto caricato prima di eseguire il riconoscimento. Per caricare una grammatica di riconoscimento vocale, utilizzare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> metodo.  
  
-   Per modificare la modalità di gestione dei tempi di inattività rispetto al riconoscimento o di riconoscimento vocale il riconoscimento, utilizzare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> proprietà.  
  
-   Per eseguire il riconoscimento sincrono, utilizzare uno del <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> metodi.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="RecognizeAsync">
      <MemberSignature Language="C#" Value="public void RecognizeAsync ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsync() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsync ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsync();" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Effettua un'unica operazione asincrona di riconoscimento vocale.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Questo metodo esegue un'operazione asincrona di riconoscimento. Il riconoscimento esegue l'operazione relativa grammatiche di riconoscimento vocale caricati e abilitati.  
  
 Durante una chiamata a questo metodo, il riconoscimento può generare gli eventi seguenti:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Generato quando il riconoscimento rileva l'input che è possibile identificare come riconoscimento vocale.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Generato quando l'input viene creata una corrispondenza ambigua con una delle grammatiche attive.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Generato quando il riconoscimento completa un'operazione di riconoscimento.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. Eccezione generata quando un <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> al termine dell'operazione.  
  
 Per recuperare il risultato di un'operazione asincrona di riconoscimento, collegare un gestore eventi per il riconoscimento <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> evento. Il riconoscimento ha generato l'evento ogni volta che completata correttamente un'operazione di riconoscimento sincrono o asincrono. Se il riconoscimento non è stato completato la <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> proprietà <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> oggetto, che è possibile accedere nel gestore per il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> evento, saranno `null`.  
  
 Per eseguire il riconoscimento sincrono, utilizzare uno del <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> metodi.  
  
   
  
## Examples  
 Nell'esempio seguente viene illustrata parte di un'applicazione console che illustra il riconoscimento vocale asincrona di base. Nell'esempio viene creato un <xref:System.Speech.Recognition.DictationGrammar>, li carica in un riconoscimento vocale in-process e che esegue un'operazione asincrona di riconoscimento. I gestori di eventi sono inclusi per illustrare gli eventi generati durante l'operazione di riconoscimento.  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create a grammar for choosing cities for a flight.  
        Choices cities = new Choices(new string[]   
        { "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I want to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Construct a Grammar object and load it to the recognizer.  
        Grammar cityChooser = new Grammar(gb);  
        cityChooser.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(cityChooser);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer and start an asynchronous  
        // recognition operation.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        completed = false;  
        Console.WriteLine("Starting asynchronous recognition...");  
        recognizer.RecognizeAsync();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
        Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsync">
      <MemberSignature Language="C#" Value="public void RecognizeAsync (System.Speech.Recognition.RecognizeMode mode);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsync(valuetype System.Speech.Recognition.RecognizeMode mode) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync(System.Speech.Recognition.RecognizeMode)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsync (mode As RecognizeMode)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsync(System::Speech::Recognition::RecognizeMode mode);" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="mode" Type="System.Speech.Recognition.RecognizeMode" />
      </Parameters>
      <Docs>
        <param name="mode">Indica se eseguire una o più operazioni di riconoscimento.</param>
        <summary>Effettua una o più operazioni asincrone di riconoscimento vocale.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Se `mode` è <xref:System.Speech.Recognition.RecognizeMode.Multiple>, il riconoscimento continua l'esecuzione di operazioni asincrone riconoscimento fino a quando il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> metodo viene chiamato.  
  
 Durante una chiamata a questo metodo, il riconoscimento può generare gli eventi seguenti:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Generato quando il riconoscimento rileva l'input che è possibile identificare come riconoscimento vocale.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Generato quando l'input viene creata una corrispondenza ambigua con una delle grammatiche attive.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Generato quando il riconoscimento completa un'operazione di riconoscimento.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. Eccezione generata quando un <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> al termine dell'operazione.  
  
 Per recuperare il risultato di un'operazione asincrona di riconoscimento, collegare un gestore eventi per il riconoscimento <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> evento. Il riconoscimento ha generato l'evento ogni volta che completata correttamente un'operazione di riconoscimento sincrono o asincrono. Se il riconoscimento non è stato completato la <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> proprietà <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> oggetto, che è possibile accedere nel gestore per il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> evento, saranno `null`.  
  
 Un'operazione asincrona di riconoscimento può non riuscire per i motivi seguenti:  
  
-   Riconoscimento vocale non viene rilevato prima della scadono gli intervalli di timeout per il <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> proprietà.  
  
-   Il motore di riconoscimento vocale rileva, ma non trova corrispondenze in uno dei relativi caricati e abilitati <xref:System.Speech.Recognition.Grammar> oggetti.  
  
 Per eseguire il riconoscimento sincrono, utilizzare uno del <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> metodi.  
  
   
  
## Examples  
 Nell'esempio seguente viene illustrata parte di un'applicazione console che illustra il riconoscimento vocale asincrona di base. Nell'esempio viene creato un <xref:System.Speech.Recognition.DictationGrammar>, li carica in un riconoscimento vocale in-process e vengono eseguite più operazioni asincrone di riconoscimento. Le operazioni asincrone vengono annullate dopo 30 secondi. I gestori di eventi sono inclusi per illustrare gli eventi generati durante l'operazione di riconoscimento.  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create a grammar for choosing cities for a flight.  
        Choices cities = new Choices(new string[] { "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I want to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Construct a Grammar object and load it to the recognizer.  
        Grammar cityChooser = new Grammar(gb);  
        cityChooser.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(cityChooser);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer and start asynchronous  
        // recognition.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        completed = false;  
        Console.WriteLine("Starting asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 30 seconds, and then cancel asynchronous recognition.  
        Thread.Sleep(TimeSpan.FromSeconds(30));  
        recognizer.RecognizeAsyncCancel();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
        Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsyncCancel">
      <MemberSignature Language="C#" Value="public void RecognizeAsyncCancel ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsyncCancel() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsyncCancel ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsyncCancel();" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Termina il riconoscimento asincrono senza attendere il completamento dell'operazione di riconoscimento corrente.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Questo metodo completa immediatamente riconoscimento asincrono. Se l'operazione di riconoscimento asincrona corrente riceve input, l'input viene troncato e il completamento dell'operazione con l'input esistente. Genera il riconoscimento di <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> evento quando viene annullata un'operazione asincrona e imposta il <xref:System.ComponentModel.AsyncCompletedEventArgs.Cancelled%2A> proprietà del <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> a `true`. Questo metodo annulla le operazioni asincrone avviate dal <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> e <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> metodi.  
  
 Per arrestare il riconoscimento asincrono senza troncamento di input, utilizzare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> metodo.  
  
   
  
## Examples  
 Nell'esempio seguente viene illustrata parte di un'applicazione console che illustra l'uso del <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> metodo. L'esempio crea e carica una grammatica di riconoscimento vocale, avvia un'operazione di riconoscimento asincrona continua e quindi viene sospesa 2 secondi prima di annullare l'operazione. Il riconoscimento riceve l'input dal file c:\temp\audioinput\sample.wav. I gestori di eventi sono inclusi per illustrare gli eventi generati durante l'operazione di riconoscimento.  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Begin asynchronous recognition from pre-recorded input.  
        recognizer.SetInputToWaveFile(@"c:\temp\audioinput\sample.wav");  
  
        completed = false;  
        Console.WriteLine("Begin continuing asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 2 seconds and then cancel the recognition operation.  
        Thread.Sleep(TimeSpan.FromSeconds(2));  
        recognizer.RecognizeAsyncCancel();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine(" - asynchronous operation canceled.");  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsyncStop">
      <MemberSignature Language="C#" Value="public void RecognizeAsyncStop ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsyncStop() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsyncStop ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsyncStop();" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Arresta il riconoscimento asincrono dopo che l'operazione di riconoscimento corrente è stata completata.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Questo metodo completa il riconoscimento asincrono senza troncamento di input. Se l'operazione di riconoscimento asincrona corrente riceve input, il riconoscimento continua accettare input fino al completamento dell'operazione di riconoscimento corrente. Genera il riconoscimento di <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> evento quando un'operazione asincrona viene arrestata e imposta il <xref:System.ComponentModel.AsyncCompletedEventArgs.Cancelled%2A> proprietà del <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> a `true`. Questo metodo arresta le operazioni asincrone avviate dal <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> e <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> metodi.  
  
 Per annullare immediatamente riconoscimento asincrono con solo l'input esistente, utilizzare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> metodo.  
  
   
  
## Examples  
 Nell'esempio seguente viene illustrata parte di un'applicazione console che illustra l'uso del <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> metodo. L'esempio crea e carica una grammatica di riconoscimento vocale, avvia un'operazione di riconoscimento asincrona continua e quindi viene sospesa 2 secondi prima di interrompere l'operazione. Il riconoscimento riceve l'input dal file c:\temp\audioinput\sample.wav. I gestori di eventi sono inclusi per illustrare gli eventi generati durante l'operazione di riconoscimento.  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Begin asynchronous recognition from pre-recorded input.  
        recognizer.SetInputToWaveFile(@"c:\temp\audioinput\sample.wav");  
  
        completed = false;  
        Console.WriteLine("Begin continuing asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 2 seconds and then stop the recognition operation.  
        Thread.Sleep(TimeSpan.FromSeconds(2));  
        recognizer.RecognizeAsyncStop();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine(" - asynchronous operation canceled.");  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt; RecognizeCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.RecognizeCompletedEventArgs&gt; RecognizeCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event RecognizeCompleted As EventHandler(Of RecognizeCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::RecognizeCompletedEventArgs ^&gt; ^ RecognizeCompleted;" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Generato quando <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> completa un'operazione di riconoscimento asincrona.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il <xref:System.Speech.Recognition.SpeechRecognitionEngine> dell'oggetto <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> metodo inizia un'operazione asincrona di riconoscimento. Quando il riconoscimento completa l'operazione asincrona, genera questo evento.  
  
 Utilizzando il gestore per il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> evento, è possibile accedere il <xref:System.Speech.Recognition.RecognitionResult> nel <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> oggetto. Se il riconoscimento non è riuscito, <xref:System.Speech.Recognition.RecognitionResult> sarà `null`. Per determinare se un timeout o un'interruzione nel input audio ha causato il riconoscimento esito negativo, è possibile accedere alle proprietà per <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.BabbleTimeout%2A>, o <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InputStreamEnded%2A>.  
  
 Per ulteriori informazioni, vedere la classe <xref:System.Speech.Recognition.RecognizeCompletedEventArgs>.  
  
 Per ottenere informazioni dettagliate sui migliori candidati riconoscimento rifiutati, collegare un gestore per il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> evento.  
  
 Quando si crea un delegato <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>, si identifica il metodo che gestirà l'evento. Per associare l'evento al gestore eventi in uso, aggiungere all'evento un'istanza del delegato. Il gestore eventi viene chiamato ogni volta che si verifica l'evento, a meno che non venga rimosso il delegato. Per ulteriori informazioni sui delegati del gestore eventi, vedere [eventi e delegati](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Nell'esempio seguente riconosce frasi, ad esempio "Visualizzare l'elenco degli artisti nella categoria jazz" o "Gospel album visualizzare". Nell'esempio viene utilizzato un gestore per il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> evento per visualizzare informazioni sui risultati del riconoscimento nella console.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display");  
        mediaMenu.Append("the list of", 0, 1);  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the", 0, 1);  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category.", 0, 1);  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(recognizer_RecognizeCompleted);  
        recognizer.LoadGrammarCompleted +=   
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void recognizer_RecognizeCompleted(object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted, error occurred during recognition: {0}", e.Error);  
        return;  
      }  
  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted: BabbleTimeout({0}), InitialSilenceTimeout({1}).",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted: AudioPosition({0}), InputStreamEnded({1}).",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
  
      if (e.Result != null)  
      {  
        Console.WriteLine("RecognizeCompleted:");  
        Console.WriteLine("  Grammar: " + e.Result.Grammar.Name);  
        Console.WriteLine("  Recognized text: " + e.Result.Text);  
        Console.WriteLine("  Confidence score: " + e.Result.Confidence);  
        Console.WriteLine("  Audio position: " + e.AudioPosition);  
      }  
  
      else  
      {  
        Console.WriteLine("RecognizeCompleted: No result.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded:  " + e.Grammar.Name);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognizeCompletedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerAudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan RecognizerAudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan RecognizerAudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property RecognizerAudioPosition As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan RecognizerAudioPosition { TimeSpan get(); };" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ottiene la posizione corrente di <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> nell'input audio in fase di elaborazione.</summary>
        <value>La posizione del riconoscimento nell'input audio in fase di elaborazione.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 La posizione audio è specifica per ogni riconoscimento vocale. Il valore zero di un flusso di input viene stabilito quando è abilitato.  
  
 Il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> riferimenti alle proprietà di <xref:System.Speech.Recognition.SpeechRecognitionEngine> la posizione dell'oggetto all'interno di input audio. Al contrario, il <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> proprietà fa riferimento la posizione del dispositivo di input nel proprio flusso audio generato. Queste posizioni possono essere diverse. Ad esempio, se ha ricevuto il riconoscimento di input per i quali non ha ancora generato un risultato di riconoscimento, il valore del <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> proprietà è minore del valore del <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> proprietà.  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerInfo">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognizerInfo RecognizerInfo { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.Recognition.RecognizerInfo RecognizerInfo" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property RecognizerInfo As RecognizerInfo" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::RecognizerInfo ^ RecognizerInfo { System::Speech::Recognition::RecognizerInfo ^ get(); };" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognizerInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ottiene informazioni sull'istanza corrente di <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>Informazioni sul riconoscimento vocale corrente.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Per ottenere informazioni su tutti i tipi di riconoscimento vocale installata per il sistema corrente, utilizzare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> metodo.  
  
   
  
## Examples  
 Nell'esempio seguente ottiene un elenco parziale dei dati per il motore di riconoscimento vocale nel processo corrente. Per ulteriori informazioni, vedere <xref:System.Speech.Recognition.RecognizerInfo>.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace RecognitionEngine  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer = new SpeechRecognitionEngine())  
      {  
        Console.WriteLine("Information for the current speech recognition engine:");  
        Console.WriteLine("  Name: {0}", recognizer.RecognizerInfo.Name);  
        Console.WriteLine("  Culture: {0}", recognizer.RecognizerInfo.Culture.ToString());  
        Console.WriteLine("  Description: {0}", recognizer.RecognizerInfo.Description);  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerUpdateReached">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; RecognizerUpdateReached;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; RecognizerUpdateReached" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
      <MemberSignature Language="VB.NET" Value="Public Event RecognizerUpdateReached As EventHandler(Of RecognizerUpdateReachedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::RecognizerUpdateReachedEventArgs ^&gt; ^ RecognizerUpdateReached;" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Generato quando un oggetto <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> in esecuzione viene sospeso per accettare le modifiche.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Le applicazioni devono utilizzare <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> sospendere un'istanza in esecuzione di <xref:System.Speech.Recognition.SpeechRecognitionEngine> prima di modificare le impostazioni o dai relativi <xref:System.Speech.Recognition.Grammar> oggetti. Il <xref:System.Speech.Recognition.SpeechRecognitionEngine> genera questo evento quando è pronto per accettare le modifiche.  
  
 Ad esempio, mentre il <xref:System.Speech.Recognition.SpeechRecognitionEngine> è sospesa, è possibile caricare, scaricare, abilitare e disabilitare <xref:System.Speech.Recognition.Grammar> oggetti e modificare i valori per il <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> proprietà. Per altre informazioni, vedere il metodo <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A>.  
  
 Quando si crea un delegato <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached>, si identifica il metodo che gestirà l'evento. Per associare l'evento al gestore eventi in uso, aggiungere all'evento un'istanza del delegato. Il gestore eventi viene chiamato ogni volta che si verifica l'evento, a meno che non venga rimosso il delegato. Per ulteriori informazioni sui delegati del gestore eventi, vedere [eventi e delegati](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 L'esempio seguente illustra un'applicazione console che carica e Scarica <xref:System.Speech.Recognition.Grammar> oggetti. L'applicazione utilizza il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> metodo per richiedere il motore di riconoscimento vocale per mettere in pausa per la ricezione di un aggiornamento. L'applicazione quindi carica o scarica un <xref:System.Speech.Recognition.Grammar> oggetto.  
  
 A ogni aggiornamento, un gestore per <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> evento scrive il nome e lo stato di attualmente caricato <xref:System.Speech.Recognition.Grammar> oggetti nella console. Come le grammatiche vengono caricate e scaricate, l'applicazione vengono innanzitutto i nomi di animali, i nomi di animali e i nomi di frutti e quindi solo i nomi di frutta.  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Collections.Generic;  
using System.Threading;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and configure its input.  
      using (recognizer = new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create the first grammar - Farm.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
  
        // Create the second grammar - Fruit.  
        Choices fruit = new Choices(new string[] { "apples", "peaches", "oranges" });  
        GrammarBuilder favorite = new GrammarBuilder(fruit);  
        Grammar favoriteFruit = new Grammar(favorite);  
        favoriteFruit.Name = "Fruit";  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizerUpdateReached +=  
          new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the Farm grammar.  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
        Console.WriteLine("Starting asynchronous, continuous recognition");  
        Console.WriteLine("  Farm grammar is loaded and enabled.");  
  
        // Pause to recognize farm animals.  
        Thread.Sleep(7000);  
        Console.WriteLine();  
  
        // Request an update and load the Fruit grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.LoadGrammarAsync(favoriteFruit);  
        Thread.Sleep(7000);  
  
        // Request an update and unload the Farm grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.UnloadGrammar(farmAnimals);  
        Thread.Sleep(7000);  
      }  
  
      // Keep the console window open.  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // At the update, get the names and enabled status of the currently loaded grammars.  
    public static void recognizer_RecognizerUpdateReached(  
      object sender, RecognizerUpdateReachedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Update reached:");  
      Thread.Sleep(1000);  
  
      string qualifier;  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      foreach (Grammar g in grammars)  
      {  
        qualifier = (g.Enabled) ? "enabled" : "disabled";  
        Console.WriteLine("  {0} grammar is loaded and {1}.",  
        g.Name, qualifier);  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("    Speech recognized: " + e.Result.Text);  
    }  
  
    // Write a message to the console when recognition fails.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("    Recognition attempt failed");  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognizerUpdateReachedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate" />
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
      </Docs>
    </Member>
    <MemberGroup MemberName="RequestRecognizerUpdate">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Richiede la sospensione del riconoscimento per aggiornarne lo stato.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Utilizzare questo metodo per sincronizzare le modifiche al sistema di riconoscimento. Ad esempio, se si carica o scarica una grammatica di riconoscimento vocale mentre il riconoscimento per elaborare l'input, utilizzare questo metodo e <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> eventi per la sincronizzazione del comportamento dell'applicazione con lo stato del sistema di riconoscimento.  
  
 Quando questo metodo viene chiamato, il riconoscimento viene sospeso o completamento di operazioni asincrone e genera un <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> evento. Oggetto <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> gestore eventi può quindi modificare lo stato del sistema di riconoscimento tra operazioni di riconoscimento. Quando si gestiscono <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> eventi, il riconoscimento viene sospeso fino a quando non restituisce il gestore dell'evento.  
  
> [!NOTE]
>  Se l'input per il riconoscimento viene modificato prima genera il riconoscimento di <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> evento, la richiesta viene eliminato.  
  
 Quando viene chiamato questo metodo:  
  
-   Se il riconoscimento non è l'elaborazione di input, viene generato immediatamente il riconoscimento di <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> evento.  
  
-   Se il riconoscimento è l'elaborazione di input che consiste di inattività o rumore di fondo, il riconoscimento sospende l'operazione di riconoscimento e genera il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> evento.  
  
-   Se il riconoscimento è l'elaborazione di input che non è costituito inattività o rumore di fondo, il riconoscimento completa l'operazione di riconoscimento e quindi genera il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> evento.  
  
 Mentre il riconoscimento sta gestendo il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> evento:  
  
-   Il riconoscimento non elabora l'input e il valore della <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> proprietà rimane invariato.  
  
-   Il riconoscimento continua a raccogliere input e il valore di <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> proprietà può essere modificata.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate();" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Richiede la sospensione del riconoscimento per aggiornarne lo stato.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Quando viene generato il riconoscimento di <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> evento, il <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> proprietà del <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> è `null`.  
  
 Per fornire un token dell'utente, utilizzare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> metodo. Per specificare un offset di posizione audio, usare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> metodo.  
  
   
  
## Examples  
 L'esempio seguente illustra un'applicazione console che carica e Scarica <xref:System.Speech.Recognition.Grammar> oggetti. L'applicazione utilizza il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> metodo per richiedere il motore di riconoscimento vocale per mettere in pausa per la ricezione di un aggiornamento. L'applicazione quindi carica o scarica un <xref:System.Speech.Recognition.Grammar> oggetto.  
  
 A ogni aggiornamento, un gestore per <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> evento scrive il nome e lo stato di attualmente caricato <xref:System.Speech.Recognition.Grammar> oggetti nella console. Come le grammatiche vengono caricate e scaricate, l'applicazione vengono innanzitutto i nomi di animali, i nomi di animali e i nomi di frutti e quindi solo i nomi di frutta.  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Collections.Generic;  
using System.Threading;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and configure its input.  
      using (recognizer = new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create the first grammar - Farm.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
  
        // Create the second grammar - Fruit.  
        Choices fruit = new Choices(new string[] { "apples", "peaches", "oranges" });  
        GrammarBuilder favorite = new GrammarBuilder(fruit);  
        Grammar favoriteFruit = new Grammar(favorite);  
        favoriteFruit.Name = "Fruit";  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizerUpdateReached +=  
          new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the Farm grammar.  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
        Console.WriteLine("Starting asynchronous, continuous recognition");  
        Console.WriteLine("  Farm grammar is loaded and enabled.");  
  
        // Pause to recognize farm animals.  
        Thread.Sleep(7000);  
        Console.WriteLine();  
  
        // Request an update and load the Fruit grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.LoadGrammarAsync(favoriteFruit);  
        Thread.Sleep(7000);  
  
        // Request an update and unload the Farm grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.UnloadGrammar(farmAnimals);  
        Thread.Sleep(7000);  
      }  
  
      // Keep the console window open.  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // At the update, get the names and enabled status of the currently loaded grammars.  
    public static void recognizer_RecognizerUpdateReached(  
      object sender, RecognizerUpdateReachedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Update reached:");  
      Thread.Sleep(1000);  
  
      string qualifier;  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      foreach (Grammar g in grammars)  
      {  
        qualifier = (g.Enabled) ? "enabled" : "disabled";  
        Console.WriteLine("  {0} grammar is loaded and {1}.",  
        g.Name, qualifier);  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("    Speech recognized: " + e.Result.Text);  
    }  
  
    // Write a message to the console when recognition fails.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("    Recognition attempt failed");  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate (object userToken);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate(object userToken) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate(System.Object)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate (userToken As Object)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate(System::Object ^ userToken);" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="userToken" Type="System.Object" />
      </Parameters>
      <Docs>
        <param name="userToken">Informazioni definite dall'utente che contengono informazioni per l'operazione.</param>
        <summary>Richiede la sospensione del riconoscimento per aggiornarne lo stato e fornire un token utente per l'evento associato.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Quando viene generato il riconoscimento di <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> evento, il <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> proprietà del <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> contiene il valore del `userToken` parametro.  
  
 Per specificare un offset di posizione audio, usare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> metodo.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate (object userToken, TimeSpan audioPositionAheadToRaiseUpdate);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate(object userToken, valuetype System.TimeSpan audioPositionAheadToRaiseUpdate) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate(System.Object,System.TimeSpan)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate (userToken As Object, audioPositionAheadToRaiseUpdate As TimeSpan)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate(System::Object ^ userToken, TimeSpan audioPositionAheadToRaiseUpdate);" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="userToken" Type="System.Object" />
        <Parameter Name="audioPositionAheadToRaiseUpdate" Type="System.TimeSpan" />
      </Parameters>
      <Docs>
        <param name="userToken">Informazioni definite dall'utente che contengono informazioni per l'operazione.</param>
        <param name="audioPositionAheadToRaiseUpdate">L'offset dall'oggetto <see cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" /> corrente per ritardare la richiesta.</param>
        <summary>Richiede la sospensione del riconoscimento per aggiornarne lo stato e fornire un offset e un token utente per l'evento associato.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il riconoscimento non avvierà la richiesta di aggiornamento per il riconoscimento fino a quando il riconoscimento <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> corrente è uguale a <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> più `audioPositionAheadToRaiseUpdate`.  
  
 Quando viene generato il riconoscimento di <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> evento, il <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> proprietà del <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> contiene il valore del `userToken` parametro.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToAudioStream">
      <MemberSignature Language="C#" Value="public void SetInputToAudioStream (System.IO.Stream audioSource, System.Speech.AudioFormat.SpeechAudioFormatInfo audioFormat);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToAudioStream(class System.IO.Stream audioSource, class System.Speech.AudioFormat.SpeechAudioFormatInfo audioFormat) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToAudioStream (audioSource As Stream, audioFormat As SpeechAudioFormatInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToAudioStream(System::IO::Stream ^ audioSource, System::Speech::AudioFormat::SpeechAudioFormatInfo ^ audioFormat);" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="audioSource" Type="System.IO.Stream" />
        <Parameter Name="audioFormat" Type="System.Speech.AudioFormat.SpeechAudioFormatInfo" />
      </Parameters>
      <Docs>
        <param name="audioSource">Flusso di input audio.</param>
        <param name="audioFormat">Il formato dell'input audio.</param>
        <summary>Configura l'oggetto <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> per ricevere l'input da un flusso audio.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Se il riconoscimento raggiunge la fine del flusso di input durante un'operazione di riconoscimento, l'operazione di riconoscimento completa con l'input disponibile. Operazioni successive riconoscimento è possono generare un'eccezione, a meno che non si aggiorna l'input per il riconoscimento.  
  
   
  
## Examples  
 Nell'esempio seguente viene illustrata parte di un'applicazione console che illustra il riconoscimento vocale base. Nell'esempio viene utilizzato l'input da un file audio, example.wav, che contiene le frasi, "test o test uno due tre" e "vendita cooper", separati da una pausa. L'esempio genera l'output seguente.  
  
```  
  
Starting asynchronous recognition...  
  Recognized text =  Testing testing 123  
  Recognized text =  Mr. Cooper  
  End of stream encountered.  
Done.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.IO;  
using System.Speech.AudioFormat;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace InputExamples  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Create and load a grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Configure the input to the recognizer.  
        recognizer.SetInputToAudioStream(  
          File.OpenRead(@"c:\temp\audioinput\example.wav"),  
          new SpeechAudioFormatInfo(  
            44100, AudioBitsPerSample.Sixteen, AudioChannel.Mono));  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Perform recognition of the whole file.  
        Console.WriteLine("Starting asynchronous recognition...");  
        completed = false;  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine("  Error encountered, {0}: {1}",  
          e.Error.GetType().Name, e.Error.Message);  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine("  End of stream encountered.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToDefaultAudioDevice">
      <MemberSignature Language="C#" Value="public void SetInputToDefaultAudioDevice ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToDefaultAudioDevice() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToDefaultAudioDevice ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToDefaultAudioDevice();" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Configura l'oggetto <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> per ricevere l'input dal dispositivo audio predefinito.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Examples  
 Nell'esempio seguente viene illustrata parte di un'applicazione console che illustra il riconoscimento vocale base. L'esempio utilizza l'output dal dispositivo audio predefinito, vengono eseguite più operazioni di riconoscimento asincrono e viene chiuso quando un utente utters la frase "exit".  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace DefaultInput  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition has finished.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Create and load the exit grammar.  
        Grammar exitGrammar = new Grammar(new GrammarBuilder("exit"));  
        exitGrammar.Name = "Exit Grammar";  
        recognizer.LoadGrammar(exitGrammar);  
  
        // Create and load the dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers to the recognizer.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Begin asynchronous recognition.  
        Console.WriteLine("Starting recognition...");  
        completed = false;  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait for recognition to finish.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("  Speech recognized:");  
      string grammarName = "<not available>";  
      if (e.Result.Grammar.Name != null &&  
        !e.Result.Grammar.Name.Equals(string.Empty))  
      {  
        grammarName = e.Result.Grammar.Name;  
      }  
      Console.WriteLine("    {0,-17} - {1}",  
        grammarName, e.Result.Text);  
  
      if (grammarName.Equals("Exit Grammar"))  
      {  
        ((SpeechRecognitionEngine)sender).RecognizeAsyncCancel();  
      }  
    }  
  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("  Recognition completed.");  
      completed = true;  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToNull">
      <MemberSignature Language="C#" Value="public void SetInputToNull ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToNull() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToNull ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToNull();" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Disabilita l'input del riconoscimento vocale.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Configurare il <xref:System.Speech.Recognition.SpeechRecognitionEngine> oggetto per nessun input quando si utilizza il <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> e <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> metodi, o quando si acquisisce un motore di riconoscimento temporaneamente non in linea.  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToWaveFile">
      <MemberSignature Language="C#" Value="public void SetInputToWaveFile (string path);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToWaveFile(string path) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToWaveFile (path As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToWaveFile(System::String ^ path);" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="path" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="path">Percorso del file da utilizzare come input.</param>
        <summary>Configura l'oggetto <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> per ricevere l'input da un file in formato audio Waveform (.wav).</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Se il riconoscimento raggiunge la fine del file di input durante un'operazione di riconoscimento, l'operazione di riconoscimento completa con l'input disponibile. Operazioni successive riconoscimento è possono generare un'eccezione, a meno che non si aggiorna l'input per il riconoscimento.  
  
   
  
## Examples  
 Nell'esempio seguente esegue il riconoscimento all'audio in un file. wav e scrive il testo riconosciuto nella console.  
  
```  
using System;  
using System.IO;  
using System.Speech.Recognition;  
using System.Speech.AudioFormat;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static bool completed;  
  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create and load a grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Configure the input to the recognizer.  
recognizer.SetInputToWaveFile(@"c:\temp\SampleWAVInput.wav");  
  
        // Attach event handlers for the results of recognition.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizeCompleted +=   
          new EventHandler<RecognizeCompletedEventArgs>(recognizer_RecognizeCompleted);  
  
        // Perform recognition on the entire file.  
        Console.WriteLine("Starting asynchronous recognition...");  
        completed = false;  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        while (!completed)  
        {  
          Console.ReadLine();  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void recognizer_RecognizeCompleted(object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine("  Error encountered, {0}: {1}",  
        e.Error.GetType().Name, e.Error.Message);  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine("  End of stream encountered.");  
      }  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToWaveStream">
      <MemberSignature Language="C#" Value="public void SetInputToWaveStream (System.IO.Stream audioSource);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToWaveStream(class System.IO.Stream audioSource) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToWaveStream (audioSource As Stream)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToWaveStream(System::IO::Stream ^ audioSource);" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="audioSource" Type="System.IO.Stream" />
      </Parameters>
      <Docs>
        <param name="audioSource">Flusso contenente i dati audio.</param>
        <summary>Configura l'oggetto <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> per ricevere l'input da un flusso che contiene i dati in formato audio Waveform (.wav).</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Se il riconoscimento raggiunge la fine del flusso di input durante un'operazione di riconoscimento, l'operazione di riconoscimento completa con l'input disponibile. Operazioni successive riconoscimento è possono generare un'eccezione, a meno che non si aggiorna l'input per il riconoscimento.  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SpeechDetected">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; SpeechDetected;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechDetectedEventArgs&gt; SpeechDetected" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechDetected As EventHandler(Of SpeechDetectedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechDetectedEventArgs ^&gt; ^ SpeechDetected;" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Generato quando <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> rileva un input identificabile come funzione vocale.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ogni riconoscimento vocale è disponibile un algoritmo per distinguere tra vocale e di inattività. Quando il <xref:System.Speech.Recognition.SpeechRecognitionEngine> esegue un'operazione di riconoscimento vocale, genera il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> evento quando l'algoritmo identifica l'input come riconoscimento vocale. Il <xref:System.Speech.Recognition.SpeechDetectedEventArgs.AudioPosition%2A> proprietà dell'oggetto associato <xref:System.Speech.Recognition.SpeechDetectedEventArgs> oggetto indica una posizione nel flusso di input in cui il riconoscimento per rilevata il riconoscimento vocale. Il <xref:System.Speech.Recognition.SpeechRecognitionEngine> genera il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> evento prima che venga generato uno qualsiasi del <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>, o <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> eventi.  
  
 Per ulteriori informazioni, vedere il <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> metodi.  
  
 Quando si crea un delegato <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, si identifica il metodo che gestirà l'evento. Per associare l'evento al gestore eventi in uso, aggiungere all'evento un'istanza del delegato. Il gestore eventi viene chiamato ogni volta che si verifica l'evento, a meno che non venga rimosso il delegato. Per ulteriori informazioni sui delegati del gestore eventi, vedere [eventi e delegati](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Nell'esempio seguente fa parte di un'applicazione console per la scelta di origine e destinazione città per un volo. L'applicazione riconosce frasi, ad esempio "Desidero entrata da Miami a Chicago."  Nell'esempio viene utilizzato il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> evento report il <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> viene rilevato il parlato ogni ora.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create a grammar.  
        Choices cities = new Choices(new string[] {   
          "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I would like to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Create a Grammar object and load it to the recognizer.  
        Grammar g = new Grammar(gb);  
        g.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(g);  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechDetected event.  
    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine("  Speech detected at AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("  Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="SpeechHypothesized">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; SpeechHypothesized;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; SpeechHypothesized" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event SpeechHypothesized As EventHandler(Of SpeechHypothesizedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechHypothesizedEventArgs ^&gt; ^ SpeechHypothesized;" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Generato quando <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> ha riconosciuto una o più parole che possono essere componenti di più frasi complete in una grammatica.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il <xref:System.Speech.Recognition.SpeechRecognitionEngine> genera numerosi <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> eventi perché tenta di identificare una frase di input. È possibile accedere al testo di frasi parzialmente riconosciute nel <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> proprietà del <xref:System.Speech.Recognition.SpeechHypothesizedEventArgs> oggetto nel gestore per il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> evento. In genere, la gestione di questi eventi è utile solo per il debug.  
  
 <xref:System.Speech.Recognition.SpeechHypothesizedEventArgs> deriva da <xref:System.Speech.Recognition.RecognitionEventArgs>.  
  
 Per ulteriori informazioni, vedere il <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> proprietà e <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> metodi.  
  
 Quando si crea un delegato <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, si identifica il metodo che gestirà l'evento. Per associare l'evento al gestore eventi in uso, aggiungere all'evento un'istanza del delegato. Il gestore eventi viene chiamato ogni volta che si verifica l'evento, a meno che non venga rimosso il delegato. Per ulteriori informazioni sui delegati del gestore eventi, vedere [eventi e delegati](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Nell'esempio seguente riconosce frasi, ad esempio "Visualizza l'elenco degli artisti nella categoria jazz". Nell'esempio viene utilizzato il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> evento per visualizzare i frammenti di frase incompleto nella console di come vengono riconosciuti.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create a grammar.  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display the list of");  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the");  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category.");  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(recognizer_SpeechHypothesized);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void recognizer_SpeechHypothesized(object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine("Speech hypothesized: " + e.Result.Text);  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
      Console.WriteLine();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine();   
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="T:System.Speech.Recognition.RecognitionEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="SpeechRecognitionRejected">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; SpeechRecognitionRejected;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; SpeechRecognitionRejected" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechRecognitionRejected As EventHandler(Of SpeechRecognitionRejectedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechRecognitionRejectedEventArgs ^&gt; ^ SpeechRecognitionRejected;" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Generato quando <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> riceve input che non corrisponde ad alcun oggetto <see cref="T:System.Speech.Recognition.Grammar" /> caricato e abilitato.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 L'evento viene generato il riconoscimento, se rileva che input non corrisponde con sufficiente sicurezza tutti i relativi caricati e abilitati <xref:System.Speech.Recognition.Grammar> oggetti. Il <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> proprietà del <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> contiene il rifiutati <xref:System.Speech.Recognition.RecognitionResult> oggetto. È possibile utilizzare il gestore per il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> evento per recuperare il riconoscimento <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> che sono stati rifiutati e il loro <xref:System.Speech.Recognition.RecognizedPhrase.Confidence%2A> punteggi.  
  
 Se l'applicazione utilizza un <xref:System.Speech.Recognition.SpeechRecognitionEngine> istanza, è possibile modificare il livello di confidenza quali sintesi e riconoscimento vocale è è accettato o rifiutato con uno degli input di <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> metodi. È possibile modificare una modalità di risposta il riconoscimento vocale non vocale input mediante la <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> proprietà.  
  
 Quando si crea un delegato <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, si identifica il metodo che gestirà l'evento. Per associare l'evento al gestore eventi in uso, aggiungere all'evento un'istanza del delegato. Il gestore eventi viene chiamato ogni volta che si verifica l'evento, a meno che non venga rimosso il delegato. Per ulteriori informazioni sui delegati del gestore eventi, vedere [eventi e delegati](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Nell'esempio seguente riconosce frasi, ad esempio "Visualizzare l'elenco degli artisti nella categoria jazz" o "Gospel album visualizzare". Nell'esempio viene utilizzato un gestore per il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> evento per visualizzare una notifica nella console quando il riconoscimento vocale per l'input non può corrispondere al contenuto della grammatica con sufficienti <xref:System.Speech.Recognition.RecognizedPhrase.Confidence%2A> per produrre un riconoscimento ha esito positivo. Il gestore visualizza anche i risultati di riconoscimento <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> che sono stati rifiutati a causa di punteggi di confidenza basso.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create a grammar.  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display");  
        mediaMenu.Append("the list of", 0, 1);  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the", 0, 1);  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category", 0, 1);  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("Speech input was rejected.");  
      foreach (RecognizedPhrase phrase in e.Result.Alternates)  
      {  
      Console.WriteLine("  Rejected phrase: " + phrase.Text);  
      Console.WriteLine("  Confidence score: " + phrase.Confidence);  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
      Console.WriteLine("  Confidence score: " + e.Result.Confidence);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="SpeechRecognized">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; SpeechRecognized;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechRecognizedEventArgs&gt; SpeechRecognized" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechRecognized As EventHandler(Of SpeechRecognizedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechRecognizedEventArgs ^&gt; ^ SpeechRecognized;" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Generato quando <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> riceve input che corrisponde a un oggetto <see cref="T:System.Speech.Recognition.Grammar" /> caricato e abilitato.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 È possibile avviare un'operazione di riconoscimento utilizzando uno del <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> metodi. Genera il riconoscimento di <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> evento se determina che input corrisponde a uno dei relativi caricato <xref:System.Speech.Recognition.Grammar> oggetti con un livello di confidenza al riconoscimento sufficiente. Il <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> proprietà del <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> contiene accettata <xref:System.Speech.Recognition.RecognitionResult> oggetto. Gestori di <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> possono ottenere gli eventi alla frase riconosciuta, nonché un elenco di riconoscimento <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> con punteggi di confidenza inferiore.  
  
 Se l'applicazione utilizza un <xref:System.Speech.Recognition.SpeechRecognitionEngine> istanza, è possibile modificare il livello di confidenza quali sintesi e riconoscimento vocale è è accettato o rifiutato con uno degli input di <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> metodi.  È possibile modificare una modalità di risposta il riconoscimento vocale non vocale input mediante la <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> proprietà.  
  
 Quando il riconoscimento riceve l'input che corrisponde a una grammatica, il <xref:System.Speech.Recognition.Grammar> può generare l'oggetto relativo <xref:System.Speech.Recognition.Grammar.SpeechRecognized> evento. Il <xref:System.Speech.Recognition.Grammar> dell'oggetto <xref:System.Speech.Recognition.Grammar.SpeechRecognized> evento viene generato prima del riconoscimento vocale <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> evento. Qualsiasi attività specifiche di una particolare grammatica dovrebbe sempre essere eseguita da un gestore per il <xref:System.Speech.Recognition.Grammar.SpeechRecognized> evento.  
  
 Quando si crea un delegato <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>, si identifica il metodo che gestirà l'evento. Per associare l'evento al gestore eventi in uso, aggiungere all'evento un'istanza del delegato. Il gestore eventi viene chiamato ogni volta che si verifica l'evento, a meno che non venga rimosso il delegato. Per ulteriori informazioni sui delegati del gestore eventi, vedere [eventi e delegati](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Nell'esempio seguente fa parte di un'applicazione console che crea la grammatica di riconoscimento vocale, costrutti un <xref:System.Speech.Recognition.Grammar> dell'oggetto e viene caricato il <xref:System.Speech.Recognition.SpeechRecognitionEngine> per eseguire il riconoscimento. Nell'esempio viene illustrato l'input vocale per un <xref:System.Speech.Recognition.SpeechRecognitionEngine>, i risultati di riconoscimento associati e gli eventi associati generati dal riconoscimento vocale.  
  
 Leggere l'input, ad esempio "Desidero entrata da Chicago a Miami" attiverà una <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> evento. Parlando la frase "Entrata me da Houston a Chicago" non attiverà una <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> evento.  
  
 Nell'esempio viene utilizzato un gestore per il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> evento per visualizzare correttamente riconosciuto frasi e la semantica contengono nella console.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer = new SpeechRecognitionEngine())  
      {  
  
        // Create SemanticResultValue objects that contain cities and airport codes.  
        SemanticResultValue chicago = new SemanticResultValue("Chicago", "ORD");  
        SemanticResultValue boston = new SemanticResultValue("Boston", "BOS");  
        SemanticResultValue miami = new SemanticResultValue("Miami", "MIA");  
        SemanticResultValue dallas = new SemanticResultValue("Dallas", "DFW");  
  
        // Create a Choices object and add the SemanticResultValue objects, using  
        // implicit conversion from SemanticResultValue to GrammarBuilder  
        Choices cities = new Choices();  
        cities.Add(new Choices(new GrammarBuilder[] { chicago, boston, miami, dallas }));  
  
        // Build the phrase and add SemanticResultKeys.  
        GrammarBuilder chooseCities = new GrammarBuilder();  
        chooseCities.Append("I want to fly from");  
        chooseCities.Append(new SemanticResultKey("origin", cities));  
        chooseCities.Append("to");  
        chooseCities.Append(new SemanticResultKey("destination", cities));  
  
        // Build a Grammar object from the GrammarBuilder.  
        Grammar bookFlight = new Grammar(chooseCities);  
        bookFlight.Name = "Book Flight";  
  
        // Add a handler for the LoadGrammarCompleted event.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Add a handler for the SpeechRecognized event.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(bookFlight);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
      Console.WriteLine();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized:  " + e.Result.Text);  
      Console.WriteLine();  
      Console.WriteLine("Semantic results:");  
      Console.WriteLine("  The flight origin is " + e.Result.Semantics["origin"].Value);  
      Console.WriteLine("  The flight destination is " + e.Result.Semantics["destination"].Value);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="T:System.Speech.Recognition.RecognitionEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="UnloadAllGrammars">
      <MemberSignature Language="C#" Value="public void UnloadAllGrammars ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UnloadAllGrammars() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
      <MemberSignature Language="VB.NET" Value="Public Sub UnloadAllGrammars ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UnloadAllGrammars();" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Scarica tutti gli oggetti <see cref="T:System.Speech.Recognition.Grammar" /> dal riconoscimento.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Se il riconoscimento sta attualmente caricando un <xref:System.Speech.Recognition.Grammar> in modo asincrono, questo metodo attende fino a quando il <xref:System.Speech.Recognition.Grammar> viene caricato, prima che venga scaricato tutti il <xref:System.Speech.Recognition.Grammar> oggetti dal <xref:System.Speech.Recognition.SpeechRecognitionEngine> istanza.  
  
 Per scaricare una grammatica specifica, utilizzare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar%2A> metodo.  
  
   
  
## Examples  
 Nell'esempio seguente viene illustrata parte di un'applicazione console che illustra il caricamento sincrono e scaricamento di grammatiche riconoscimento vocale.  
  
```  
Loading grammars...  
Loaded grammars:  
 - Grammar1  
 - Grammar2  
 - Grammar3  
  
Unloading Grammar1...  
Loaded grammars:  
 - Grammar2  
 - Grammar3  
  
Unloading all grammars...  
No grammars loaded.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Collections.Generic;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace UnloadGrammars  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        Console.WriteLine("Loading grammars...");  
  
        // Create and load a number of grammars.  
        Grammar grammar1 = new Grammar(new GrammarBuilder("first grammar"));  
        grammar1.Name = "Grammar1";  
        recognizer.LoadGrammar(grammar1);  
  
        Grammar grammar2 = new Grammar(new GrammarBuilder("second grammar"));  
        grammar2.Name = "Grammar2";  
        recognizer.LoadGrammar(grammar2);  
  
        Grammar grammar3 = new Grammar(new GrammarBuilder("third grammar"));  
        grammar3.Name = "Grammar3";  
        recognizer.LoadGrammar(grammar3);  
  
        // List the recognizer's loaded grammars.  
        ListGrammars(recognizer);  
  
        // Unload one grammar and list the loaded grammars.  
        Console.WriteLine("Unloading Grammar1...");  
        recognizer.UnloadGrammar(grammar1);  
        ListGrammars(recognizer);  
  
        // Unload all grammars and list the loaded grammars.  
        Console.WriteLine("Unloading all grammars...");  
        recognizer.UnloadAllGrammars();  
        ListGrammars(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListGrammars(SpeechRecognitionEngine recognizer)  
    {  
      // Make a copy of the recognizer's grammar collection.  
      List<Grammar> loadedGrammars = new List<Grammar>(recognizer.Grammars);  
  
      if (loadedGrammars.Count > 0)  
      {  
        Console.WriteLine("Loaded grammars:");  
        foreach (Grammar g in recognizer.Grammars)  
        {  
          Console.WriteLine(" - {0}", g.Name);  
        }  
      }  
      else  
      {  
        Console.WriteLine("No grammars loaded.");  
      }  
      Console.WriteLine();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="UnloadGrammar">
      <MemberSignature Language="C#" Value="public void UnloadGrammar (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UnloadGrammar(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UnloadGrammar(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">L'oggetto di grammatica di cui annullare il caricamento.</param>
        <summary>Scarica un oggetto <see cref="T:System.Speech.Recognition.Grammar" /> specificato dall'istanza <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Se il riconoscimento è in esecuzione, le applicazioni devono utilizzare <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> per sospendere il <xref:System.Speech.Recognition.SpeechRecognitionEngine> istanza prima del caricamento, scaricamento, abilitazione o disabilitazione di un <xref:System.Speech.Recognition.Grammar> oggetto. Per scaricare tutti <xref:System.Speech.Recognition.Grammar> oggetti, utilizzare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars%2A> metodo.  
  
   
  
## Examples  
 Nell'esempio seguente viene illustrata parte di un'applicazione console che illustra il caricamento sincrono e scaricamento di grammatiche riconoscimento vocale.  
  
```  
Loading grammars...  
Loaded grammars:  
 - Grammar1  
 - Grammar2  
 - Grammar3  
  
Unloading Grammar1...  
Loaded grammars:  
 - Grammar2  
 - Grammar3  
  
Unloading all grammars...  
No grammars loaded.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Collections.Generic;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace UnloadGrammars  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        Console.WriteLine("Loading grammars...");  
  
        // Create and load a number of grammars.  
        Grammar grammar1 = new Grammar(new GrammarBuilder("first grammar"));  
        grammar1.Name = "Grammar1";  
        recognizer.LoadGrammar(grammar1);  
  
        Grammar grammar2 = new Grammar(new GrammarBuilder("second grammar"));  
        grammar2.Name = "Grammar2";  
        recognizer.LoadGrammar(grammar2);  
  
        Grammar grammar3 = new Grammar(new GrammarBuilder("third grammar"));  
        grammar3.Name = "Grammar3";  
        recognizer.LoadGrammar(grammar3);  
  
        // List the recognizer's loaded grammars.  
        ListGrammars(recognizer);  
  
        // Unload one grammar and list the loaded grammars.  
        Console.WriteLine("Unloading Grammar1...");  
        recognizer.UnloadGrammar(grammar1);  
        ListGrammars(recognizer);  
  
        // Unload all grammars and list the loaded grammars.  
        Console.WriteLine("Unloading all grammars...");  
        recognizer.UnloadAllGrammars();  
        ListGrammars(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListGrammars(SpeechRecognitionEngine recognizer)  
    {  
      // Make a copy of the recognizer's grammar collection.  
      List<Grammar> loadedGrammars = new List<Grammar>(recognizer.Grammars);  
  
      if (loadedGrammars.Count > 0)  
      {  
        Console.WriteLine("Loaded grammars:");  
        foreach (Grammar g in recognizer.Grammars)  
        {  
          Console.WriteLine(" - {0}", g.Name);  
        }  
      }  
      else  
      {  
        Console.WriteLine("No grammars loaded.");  
      }  
      Console.WriteLine();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="Grammar" /> è <see langword="null" />.</exception>
        <exception cref="T:System.InvalidOperationException">La grammatica non viene caricata nel sistema di riconoscimento o il sistema di riconoscimento sta al momento caricando la grammatica in modo asincrono.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
      </Docs>
    </Member>
    <MemberGroup MemberName="UpdateRecognizerSetting">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Aggiorna il valore di un'impostazione per il riconoscimento.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Le impostazioni per il riconoscimento possono contenere una stringa, intero a 64 bit o dati di indirizzo di memoria. Nella tabella seguente vengono descritte le impostazioni definite per un riconoscimento vocale API di Microsoft (SAPI)-riconoscimento conformo. Le seguenti impostazioni devono avere lo stesso intervallo per ogni riconoscimento che supporta l'impostazione. Un riconoscimento conformi SAPI non è necessario per supportare queste impostazioni e può supportare altre impostazioni.  
  
|nome|Descrizione|  
|----------|-----------------|  
|`ResourceUsage`|Specifica l'utilizzo della CPU del riconoscimento. L'intervallo è compreso tra 0 e 100. Il valore predefinito è 50.|  
|`ResponseSpeed`|Indica la lunghezza di inattività alla fine dell'input non ambiguo prima che il riconoscimento vocale viene completata un'operazione di riconoscimento. L'intervallo è compreso tra 0 e 10.000 millisecondi (ms). Questa impostazione corrisponde al riconoscimento <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> proprietà. Predefinito = 150 ms.|  
|`ComplexResponseSpeed`|Indica la lunghezza di inattività in millisecondi (ms) alla fine dell'input ambiguo prima che il riconoscimento vocale viene completata un'operazione di riconoscimento. L'intervallo è compreso tra 0 e 10.000 ms. Questa impostazione corrisponde al riconoscimento <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> proprietà. Predefinito = 500ms.|  
|`AdaptationOn`|Indica se l'adattamento del modello acustico è ON (valore = `1`) o OFF (valore = `0`). Il valore predefinito è `1` (ON).|  
|`PersistedBackgroundAdaptation`|Indica se l'adattamento è ON (valore = `1`) o OFF (valore = `0`), e viene mantenuta l'impostazione del Registro di sistema. Il valore predefinito è `1` (ON).|  
  
 Per restituire una delle impostazioni del riconoscimento, utilizzare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting%2A> metodo.  
  
 Ad eccezione di `PersistedBackgroundAdaptation`, i valori di proprietà impostati mediante il <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> metodi rimangono valide solo per l'istanza corrente di <xref:System.Speech.Recognition.SpeechRecognitionEngine>, dopo cui ripristinare le impostazioni predefinite.  
  
 È possibile modificare una modalità di risposta il riconoscimento vocale non vocale input mediante la <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> proprietà.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="UpdateRecognizerSetting">
      <MemberSignature Language="C#" Value="public void UpdateRecognizerSetting (string settingName, int updatedValue);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UpdateRecognizerSetting(string settingName, int32 updatedValue) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.Int32)" />
      <MemberSignature Language="VB.NET" Value="Public Sub UpdateRecognizerSetting (settingName As String, updatedValue As Integer)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UpdateRecognizerSetting(System::String ^ settingName, int updatedValue);" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
        <Parameter Name="updatedValue" Type="System.Int32" />
      </Parameters>
      <Docs>
        <param name="settingName">Nome dell'impostazione da aggiornare.</param>
        <param name="updatedValue">Il nuovo valore per l'impostazione.</param>
        <summary>Aggiorna l'impostazione specificata per <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> con il valore Integer specificato.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ad eccezione di `PersistedBackgroundAdaptation`, i valori di proprietà impostati mediante il <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> metodo rimangono valide solo per l'istanza corrente di <xref:System.Speech.Recognition.SpeechRecognitionEngine>, dopo cui ripristinare le impostazioni predefinite. Vedere <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> per le descrizioni delle impostazioni supportate.  
  
   
  
## Examples  
 Nell'esempio seguente fa parte di un'applicazione console che restituisce i valori per un numero di impostazioni definite per il riconoscimento che supporta le impostazioni locali en-US. L'esempio aggiorna le impostazioni del livello di confidenza e successivamente sottoposto a query il riconoscimento per controllare i valori aggiornati. L'esempio genera l'output seguente.  
  
```  
Settings for recognizer MS-1033-80-DESK:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 150  
  ComplexResponseSpeed           = 500  
  AdaptationOn                   = 1  
  PersistedBackgroundAdaptation  = 1  
  
Updated settings:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 200  
  ComplexResponseSpeed           = 300  
  AdaptationOn                   = 0  
  PersistedBackgroundAdaptation  = 0  
  
Press any key to exit...  
```  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace RecognizerSettings  
{  
  class Program  
  {  
    static readonly string[] settings = new string[] {  
      "ResourceUsage",  
      "ResponseSpeed",  
      "ComplexResponseSpeed",  
      "AdaptationOn",  
      "PersistedBackgroundAdaptation",  
    };  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        Console.WriteLine("Settings for recognizer {0}:",  
          recognizer.RecognizerInfo.Name);  
        Console.WriteLine();  
  
        // List the current settings.  
        ListSettings(recognizer);  
  
        // Change some of the settings.  
        recognizer.UpdateRecognizerSetting("ResponseSpeed", 200);  
        recognizer.UpdateRecognizerSetting("ComplexResponseSpeed", 300);  
        recognizer.UpdateRecognizerSetting("AdaptationOn", 1);  
        recognizer.UpdateRecognizerSetting("PersistedBackgroundAdaptation", 0);  
  
        Console.WriteLine("Updated settings:");  
        Console.WriteLine();  
  
        // List the updated settings.  
        ListSettings(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListSettings(SpeechRecognitionEngine recognizer)  
    {  
      foreach (string setting in settings)  
      {  
        try  
        {  
          object value = recognizer.QueryRecognizerSetting(setting);  
          Console.WriteLine("  {0,-30} = {1}", setting, value);  
        }  
        catch  
        {  
          Console.WriteLine("  {0,-30} is not supported by this recognizer.",  
            setting);  
        }  
      }  
      Console.WriteLine();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="settingName" /> è <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="settingName" /> è la stringa vuota ("").</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">Il riconoscimento non ha un'impostazione con tale nome.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="UpdateRecognizerSetting">
      <MemberSignature Language="C#" Value="public void UpdateRecognizerSetting (string settingName, string updatedValue);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UpdateRecognizerSetting(string settingName, string updatedValue) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub UpdateRecognizerSetting (settingName As String, updatedValue As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UpdateRecognizerSetting(System::String ^ settingName, System::String ^ updatedValue);" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
        <Parameter Name="updatedValue" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="settingName">Nome dell'impostazione da aggiornare.</param>
        <param name="updatedValue">Il nuovo valore per l'impostazione.</param>
        <summary>Aggiorna l'impostazione specificata del motore di riconoscimento vocale con il valore di stringa specificato.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ad eccezione di `PersistedBackgroundAdaptation`, i valori di proprietà impostati mediante il <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> metodo rimangono valide solo per l'istanza corrente di <xref:System.Speech.Recognition.SpeechRecognitionEngine>, dopo cui ripristinare le impostazioni predefinite. Vedere <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> per le descrizioni delle impostazioni supportate.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="settingName" /> è <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="settingName" /> è la stringa vuota ("").</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">Il riconoscimento non ha un'impostazione con tale nome.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      </Docs>
    </Member>
  </Members>
</Type>
<Type Name="SpeechRecognitionEngine" FullName="System.Speech.Recognition.SpeechRecognitionEngine">
  <Metadata>
    <Meta Name="ms.openlocfilehash" Value="4a68990136b4c4a3e7465b21d6054268587b7843" />
    <Meta Name="ms.sourcegitcommit" Value="16d2d159872fd213cae4b8f371d7ae9c8b027c89" />
    <Meta Name="ms.translationtype" Value="HT" />
    <Meta Name="ms.contentlocale" Value="it-IT" />
    <Meta Name="ms.lasthandoff" Value="11/17/2018" />
    <Meta Name="ms.locfileid" Value="51894597" />
  </Metadata>
  <TypeSignature Language="C#" Value="public class SpeechRecognitionEngine : IDisposable" />
  <TypeSignature Language="ILAsm" Value=".class public auto ansi beforefieldinit SpeechRecognitionEngine extends System.Object implements class System.IDisposable" />
  <TypeSignature Language="DocId" Value="T:System.Speech.Recognition.SpeechRecognitionEngine" />
  <TypeSignature Language="VB.NET" Value="Public Class SpeechRecognitionEngine&#xA;Implements IDisposable" />
  <TypeSignature Language="C++ CLI" Value="public ref class SpeechRecognitionEngine : IDisposable" />
  <TypeSignature Language="F#" Value="type SpeechRecognitionEngine = class&#xA;    interface IDisposable" />
  <AssemblyInfo>
    <AssemblyName>System.Speech</AssemblyName>
    <AssemblyVersion>3.0.0.0</AssemblyVersion>
    <AssemblyVersion>4.0.0.0</AssemblyVersion>
  </AssemblyInfo>
  <Base>
    <BaseTypeName>System.Object</BaseTypeName>
  </Base>
  <Interfaces>
    <Interface>
      <InterfaceName>System.IDisposable</InterfaceName>
    </Interface>
  </Interfaces>
  <Docs>
    <summary>Fornisce i mezzi per accedere e gestire un motore di riconoscimento vocale in-process.</summary>
    <remarks>
      <format type="text/markdown"><![CDATA[  
  
## Remarks  
 È possibile creare un'istanza di questa classe per uno qualsiasi dei sistemi di riconoscimento vocale installato. Per ottenere informazioni sui tipi di riconoscimento vengono installati, usare il metodo statico <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> (metodo).  
  
 Questa classe è per l'esecuzione di speech recognition motori in-process e consente di controllare vari aspetti del riconoscimento vocale, come indicato di seguito:  
  
-   Per creare un sistema di riconoscimento vocale in-process, usare uno del <xref:System.Speech.Recognition.SpeechRecognitionEngine.%23ctor%2A> costruttori.  
  
-   Per gestire le grammatiche di riconoscimento vocale, usare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar%2A>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars%2A> metodi e il <xref:System.Speech.Recognition.SpeechRecognitionEngine.Grammars%2A> proprietà.  
  
-   Per configurare l'input al riconoscimento, usare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>, o <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A> (metodo).  
  
-   Per eseguire il riconoscimento vocale, usare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> (metodo).  
  
-   Per modificare la modalità di riconoscimento gestione input imprevisti o inattività, usare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> proprietà.  
  
-   Per modificare il numero di alternative di riconoscimento restituisce, usare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates%2A> proprietà. Il riconoscimento restituisce risultati del riconoscimento in un <xref:System.Speech.Recognition.RecognitionResult> oggetto.  
  
-   Per sincronizzare le modifiche apportate al sistema di riconoscimento, usare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> (metodo). Il riconoscimento Usa più thread per eseguire attività.  
  
-   Per emulare input al riconoscimento, usare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> e <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> metodi.  
  
 Il <xref:System.Speech.Recognition.SpeechRecognitionEngine> oggetto è per le sole di utilizzo del processo di creazione di un'istanza dell'oggetto. Al contrario, il <xref:System.Speech.Recognition.SpeechRecognizer> condivide un singolo sistema di riconoscimento con tutte le applicazioni che desiderano usarla.  
  
> [!NOTE]
>  Chiamare sempre <xref:System.Speech.Recognition.SpeechRecognitionEngine.Dispose%2A> prima di rilasciare l'ultimo riferimento al riconoscimento vocale. In caso contrario, le risorse in uso verranno non verranno liberate finché il garbage collector chiama l'oggetto di sistema di riconoscimento `Finalize` (metodo).  
  
   
  
## Examples  
 Nell'esempio seguente mostra parte di un'applicazione console che illustra il riconoscimento vocale di base. Poiché questo esempio Usa la `Multiple` modalità del <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> metodo, esegue il riconoscimento fino a quando non si chiude la finestra della console o interrompere il debug.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
    </remarks>
    <altmember cref="T:System.Speech.Recognition.Grammar" />
    <altmember cref="T:System.Speech.Recognition.SpeechRecognizer" />
  </Docs>
  <Members>
    <MemberGroup MemberName=".ctor">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Inizializza una nuova istanza della classe <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 È possibile costruire un <xref:System.Speech.Recognition.SpeechRecognitionEngine> istanza da una qualsiasi delle operazioni seguenti:  
  
-   Il motore di riconoscimento vocale predefinito per il sistema  
  
-   Un motore di riconoscimento vocale specifico che si specifica il nome  
  
-   Il motore di riconoscimento vocale predefinito per le impostazioni locali specificato  
  
-   Un motore di riconoscimento specifico che soddisfa i criteri specificati in un <xref:System.Speech.Recognition.RecognizerInfo> oggetto.  
  
 Prima di iniziare il riconoscimento vocale, riconoscimento è necessario caricare grammatica di riconoscimento vocale in almeno uno e configurare l'input per il riconoscimento.  
  
 Per caricare una grammatica, chiamare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> (metodo).  
  
 Per configurare l'input audio, usare uno dei metodi seguenti:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor" />
      <MemberSignature Language="VB.NET" Value="Public Sub New ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine();" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters />
      <Docs>
        <summary>Inizializza una nuova istanza della classe <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> utilizzando il riconoscimento vocale predefinito per il sistema.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Prima di iniziare il riconoscimento vocale, riconoscimento vocale è necessario caricare almeno una grammatica di riconoscimento e configurare l'input per il riconoscimento.  
  
 Per caricare una grammatica, chiamare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> (metodo).  
  
 Per configurare l'input audio, usare uno dei metodi seguenti:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (System.Globalization.CultureInfo culture);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(class System.Globalization.CultureInfo culture) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Globalization.CultureInfo)" />
      <MemberSignature Language="VB.NET" Value="Public Sub New (culture As CultureInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine(System::Globalization::CultureInfo ^ culture);" />
      <MemberSignature Language="F#" Value="new System.Speech.Recognition.SpeechRecognitionEngine : System.Globalization.CultureInfo -&gt; System.Speech.Recognition.SpeechRecognitionEngine" Usage="new System.Speech.Recognition.SpeechRecognitionEngine culture" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="culture" Type="System.Globalization.CultureInfo" />
      </Parameters>
      <Docs>
        <param name="culture">Le impostazioni locali che il riconoscimento vocale deve supportare.</param>
        <summary>Inizializza una nuova istanza della classe <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> utilizzando il riconoscimento vocale predefinito per le impostazioni locali specificate.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Microsoft Windows e l'API System. Speech accettare tutti i codici dei paesi di lingua validi. Per eseguire il riconoscimento vocale usando la lingua specificata nel `CultureInfo` argomento, un motore di riconoscimento vocale che supporta che codice paese di linguaggio deve essere installato. I motori di riconoscimento vocale fornita con Microsoft Windows 7 funzionano con i seguenti codici di lingua paese.  
  
-   en-GB. Inglese (Regno Unito)  
  
-   en-US. Inglese (Stati Uniti)  
  
-   de-DE. Tedesco (Germania)  
  
-   es-ES. Spagnolo (Spagna)  
  
-   fr-FR. Francese (Francia)  
  
-   ja-JP. Giapponese (Giappone)  
  
-   zh-CN. Cinese (Cina)  
  
-   zh-TW. Cinese (Taiwan)  
  
 Codici di lingua di due lettere, ad esempio "en", "fr", o "es" sono inoltre consentiti.  
  
 Prima di iniziare il riconoscimento vocale, riconoscimento è necessario caricare grammatica di riconoscimento vocale in almeno uno e configurare l'input per il riconoscimento.  
  
 Per caricare una grammatica, chiamare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> (metodo).  
  
 Per configurare l'input audio, usare uno dei metodi seguenti:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 Nell'esempio seguente mostra parte di un'applicazione console che illustra il riconoscimento vocale di base e la Inizializza un riconoscimento vocale per le impostazioni locali en-US.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentException">Nessuno dei sistemi di riconoscimento di input vocali installato supporta le impostazioni locali specificate oppure il parametro <paramref name="culture" /> è la lingua inglese.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="Culture" /> è <see langword="null" />.</exception>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (System.Speech.Recognition.RecognizerInfo recognizerInfo);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(class System.Speech.Recognition.RecognizerInfo recognizerInfo) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine(System::Speech::Recognition::RecognizerInfo ^ recognizerInfo);" />
      <MemberSignature Language="F#" Value="new System.Speech.Recognition.SpeechRecognitionEngine : System.Speech.Recognition.RecognizerInfo -&gt; System.Speech.Recognition.SpeechRecognitionEngine" Usage="new System.Speech.Recognition.SpeechRecognitionEngine recognizerInfo" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="recognizerInfo" Type="System.Speech.Recognition.RecognizerInfo" />
      </Parameters>
      <Docs>
        <param name="recognizerInfo">Le informazioni per il riconoscimento vocale specifico.</param>
        <summary>Inizializza una nuova istanza dell'oggetto <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />  utilizzando le informazioni disponibili in un oggetto <see cref="T:System.Speech.Recognition.RecognizerInfo" /> per specificare il riconoscimento da utilizzare.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 È possibile creare un'istanza di questa classe per uno qualsiasi dei sistemi di riconoscimento vocale installato. Per ottenere informazioni sui tipi di riconoscimento vengono installati, usare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> (metodo).  
  
 Prima di iniziare il riconoscimento vocale, riconoscimento è necessario caricare grammatica di riconoscimento vocale in almeno uno e configurare l'input per il riconoscimento.  
  
 Per caricare una grammatica, chiamare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> (metodo).  
  
 Per configurare l'input audio, usare uno dei metodi seguenti:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 Nell'esempio seguente mostra parte di un'applicazione console che illustra il riconoscimento vocale di base e la Inizializza un riconoscimento vocale che supporta la lingua inglese.  
  
```csharp  
 using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Select a speech recognizer that supports English.  
      RecognizerInfo info = null;  
      foreach (RecognizerInfo ri in SpeechRecognitionEngine.InstalledRecognizers())  
      {  
        if (ri.Culture.TwoLetterISOLanguageName.Equals("en"))  
        {  
          info = ri;  
          break;  
        }  
      }  
      if (info == null) return;  
  
      // Create the selected recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(info))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (string recognizerId);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(string recognizerId) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub New (recognizerId As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine(System::String ^ recognizerId);" />
      <MemberSignature Language="F#" Value="new System.Speech.Recognition.SpeechRecognitionEngine : string -&gt; System.Speech.Recognition.SpeechRecognitionEngine" Usage="new System.Speech.Recognition.SpeechRecognitionEngine recognizerId" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="recognizerId" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="recognizerId">Il nome del token del riconoscimento vocale da utilizzare.</param>
        <summary>Inizializza una nuova istanza della classe <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> con una parametro di stringa che specifica il nome del riconoscimento da utilizzare.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il nome del token del riconoscimento è il valore della <xref:System.Speech.Recognition.RecognizerInfo.Id%2A> proprietà del <xref:System.Speech.Recognition.RecognizerInfo> oggetto restituito dal <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo%2A> proprietà del motore di riconoscimento. Per ottenere una raccolta di tutti i riconoscitori installati, usare il metodo statico <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> (metodo).  
  
 Prima di iniziare il riconoscimento vocale, riconoscimento è necessario caricare grammatica di riconoscimento vocale in almeno uno e configurare l'input per il riconoscimento.  
  
 Per caricare una grammatica, chiamare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> (metodo).  
  
 Per configurare l'input audio, usare uno dei metodi seguenti:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 L'esempio seguente mostra parte di un'applicazione console che illustra il riconoscimento vocale di base e crea un'istanza di 8.0 il riconoscimento vocale per Windows (in lingua inglese - Stati Uniti).  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an instance of the Microsoft Speech Recognizer 8.0 for  
      // Windows (English - US).  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine("MS-1033-80-DESK"))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized += new EventHandler(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentException">Non è installato alcun riconoscimento di input vocale con quel nome di token oppure il parametro <paramref name="recognizerId" /> è la stringa vuota ("").</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="recognizerId" /> è <see langword="null" />.</exception>
      </Docs>
    </Member>
    <Member MemberName="AudioFormat">
      <MemberSignature Language="C#" Value="public System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioFormat As SpeechAudioFormatInfo" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::AudioFormat::SpeechAudioFormatInfo ^ AudioFormat { System::Speech::AudioFormat::SpeechAudioFormatInfo ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioFormat : System.Speech.AudioFormat.SpeechAudioFormatInfo" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.AudioFormat.SpeechAudioFormatInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ottiene il formato dell'audio ricevuto da <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>Il formato audio dell'input all'istanza di <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> o <see langword="null" /> se l'input non è configurato o non impostato su input null.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Per configurare l'input audio, usare uno dei metodi seguenti:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 L'esempio seguente usa <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat%2A> per ottenere e visualizzare i dati in formato audio.  
  
```  
static void DisplayAudioDeviceFormat(Label label, SpeechRecognitionEngine recognitionEngine)   
{  
  
  if (recognitionEngine != null && label != null)   
  {  
    label.Text = String.Format("Encoding Format:         {0}\n" +  
          "AverageBytesPerSecond    {1}\n" +  
          "BitsPerSample            {2}\n" +  
          "BlockAlign               {3}\n" +  
          "ChannelCount             {4}\n" +  
          "SamplesPerSecond         {5}",  
          recognitionEngine.AudioFormat.EncodingFormat.ToString(),  
          recognitionEngine.AudioFormat.AverageBytesPerSecond,  
          recognitionEngine.AudioFormat.BitsPerSample,  
          recognitionEngine.AudioFormat.BlockAlign,  
          recognitionEngine.AudioFormat.ChannelCount,  
          recognitionEngine.AudioFormat.SamplesPerSecond);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.AudioFormat.SpeechAudioFormatInfo" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioFormat" />
      </Docs>
    </Member>
    <Member MemberName="AudioLevel">
      <MemberSignature Language="C#" Value="public int AudioLevel { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance int32 AudioLevel" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioLevel As Integer" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property int AudioLevel { int get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioLevel : int" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Int32</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ottiene il livello dell'audio ricevuto da <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>Livello audio dell'input del riconoscimento vocale, da 0 a 100.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il valore 0 rappresenta silenzio, e 100 rappresenta il massimo volume di input.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioLevel" />
      </Docs>
    </Member>
    <Member MemberName="AudioLevelUpdated">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; AudioLevelUpdated;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; AudioLevelUpdated" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioLevelUpdated As EventHandler(Of AudioLevelUpdatedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioLevelUpdatedEventArgs ^&gt; ^ AudioLevelUpdated;" />
      <MemberSignature Language="F#" Value="member this.AudioLevelUpdated : EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; " Usage="member this.AudioLevelUpdated : System.EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Generato quando <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> segnala il livello del relativo input audio.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il <xref:System.Speech.Recognition.SpeechRecognitionEngine> genera questo evento più volte al secondo. La frequenza con cui l'evento viene generato varia a seconda del computer in cui viene eseguita l'applicazione.  
  
 Per ottenere il livello audio al momento dell'evento, usare il <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs.AudioLevel%2A> proprietà del controllo associato <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs>. Per ottenere il livello corrente audio di input al riconoscimento, usare il riconoscimento <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel%2A> proprietà.  
  
 Quando si crea un delegato di <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated>, si identifica il metodo con cui gestire l'evento. Per associare l'evento al gestore eventi in uso, aggiungere all'evento un'istanza del delegato. Il gestore eventi viene chiamato ogni volta che si verifica l'evento, a meno che non venga rimosso il delegato. Per altre informazioni sui delegati del gestore eventi, vedere [eventi e delegati](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 L'esempio seguente aggiunge un gestore per il <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated> evento da un <xref:System.Speech.Recognition.SpeechRecognitionEngine> oggetto. Il gestore restituisce il nuovo livello audio nella console.  
  
```  
private SpeechRecognitionEngine recognizer;  
  
// Initialize the SpeechRecognitionEngine object.   
private void Initialize()  
{  
  recognizer = new SpeechRecognitionEngine();  
  
  // Add an event handler for the AudioLevelUpdated event.  
  recognizer.AudioLevelUpdated +=   
   new EventHandler<AudioLevelUpdatedEventArgs>(recognizer_AudioLevelUpdated);  
  
  // Add other initialization code here.  
  
}  
  
// Write the audio level to the console when the AudioLevelUpdated event is raised.  
void recognizer_AudioLevelUpdated(object sender, AudioLevelUpdatedEventArgs e)  
{  
  Console.WriteLine("The audio level is now: {0}.", e.AudioLevel);  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioLevelUpdatedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel" />
      </Docs>
    </Member>
    <Member MemberName="AudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan AudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan AudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioPosition As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan AudioPosition { TimeSpan get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioPosition : TimeSpan" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ottiene la posizione corrente nel flusso audio generato dal dispositivo che fornisce l'input a <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>La posizione corrente nel flusso audio generato dal dispositivo di input.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> proprietà fa riferimento la posizione del dispositivo di input nel relativo flusso audio generato. Al contrario, il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> proprietà fa riferimento la posizione del motore di riconoscimento all'interno di relativo input audio. Queste posizioni possono essere diverse. Ad esempio, se ha ricevuto il riconoscimento di input per i quali non ha ancora generato un risultato del riconoscimento, il valore della <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> proprietà è minore del valore del <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> proprietà.  
  
   
  
## Examples  
 Nell'esempio seguente, il riconoscimento vocale in-process Usa una grammatica di dettatura per associare l'input vocale. Un gestore per il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> eventi scrivono nella console di <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel%2A> quando il riconoscimento vocale rileva all'input vocale.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine for US English.  
      using (recognizer = new SpeechRecognitionEngine(  
        new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create a grammar for finding services in different cities.  
        Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
        Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
        GrammarBuilder findServices = new GrammarBuilder("Find");  
        findServices.Append(services);  
        findServices.Append("near");  
        findServices.Append(cities);  
  
        // Create a Grammar object from the GrammarBuilder and load it to the recognizer.  
        Grammar servicesGrammar = new Grammar(findServices);  
        recognizer.LoadGrammarAsync(servicesGrammar);  
  
        // Add handlers for events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  
  
        // Start asynchronous recognition.  
        recognizer.RecognizeAsync();  
        Console.WriteLine("Starting asynchronous recognition...");  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Gather information about detected speech and write it to the console.  
    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Speech detected:");  
      Console.WriteLine("  Audio level: " + recognizer.AudioLevel);  
      Console.WriteLine("  Audio position at the event: " + e.AudioPosition);  
      Console.WriteLine("  Current audio position: " + recognizer.AudioPosition);  
      Console.WriteLine("  Current recognizer audio position: " +   
        recognizer.RecognizerAudioPosition);  
    }  
  
    // Write the text of the recognition result to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("\nSpeech recognized: " + e.Result.Text);  
  
      // Add event handler code here.  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="AudioSignalProblemOccurred">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; AudioSignalProblemOccurred;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; AudioSignalProblemOccurred" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioSignalProblemOccurred As EventHandler(Of AudioSignalProblemOccurredEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioSignalProblemOccurredEventArgs ^&gt; ^ AudioSignalProblemOccurred;" />
      <MemberSignature Language="F#" Value="member this.AudioSignalProblemOccurred : EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; " Usage="member this.AudioSignalProblemOccurred : System.EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Generato quando <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> rileva un problema nel segnale audio.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Per ottenere si è verificato il problema, usare il <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs.AudioSignalProblem%2A> proprietà del controllo associato <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs>.  
  
 Quando si crea un delegato di <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred>, si identifica il metodo con cui gestire l'evento. Per associare l'evento al gestore eventi in uso, aggiungere all'evento un'istanza del delegato. Il gestore eventi viene chiamato ogni volta che si verifica l'evento, a meno che non venga rimosso il delegato. Per altre informazioni sui delegati del gestore eventi, vedere [eventi e delegati](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 L'esempio seguente definisce un gestore eventi che raccoglie informazioni su un <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred> evento.  
  
```  
private SpeechRecognitionEngine recognizer;  
  
// Initialize the speech recognition engine.  
private void Initialize()  
{  
  recognizer = new SpeechRecognitionEngine();  
  
  // Add a handler for the AudioSignalProblemOccurred event.  
  recognizer.AudioSignalProblemOccurred +=   
    new EventHandler<AudioSignalProblemOccurredEventArgs>(  
      recognizer_AudioSignalProblemOccurred);  
}  
  
// Gather information when the AudioSignalProblemOccurred event is raised.  
void recognizer_AudioSignalProblemOccurred(object sender, AudioSignalProblemOccurredEventArgs e)  
{  
  StringBuilder details = new StringBuilder();  
  
  details.AppendLine("Audio signal problem information:");  
  details.AppendFormat(  
    " Audio level:               {0}" + Environment.NewLine +  
    " Audio position:            {1}" + Environment.NewLine +  
    " Audio signal problem:      {2}" + Environment.NewLine +  
    " Recognition engine audio position: {3}" + Environment.NewLine,  
    e.AudioLevel, e.AudioPosition,  e.AudioSignalProblem,  
    e.recoEngineAudioPosition);  
  
  // Insert additional event handler code here.  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioSignalProblem" />
        <altmember cref="T:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="AudioState">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.AudioState AudioState { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.Speech.Recognition.AudioState AudioState" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioState" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioState As AudioState" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::AudioState AudioState { System::Speech::Recognition::AudioState get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioState : System.Speech.Recognition.AudioState" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioState" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.AudioState</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ottiene lo stato dell'audio ricevuto da <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>Lo stato dell'input audio per il riconoscimento vocale.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> proprietà rappresenta lo stato dell'audio con un membro del <xref:System.Speech.Recognition.AudioState> enumerazione.  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioState" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioState" />
      </Docs>
    </Member>
    <Member MemberName="AudioStateChanged">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; AudioStateChanged;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioStateChangedEventArgs&gt; AudioStateChanged" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioStateChanged As EventHandler(Of AudioStateChangedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioStateChangedEventArgs ^&gt; ^ AudioStateChanged;" />
      <MemberSignature Language="F#" Value="member this.AudioStateChanged : EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; " Usage="member this.AudioStateChanged : System.EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Generato in seguito alla modifica dello stato nell'audio ricevuto da <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Per ottenere lo stato audio al momento dell'evento, usare il <xref:System.Speech.Recognition.AudioStateChangedEventArgs.AudioState%2A> proprietà del controllo associato <xref:System.Speech.Recognition.AudioStateChangedEventArgs>. Per ottenere lo stato corrente audio dell'input al riconoscimento, usare il riconoscimento <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> proprietà. Per altre informazioni sullo stato audio, vedere il <xref:System.Speech.Recognition.AudioState> enumerazione.  
  
 Quando si crea un delegato di <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged>, si identifica il metodo con cui gestire l'evento. Per associare l'evento al gestore eventi in uso, aggiungere all'evento un'istanza del delegato. Il gestore eventi viene chiamato ogni volta che si verifica l'evento, a meno che non venga rimosso il delegato. Per altre informazioni sui delegati del gestore eventi, vedere [eventi e delegati](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 L'esempio seguente usa un gestore per il <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> del nuovo evento da scrivere il riconoscimento <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> nella console ogni volta che viene modificato, utilizzando un membro del <xref:System.Speech.Recognition.AudioState> enumerazione.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a grammar.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder("On this farm he had a");  
        farm.Append(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Attach event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(recognizer_AudioStateChanged);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine();  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
        Console.WriteLine();  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Done.");  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the AudioStateChanged event.  
    static void recognizer_AudioStateChanged(object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("The new audio state is: " + e.AudioState);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioState" />
        <altmember cref="T:System.Speech.Recognition.AudioStateChangedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioState" />
      </Docs>
    </Member>
    <Member MemberName="BabbleTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan BabbleTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan BabbleTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
      <MemberSignature Language="VB.NET" Value="Public Property BabbleTimeout As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan BabbleTimeout { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.BabbleTimeout : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0;netframework-4.5;netframework-4.5.1;netframework-4.5.2;netframework-4.6;netframework-4.6.1;netframework-4.6.2;netframework-4.7;netframework-4.7.1;netframework-4.7.2;netframework-4.8">
          <AttributeName>System.ComponentModel.EditorBrowsable</AttributeName>
        </Attribute>
        <Attribute FrameworkAlternate="netframework-3.0;netframework-3.5">
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ottiene o imposta l'intervallo di tempo durante il quale <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> accetta input contenente solo il rumore di fondo, prima di completare il riconoscimento.</summary>
        <value>La durata dell'intervallo di tempo.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ogni riconoscimento vocale è disponibile un algoritmo per distinguere tra silenzio e riconoscimento vocale. Il riconoscimento classifica come radiazione di fondo che non corrispondono alla regola iniziale di uno qualsiasi del riconoscimento di input qualsiasi silenzio-non caricati e abilitati grammatiche di riconoscimento vocale. Se il riconoscimento riceve solo rumore di fondo e silenzio entro l'intervallo di timeout vocale ha, il riconoscimento completa tale operazione di riconoscimento.  
  
-   Per le operazioni di riconoscimento asincrona, il riconoscimento genera il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> evento, in cui la <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.BabbleTimeout%2A?displayProperty=nameWithType> proprietà è `true`e il <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A?displayProperty=nameWithType> proprietà è `null`.  
  
-   Per operazioni di riconoscimento sincrono ed emulazione, restituisce il riconoscimento `null`, anziché un valore valido <xref:System.Speech.Recognition.RecognitionResult>.  
  
 Se il periodo di timeout vocale ha è impostato su 0, il riconoscimento non esegue una verifica di timeout vocale ha. L'intervallo di timeout può essere qualsiasi valore non negativo. Il valore predefinito è 0 secondi.  
  
   
  
## Examples  
 L'esempio seguente mostra parte di un'applicazione console che illustra il riconoscimento vocale base che consente di impostare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> e <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> delle proprietà di un <xref:System.Speech.Recognition.SpeechRecognitionEngine> prima dell'avvio di riconoscimento vocale. Gestori per il riconoscimento vocale <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> e <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> eventi di output delle informazioni di evento nella console per dimostrare come la <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> delle proprietà di un <xref:System.Speech.Recognition.SpeechRecognitionEngine> influire sul funzionamento del riconoscimento.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Load a Grammar object.  
        recognizer.LoadGrammar(CreateServicesGrammar("FindServices"));  
  
        // Add event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(  
            AudioStateChangedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(3);  
        recognizer.BabbleTimeout = TimeSpan.FromSeconds(2);  
        recognizer.EndSilenceTimeout = TimeSpan.FromSeconds(1);  
        recognizer.EndSilenceTimeoutAmbiguous = TimeSpan.FromSeconds(1.5);  
  
        Console.WriteLine("BabbleTimeout: {0}", recognizer.BabbleTimeout);  
        Console.WriteLine("InitialSilenceTimeout: {0}", recognizer.InitialSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeout: {0}", recognizer.EndSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeoutAmbiguous: {0}", recognizer.EndSilenceTimeoutAmbiguous);  
        Console.WriteLine();  
  
        // Start asynchronous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Single);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Create a grammar and build it into a Grammar object.   
    static Grammar CreateServicesGrammar(string grammarName)  
    {  
  
      // Create a grammar for finding services in different cities.  
      Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
      Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
      GrammarBuilder findServices = new GrammarBuilder("Find");  
      findServices.Append(services);  
      findServices.Append("near");  
      findServices.Append(cities);  
  
      // Create a Grammar object from the GrammarBuilder..  
      Grammar servicesGrammar = new Grammar(findServices);  
      servicesGrammar.Name = ("FindServices");  
      return servicesGrammar;  
    }  
  
    // Handle the AudioStateChanged event.  
    static void AudioStateChangedHandler(  
      object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("AudioStateChanged ({0}): {1}",  
        DateTime.Now.ToString("mm:ss.f"), e.AudioState);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("RecognizeCompleted ({0}):",  
        DateTime.Now.ToString("mm:ss.f"));  
  
      string resultText;  
      if (e.Result != null) { resultText = e.Result.Text; }  
      else { resultText = "<null>"; }  
  
      Console.WriteLine(  
        " BabbleTimeout: {0}; InitialSilenceTimeout: {1}; Result text: {2}",  
        e.BabbleTimeout, e.InitialSilenceTimeout, resultText);  
      if (e.Error != null)  
      {  
        Console.WriteLine(" Exception message: ", e.Error.Message);  
      }  
  
      // Start the next asynchronous recognition operation.  
      ((SpeechRecognitionEngine)sender).RecognizeAsync(RecognizeMode.Single);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Questa proprietà è impostata su un valore minore di 0 secondi.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <MemberGroup MemberName="Dispose">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Elimina l'oggetto <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
      </Docs>
    </MemberGroup>
    <Member MemberName="Dispose">
      <MemberSignature Language="C#" Value="public void Dispose ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig newslot virtual instance void Dispose() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Dispose" />
      <MemberSignature Language="VB.NET" Value="Public Sub Dispose ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; virtual void Dispose();" />
      <MemberSignature Language="F#" Value="abstract member Dispose : unit -&gt; unit&#xA;override this.Dispose : unit -&gt; unit" Usage="speechRecognitionEngine.Dispose " />
      <MemberType>Method</MemberType>
      <Implements>
        <InterfaceMember>M:System.IDisposable.Dispose</InterfaceMember>
      </Implements>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Elimina l'oggetto <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <Member MemberName="Dispose">
      <MemberSignature Language="C#" Value="protected virtual void Dispose (bool disposing);" />
      <MemberSignature Language="ILAsm" Value=".method familyhidebysig newslot virtual instance void Dispose(bool disposing) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Dispose(System.Boolean)" />
      <MemberSignature Language="VB.NET" Value="Protected Overridable Sub Dispose (disposing As Boolean)" />
      <MemberSignature Language="C++ CLI" Value="protected:&#xA; virtual void Dispose(bool disposing);" />
      <MemberSignature Language="F#" Value="abstract member Dispose : bool -&gt; unit&#xA;override this.Dispose : bool -&gt; unit" Usage="speechRecognitionEngine.Dispose disposing" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="disposing" Type="System.Boolean" />
      </Parameters>
      <Docs>
        <param name="disposing">
          <see langword="true" /> per rilasciare sia le risorse gestite sia quelle non gestite; <see langword="false" /> per rilasciare solo le risorse non gestite.</param>
        <summary>Elimina l'oggetto <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> e rilascia le risorse usate durante la sessione.</summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <MemberGroup MemberName="EmulateRecognize">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Emula l'input al riconoscimento vocale, utilizzando il testo anziché l'audio per il riconoscimento vocale sincrono.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Questi metodi bypass dell'input audio di sistema e fornire il testo per il riconoscimento come <xref:System.String> oggetti o come una matrice di <xref:System.Speech.Recognition.RecognizedWordUnit> oggetti. Ciò può essere utile quando si esegue il test o il debug di un'applicazione o grammatica. Ad esempio, si usa l'emulazione per determinare se una parola in una grammatica e quali semantica viene restituita quando viene riconosciuta la parola. Usare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A> metodo per disabilitare l'input audio per il motore di riconoscimento vocale durante le operazioni di emulazione.  
  
 Genera il riconoscimento vocale il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventi come se l'operazione di riconoscimento non viene emulato. Il riconoscimento nuove righe e gli spazi vuoti aggiuntivi vengono ignorati e punteggiatura vengono considerati input letterale.  
  
> [!NOTE]
>  Il <xref:System.Speech.Recognition.RecognitionResult> un valore dell'oggetto generato dal riconoscimento vocale in risposta all'input emulato è `null` per la relativa <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> proprietà.  
  
 Per emulare il riconoscimento asincrono, utilizzare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> (metodo).  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(string inputText) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Function EmulateRecognize (inputText As String) As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(System::String ^ inputText);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : string -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.EmulateRecognize inputText" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0">
          <AttributeName>System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="inputText">Input per l'operazione di riconoscimento.</param>
        <summary>Emula l'input di una frase al riconoscimento vocale, utilizzando il testo anziché l'audio per il riconoscimento vocale sincrono.</summary>
        <returns>Il risultato dell'operazione di riconoscimento o <see langword="null" /> se l'operazione non riesce o il riconoscimento non è abilitato.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Genera il riconoscimento vocale il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventi come se l'operazione di riconoscimento non viene emulato.  
  
 I sistemi di riconoscimento fornite con Windows 7 e Vista Ignora maiuscole/minuscole e carattere width quando si applicano le regole di grammatica alla frase di input. Per altre informazioni su questo tipo di confronto, vedere la <xref:System.Globalization.CompareOptions> valori di enumerazione <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> e <xref:System.Globalization.CompareOptions.IgnoreWidth>. I sistemi di riconoscimento anche ignorare nuove righe e lo spazio vuoto aggiuntivo e considerare i segni di punteggiatura come input letterale.  
  
   
  
## Examples  
 Esempio di codice seguente fa parte di un'applicazione console che illustra l'input emulato, i risultati di riconoscimento associati e gli eventi associati generati dal riconoscimento vocale. L'esempio genera l'output seguente.  
  
```  
TestRecognize("Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = Smith  
...Recognition result text = Smith  
  
TestRecognize("Jones")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Jones; Text = Jones  
...Recognition result text = Jones  
  
TestRecognize("Mister")...  
 SpeechDetected event raised.  
 SpeechHypothesized event raised.  
  Grammar = Smith; Text = mister  
 SpeechRecognitionRejected event raised.  
  Grammar = <not available>; Text =  
...No recognition result.  
  
TestRecognize("Mister Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = mister Smith  
...Recognition result text = mister Smith  
  
press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace Sre_EmulateRecognize  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Load grammars.  
        recognizer.LoadGrammar(CreateNameGrammar("Smith"));  
        recognizer.LoadGrammar(CreateNameGrammar("Jones"));  
  
        // Disable audio input to the recognizer.  
        recognizer.SetInputToNull();  
  
        // Add handlers for events raised by the EmulateRecognize method.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
  
        // Start four synchronous emulated recognition operations.  
        TestRecognize(recognizer, "Smith");  
        TestRecognize(recognizer, "Jones");  
        TestRecognize(recognizer, "Mister");  
        TestRecognize(recognizer, "Mister Smith");  
      }  
  
      Console.WriteLine("press any key to exit...");  
      Console.ReadKey(true);  
    }  
  
    // Create a simple name grammar.  
    // Set the grammar name to the surname.  
    private static Grammar CreateNameGrammar(string surname)  
    {  
      GrammarBuilder builder = new GrammarBuilder("mister", 0, 1);  
      builder.Append(surname);  
  
      Grammar nameGrammar = new Grammar(builder);  
      nameGrammar.Name = surname;  
  
      return nameGrammar;  
    }  
  
    // Send emulated input to the recognizer for synchronous recognition.  
    private static void TestRecognize(  
      SpeechRecognitionEngine recognizer, string input)  
    {  
      Console.WriteLine("TestRecognize(\"{0}\")...", input);  
      RecognitionResult result =  
        recognizer.EmulateRecognize(input,CompareOptions.IgnoreCase);  
      if (result != null)  
      {  
        Console.WriteLine("...Recognition result text = {0}",  
          result.Text ?? "<null>");  
      }  
      else  
      {  
        Console.WriteLine("...No recognition result.");  
      }  
      Console.WriteLine();  
    }  
  
    static void SpeechDetectedHandler(  
      object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechDetected event raised.");  
    }  
  
    // Handle events.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechHypothesized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognitionRejected event raised.");  
      if (e.Result != null)  
      {  
        string grammarName;  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name ?? "<none>";  
        }  
        else  
        {  
          grammarName = "<not available>";  
        }  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          grammarName, e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Nel riconoscimento non sono caricate grammatiche di riconoscimento vocale.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="inputText" /> è <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="inputText" /> è la stringa vuota ("").</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(class System.Speech.Recognition.RecognizedWordUnit[] wordUnits, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(cli::array &lt;System::Speech::Recognition::RecognizedWordUnit ^&gt; ^ wordUnits, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : System.Speech.Recognition.RecognizedWordUnit[] * System.Globalization.CompareOptions -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.EmulateRecognize (wordUnits, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="wordUnits" Type="System.Speech.Recognition.RecognizedWordUnit[]" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="wordUnits">Matrice di unità di parole che contiene l'input per l'operazione di riconoscimento.</param>
        <param name="compareOptions">Combinazione bit per bit dei valori di enumerazione che descrivono il tipo di confronto da utilizzare per l'operazione di riconoscimento emulato.</param>
        <summary>Emula l'input di parole specifiche al riconoscimento vocale, utilizzando il testo anziché l'audio per il riconoscimento vocale sincrono, e specifica come il riconoscimento gestisce il confronto Unicode tra le parole e le grammatiche di riconoscimento vocale caricate.</summary>
        <returns>Il risultato dell'operazione di riconoscimento o <see langword="null" /> se l'operazione non riesce o il riconoscimento non è abilitato.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Genera il riconoscimento vocale il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventi come se l'operazione di riconoscimento non viene emulato.  
  
 Usa il riconoscimento `compareOptions` quando si applica le regole di grammatica alla frase di input. I sistemi di riconoscimento fornite con Windows 7 e Vista Ignora maiuscole/minuscole se il <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> o <xref:System.Globalization.CompareOptions.IgnoreCase> è presente un valore. Il riconoscimento Ignora sempre la larghezza dei caratteri e mai ignora il tipo Kana. Il riconoscimento inoltre nuove righe e gli spazi vuoti aggiuntivi vengono ignorati e punteggiatura vengono considerati input letterale. Per altre informazioni sul tipo di distinzione Kana e larghezza dei caratteri, vedere il <xref:System.Globalization.CompareOptions> enumerazione.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Nel riconoscimento non sono caricate grammatiche di riconoscimento vocale.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="wordUnits" /> è <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="wordUnits" /> contiene uno o più elementi <see langword="null" />.</exception>
        <exception cref="T:System.NotSupportedException">
          <paramref name="compareOptions" /> contiene il flag <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" /> o <see cref="F:System.Globalization.CompareOptions.StringSort" />.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(string inputText, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String,System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(System::String ^ inputText, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : string * System.Globalization.CompareOptions -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.EmulateRecognize (inputText, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="inputText">Frase di Input per l'operazione di riconoscimento.</param>
        <param name="compareOptions">Combinazione bit per bit dei valori di enumerazione che descrivono il tipo di confronto da utilizzare per l'operazione di riconoscimento emulato.</param>
        <summary>Emula l'input di una frase al riconoscimento vocale, utilizzando il testo anziché l'audio per il riconoscimento vocale sincrono, e specifica come il riconoscimento gestisce il confronto Unicode tra la frase e le grammatiche di riconoscimento vocale caricate.</summary>
        <returns>Il risultato dell'operazione di riconoscimento o <see langword="null" /> se l'operazione non riesce o il riconoscimento non è abilitato.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Genera il riconoscimento vocale il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventi come se l'operazione di riconoscimento non viene emulato.  
  
 Usa il riconoscimento `compareOptions` quando si applica le regole di grammatica alla frase di input. I sistemi di riconoscimento fornite con Windows 7 e Vista Ignora maiuscole/minuscole se il <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> o <xref:System.Globalization.CompareOptions.IgnoreCase> è presente un valore. Il riconoscimento Ignora sempre la larghezza dei caratteri e mai ignora il tipo Kana. Il riconoscimento inoltre nuove righe e gli spazi vuoti aggiuntivi vengono ignorati e punteggiatura vengono considerati input letterale. Per altre informazioni sul tipo di distinzione Kana e larghezza dei caratteri, vedere il <xref:System.Globalization.CompareOptions> enumerazione.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Nel riconoscimento non sono caricate grammatiche di riconoscimento vocale.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="inputText" /> è <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="inputText" /> è la stringa vuota ("").</exception>
        <exception cref="T:System.NotSupportedException">
          <paramref name="compareOptions" /> contiene il flag <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" /> o <see cref="F:System.Globalization.CompareOptions.StringSort" />.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <MemberGroup MemberName="EmulateRecognizeAsync">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Emula l'input al riconoscimento vocale, utilizzando il testo anziché l'audio per il riconoscimento vocale asincrono.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Questi metodi bypass dell'input audio di sistema e fornire il testo per il riconoscimento come <xref:System.String> oggetti o come una matrice di <xref:System.Speech.Recognition.RecognizedWordUnit> oggetti. Ciò può essere utile quando si esegue il test o il debug di un'applicazione o grammatica. Ad esempio, si usa l'emulazione per determinare se una parola in una grammatica e quali semantica viene restituita quando viene riconosciuta la parola. Usare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A> metodo per disabilitare l'input audio per il motore di riconoscimento vocale durante le operazioni di emulazione.  
  
 Genera il riconoscimento vocale il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventi come se l'operazione di riconoscimento non viene emulato. Quando il riconoscimento viene completata l'operazione di riconoscimento asincrona, genera il <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> evento. Il riconoscimento nuove righe e gli spazi vuoti aggiuntivi vengono ignorati e punteggiatura vengono considerati input letterale.  
  
> [!NOTE]
>  Il <xref:System.Speech.Recognition.RecognitionResult> un valore dell'oggetto generato dal riconoscimento vocale in risposta all'input emulato è `null` per la relativa <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> proprietà.  
  
 Per emulare un riconoscimento sincrono, usare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> (metodo).  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (string inputText);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(string inputText) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub EmulateRecognizeAsync (inputText As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(System::String ^ inputText);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : string -&gt; unit" Usage="speechRecognitionEngine.EmulateRecognizeAsync inputText" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0">
          <AttributeName>System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="inputText">Input per l'operazione di riconoscimento.</param>
        <summary>Emula l'input di una frase al riconoscimento vocale, utilizzando il testo anziché l'audio per il riconoscimento vocale asincrono.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Genera il riconoscimento vocale il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventi come se l'operazione di riconoscimento non viene emulato. Quando il riconoscimento viene completata l'operazione di riconoscimento asincrona, genera il <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> evento.  
  
 I sistemi di riconoscimento fornite con Windows 7 e Vista Ignora maiuscole/minuscole e carattere width quando si applicano le regole di grammatica alla frase di input. Per altre informazioni su questo tipo di confronto, vedere la <xref:System.Globalization.CompareOptions> valori di enumerazione <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> e <xref:System.Globalization.CompareOptions.IgnoreWidth>. I sistemi di riconoscimento anche ignorare nuove righe e lo spazio vuoto aggiuntivo e considerare i segni di punteggiatura come input letterale.  
  
   
  
## Examples  
 Esempio di codice seguente fa parte di un'applicazione console che illustra l'input emulato asincrona, i risultati di riconoscimento associati e gli eventi associati generati dal riconoscimento vocale. L'esempio genera l'output seguente.  
  
```  
  
TestRecognizeAsync("Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = Smith  
 EmulateRecognizeCompleted event raised.  
  Grammar = Smith; Text = Smith  
 Done.  
  
TestRecognizeAsync("Jones")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Jones; Text = Jones  
 EmulateRecognizeCompleted event raised.  
  Grammar = Jones; Text = Jones  
 Done.  
  
TestRecognizeAsync("Mister")...  
 SpeechDetected event raised.  
 SpeechHypothesized event raised.  
  Grammar = Smith; Text = mister  
 SpeechRecognitionRejected event raised.  
  Grammar = <not available>; Text =  
 EmulateRecognizeCompleted event raised.  
  No recognition result available.  
 Done.  
  
TestRecognizeAsync("Mister Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = mister Smith  
 EmulateRecognizeCompleted event raised.  
  Grammar = Smith; Text = mister Smith  
 Done.  
  
press any key to exit...  
```  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SreEmulateRecognizeAsync  
{  
  class Program  
  {  
    // Indicate when an asynchronous operation is finished.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Load grammars.  
        recognizer.LoadGrammar(CreateNameGrammar("Smith"));  
        recognizer.LoadGrammar(CreateNameGrammar("Jones"));  
  
        // Configure the audio input.  
        recognizer.SetInputToNull();  
  
        // Add event handlers for the events raised by the  
        // EmulateRecognizeAsync method.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHander);  
  
        // Start four asynchronous emulated recognition operations.  
        TestRecognizeAsync(recognizer, "Smith");  
        TestRecognizeAsync(recognizer, "Jones");  
        TestRecognizeAsync(recognizer, "Mister");  
        TestRecognizeAsync(recognizer, "Mister Smith");  
      }  
  
      Console.WriteLine("press any key to exit...");  
      Console.ReadKey(true);  
    }  
  
    // Create a simple name grammar.  
    // Set the grammar name to the surname.  
    private static Grammar CreateNameGrammar(string surname)  
    {  
      GrammarBuilder builder = new GrammarBuilder("mister", 0, 1);  
      builder.Append(surname);  
  
      Grammar nameGrammar = new Grammar(builder);  
      nameGrammar.Name = surname;  
  
      return nameGrammar;  
    }  
  
    // Send emulated input to the recognizer for asynchronous  
    // recognition.  
    private static void TestRecognizeAsync(  
      SpeechRecognitionEngine recognizer, string input)  
    {  
      completed = false;  
  
      Console.WriteLine("TestRecognizeAsync(\"{0}\")...", input);  
      recognizer.EmulateRecognizeAsync(input);  
  
      // Wait for the operation to complete.  
      while (!completed)  
      {  
        Thread.Sleep(333);  
      }  
  
      Console.WriteLine(" Done.");  
      Console.WriteLine();  
    }  
  
    static void SpeechDetectedHandler(  
      object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechDetected event raised.");  
    }  
  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechHypothesized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    // Handle events.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognitionRejected event raised.");  
      if (e.Result != null)  
      {  
        string grammarName;  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name ?? "<none>";  
        }  
        else  
        {  
          grammarName = "<not available>";  
        }  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          grammarName, e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text );  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void EmulateRecognizeCompletedHander(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" EmulateRecognizeCompleted event raised.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine("  {0} exception encountered: {1}:",  
          e.Error.GetType().Name, e.Error.Message);  
      }  
      else if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      else if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Nel riconoscimento non sono caricate grammatiche di riconoscimento vocale, oppure ci sono operazioni di riconoscimento asincrone non ancora complete.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="inputText" /> è <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="inputText" /> è la stringa vuota ("").</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(class System.Speech.Recognition.RecognizedWordUnit[] wordUnits, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(cli::array &lt;System::Speech::Recognition::RecognizedWordUnit ^&gt; ^ wordUnits, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : System.Speech.Recognition.RecognizedWordUnit[] * System.Globalization.CompareOptions -&gt; unit" Usage="speechRecognitionEngine.EmulateRecognizeAsync (wordUnits, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="wordUnits" Type="System.Speech.Recognition.RecognizedWordUnit[]" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="wordUnits">Matrice di unità di parole che contiene l'input per l'operazione di riconoscimento.</param>
        <param name="compareOptions">Combinazione bit per bit dei valori di enumerazione che descrivono il tipo di confronto da utilizzare per l'operazione di riconoscimento emulato.</param>
        <summary>Emula l'input di parole specifiche al riconoscimento vocale, utilizzando una matrice di oggetti <see cref="T:System.Speech.Recognition.RecognizedWordUnit" /> anziché l'audio per il riconoscimento vocale asincrono, e specifica come il riconoscimento gestisce il confronto Unicode tra le parole e le grammatiche di riconoscimento vocale caricate.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Genera il riconoscimento vocale il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventi come se l'operazione di riconoscimento non viene emulato. Quando il riconoscimento viene completata l'operazione di riconoscimento asincrona, genera il <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> evento.  
  
 Usa il riconoscimento `compareOptions` quando si applica le regole di grammatica alla frase di input. I sistemi di riconoscimento fornite con Windows 7 e Vista Ignora maiuscole/minuscole se il <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> o <xref:System.Globalization.CompareOptions.IgnoreCase> è presente un valore. I sistemi di riconoscimento Ignora sempre la larghezza dei caratteri e mai ignorato il tipo Kana. I sistemi di riconoscimento anche ignorare nuove righe e lo spazio vuoto aggiuntivo e considerare i segni di punteggiatura come input letterale. Per altre informazioni sul tipo di distinzione Kana e larghezza dei caratteri, vedere il <xref:System.Globalization.CompareOptions> enumerazione.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Nel riconoscimento non sono caricate grammatiche di riconoscimento vocale, oppure ci sono operazioni di riconoscimento asincrone non ancora complete.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="wordUnits" /> è <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="wordUnits" /> contiene uno o più elementi <see langword="null" />.</exception>
        <exception cref="T:System.NotSupportedException">
          <paramref name="compareOptions" /> contiene il flag <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" /> o <see cref="F:System.Globalization.CompareOptions.StringSort" />.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (string inputText, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(string inputText, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String,System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(System::String ^ inputText, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : string * System.Globalization.CompareOptions -&gt; unit" Usage="speechRecognitionEngine.EmulateRecognizeAsync (inputText, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="inputText">Frase di Input per l'operazione di riconoscimento.</param>
        <param name="compareOptions">Combinazione bit per bit dei valori di enumerazione che descrivono il tipo di confronto da utilizzare per l'operazione di riconoscimento emulato.</param>
        <summary>Emula l'input di una frase al riconoscimento vocale, utilizzando il testo anziché l'audio per il riconoscimento vocale asincrono, e specifica come il riconoscimento gestisce il confronto Unicode tra la frase e le grammatiche di riconoscimento vocale caricate.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Genera il riconoscimento vocale il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventi come se l'operazione di riconoscimento non viene emulato. Quando il riconoscimento viene completata l'operazione di riconoscimento asincrona, genera il <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> evento.  
  
 Usa il riconoscimento `compareOptions` quando si applica le regole di grammatica alla frase di input. I sistemi di riconoscimento fornite con Windows 7 e Vista Ignora maiuscole/minuscole se il <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> o <xref:System.Globalization.CompareOptions.IgnoreCase> è presente un valore. I sistemi di riconoscimento Ignora sempre la larghezza dei caratteri e mai ignorato il tipo Kana. I sistemi di riconoscimento anche ignorare nuove righe e lo spazio vuoto aggiuntivo e considerare i segni di punteggiatura come input letterale. Per altre informazioni sul tipo di distinzione Kana e larghezza dei caratteri, vedere il <xref:System.Globalization.CompareOptions> enumerazione.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Nel riconoscimento non sono caricate grammatiche di riconoscimento vocale, oppure ci sono operazioni di riconoscimento asincrone non ancora complete.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="inputText" /> è <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="inputText" /> è la stringa vuota ("").</exception>
        <exception cref="T:System.NotSupportedException">
          <paramref name="compareOptions" /> contiene il flag <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" /> o <see cref="F:System.Globalization.CompareOptions.StringSort" />.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; EmulateRecognizeCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; EmulateRecognizeCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event EmulateRecognizeCompleted As EventHandler(Of EmulateRecognizeCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::EmulateRecognizeCompletedEventArgs ^&gt; ^ EmulateRecognizeCompleted;" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeCompleted : EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; " Usage="member this.EmulateRecognizeCompleted : System.EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Generato quando <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> completa un'operazione di riconoscimento asincrona di input emulato.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ogni <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> metodo avvia un'operazione di riconoscimento asincrona. Il <xref:System.Speech.Recognition.SpeechRecognitionEngine> genera il <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> eventi quando completa l'operazione asincrona.  
  
 Il <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> operazione può generare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventi. Il <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> eventi sono l'ultimo evento di questo tipo che genera la sospensione del riconoscimento per l'operazione specificata.  
  
 Se il riconoscimento emulato ha esito positivo, è possibile accedere al risultato del riconoscimento usando uno dei seguenti:  
  
-   Il <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs.Result%2A> proprietà nel <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs> oggetto nel gestore per il <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> evento.  
  
-   <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> proprietà nel <xref:System.Speech.Recognition.SpeechRecognizedEventArgs> oggetto nel gestore per il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> evento.  
  
 Se il riconoscimento emulato non è riuscito, il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> non viene generato e il <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs.Result%2A> sarà null.  
  
 <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs> deriva da <xref:System.ComponentModel.AsyncCompletedEventArgs>.  
  
 <xref:System.Speech.Recognition.SpeechRecognizedEventArgs> deriva da <xref:System.Speech.Recognition.RecognitionEventArgs>.  
  
 Quando si crea un delegato di <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted>, si identifica il metodo con cui gestire l'evento. Per associare l'evento al gestore eventi in uso, aggiungere all'evento un'istanza del delegato. Il gestore eventi viene chiamato ogni volta che si verifica l'evento, a meno che non venga rimosso il delegato. Per altre informazioni sui delegati del gestore eventi, vedere [eventi e delegati](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Nell'esempio seguente fa parte di un'applicazione console che carica una grammatica di riconoscimento vocale e illustra input emulato asincrona, i risultati di riconoscimento associati e gli eventi associati generati dal riconoscimento vocale.  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace InProcessRecognizer  
{  
  class Program  
  {  
    // Indicate whether the asynchronous emulate recognition  
    // operation has completed.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
  
      // Initialize an instance of an in-process recognizer.  
      using (SpeechRecognitionEngine recognizer =   
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
        recognizer.LoadGrammar(testGrammar);  
  
        // Attach event handlers for recognition events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call mathches the grammar  
        // and generates a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing testing");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call does not match the grammar  
        // or generate a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing one two three");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null)  
      {  
        Console.WriteLine("Result of 1st call to EmulateRecognizeAsync = {0}",  
          e.Result.Text ?? "<no text>");  
        Console.WriteLine();  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }  
  
    // Handle the EmulateRecognizeCompleted event.  
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      if (e.Result == null)  
      {  
        Console.WriteLine("Result of 2nd call to EmulateRecognizeAsync = No result generated.");  
      }  
  
      // Indicate the asynchronous operation is complete.  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EndSilenceTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan EndSilenceTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan EndSilenceTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
      <MemberSignature Language="VB.NET" Value="Public Property EndSilenceTimeout As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan EndSilenceTimeout { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.EndSilenceTimeout : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0;netframework-4.5;netframework-4.5.1;netframework-4.5.2;netframework-4.6;netframework-4.6.1;netframework-4.6.2;netframework-4.7;netframework-4.7.1;netframework-4.7.2;netframework-4.8">
          <AttributeName>System.ComponentModel.EditorBrowsable</AttributeName>
        </Attribute>
        <Attribute FrameworkAlternate="netframework-3.0;netframework-3.5">
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ottiene o imposta l'intervallo di silenzio che <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> accetterà alla fine dell'input non ambiguo prima di completare un'operazione di riconoscimento.</summary>
        <value>La durata dell'intervallo di silenzio.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il riconoscimento vocale Usa questo intervallo di timeout quando l'input di riconoscimento non è ambiguo. Ad esempio, per una grammatica di riconoscimento vocale che supporta il riconoscimento del "nuovo di giochi," o "nuova partita", "nuovo di giochi," è un input non ambiguo e "nuovo gioco" è un input ambiguo.  
  
 Questa proprietà determina il tempo di attesa il motore di riconoscimento vocale per l'input aggiuntivo prima di completare un'operazione di riconoscimento. L'intervallo di timeout può essere di 0 secondi a 10 secondi. Il valore predefinito è 150 millisecondi.  
  
 Per impostare l'intervallo di timeout per l'input ambiguo, usare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> proprietà.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Questa proprietà è impostata su un valore inferiore a 0 secondi o maggiore di 10 secondi.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <Member MemberName="EndSilenceTimeoutAmbiguous">
      <MemberSignature Language="C#" Value="public TimeSpan EndSilenceTimeoutAmbiguous { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan EndSilenceTimeoutAmbiguous" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
      <MemberSignature Language="VB.NET" Value="Public Property EndSilenceTimeoutAmbiguous As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan EndSilenceTimeoutAmbiguous { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.EndSilenceTimeoutAmbiguous : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0;netframework-4.5;netframework-4.5.1;netframework-4.5.2;netframework-4.6;netframework-4.6.1;netframework-4.6.2;netframework-4.7;netframework-4.7.1;netframework-4.7.2;netframework-4.8">
          <AttributeName>System.ComponentModel.EditorBrowsable</AttributeName>
        </Attribute>
        <Attribute FrameworkAlternate="netframework-3.0;netframework-3.5">
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ottiene o imposta l'intervallo di silenzio che <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> accetterà alla fine dell'input ambiguo prima di completare un'operazione di riconoscimento.</summary>
        <value>La durata dell'intervallo di silenzio.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il riconoscimento vocale Usa questo intervallo di timeout quando l'input del riconoscimento è ambiguo. Ad esempio, per una grammatica di riconoscimento vocale che supporta il riconoscimento del "nuovo di giochi," o "nuova partita", "nuovo di giochi," è un input non ambiguo e "nuovo gioco" è un input ambiguo.  
  
 Questa proprietà determina il tempo di attesa il motore di riconoscimento vocale per l'input aggiuntivo prima di completare un'operazione di riconoscimento. L'intervallo di timeout può essere di 0 secondi a 10 secondi. Il valore predefinito è 500 millisecondi.  
  
 Per impostare l'intervallo di timeout per l'input non ambiguo, usare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> proprietà.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Questa proprietà è impostata su un valore inferiore a 0 secondi o maggiore di 10 secondi.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <Member MemberName="Grammars">
      <MemberSignature Language="C#" Value="public System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt; Grammars { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Collections.ObjectModel.ReadOnlyCollection`1&lt;class System.Speech.Recognition.Grammar&gt; Grammars" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.Grammars" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property Grammars As ReadOnlyCollection(Of Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::Grammar ^&gt; ^ Grammars { System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::Grammar ^&gt; ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.Grammars : System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt;" Usage="System.Speech.Recognition.SpeechRecognitionEngine.Grammars" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ottiene una raccolta di oggetti <see cref="T:System.Speech.Recognition.Grammar" /> caricati in questa istanza di <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>Raccolta di oggetti <see cref="T:System.Speech.Recognition.Grammar" />.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Examples  
 L'esempio seguente vengono restituite informazioni nella console per ogni grammatica di riconoscimento vocale attualmente caricata da un riconoscimento vocale.  
  
> [!IMPORTANT]
>  Copiare l'insieme di grammatica per evitare errori se la raccolta viene modificata mentre questo metodo enumera gli elementi della raccolta.  
  
```csharp  
  
private static void ListGrammars(SpeechRecognitionEngine recognizer)  
{  
  string qualifier;  
  List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
  foreach (Grammar g in grammars)  
  {  
    qualifier = (g.Enabled) ? "enabled" : "disabled";  
  
    Console.WriteLine("Grammar {0} is loaded and is {1}.",  
      g.Name, qualifier);  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.Grammar" />
      </Docs>
    </Member>
    <Member MemberName="InitialSilenceTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan InitialSilenceTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan InitialSilenceTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
      <MemberSignature Language="VB.NET" Value="Public Property InitialSilenceTimeout As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan InitialSilenceTimeout { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.InitialSilenceTimeout : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0;netframework-4.5;netframework-4.5.1;netframework-4.5.2;netframework-4.6;netframework-4.6.1;netframework-4.6.2;netframework-4.7;netframework-4.7.1;netframework-4.7.2;netframework-4.8">
          <AttributeName>System.ComponentModel.EditorBrowsable</AttributeName>
        </Attribute>
        <Attribute FrameworkAlternate="netframework-3.0;netframework-3.5">
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ottiene o imposta l'intervallo di tempo durante il quale <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> accetta input contenente solo silenzio, prima di completare il riconoscimento.</summary>
        <value>La durata dell'intervallo di silenzio.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ogni riconoscimento vocale è disponibile un algoritmo per distinguere tra silenzio e riconoscimento vocale. Se l'input di riconoscimento è silenzio durante il periodo di timeout di inattività iniziale, il riconoscimento completa tale operazione di riconoscimento.  
  
-   Per le operazioni di riconoscimento asincrona ed emulazione, il riconoscimento genera il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> evento, in cui la <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InitialSilenceTimeout%2A?displayProperty=nameWithType> proprietà è `true`e il <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A?displayProperty=nameWithType> proprietà è `null`.  
  
-   Per operazioni di riconoscimento sincrono ed emulazione, restituisce il riconoscimento `null`, anziché un valore valido <xref:System.Speech.Recognition.RecognitionResult>.  
  
 Se l'intervallo di timeout di inattività iniziale è impostato su 0, il riconoscimento non esegue una verifica di timeout di inattività iniziale. L'intervallo di timeout può essere qualsiasi valore non negativo. Il valore predefinito è 0 secondi.  
  
   
  
## Examples  
 Nell'esempio seguente mostra parte di un'applicazione console che illustra il riconoscimento vocale di base. Nell'esempio viene impostata la <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> e <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> delle proprietà di un <xref:System.Speech.Recognition.SpeechRecognitionEngine> prima dell'avvio di riconoscimento vocale. Gestori per il riconoscimento vocale <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> e <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> eventi di output delle informazioni di evento nella console per dimostrare come la <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> delle proprietà di un <xref:System.Speech.Recognition.SpeechRecognitionEngine> tali proprietà influiscono sulle operazioni di riconoscimento.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Load a Grammar object.  
        recognizer.LoadGrammar(CreateServicesGrammar("FindServices"));  
  
        // Add event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(  
            AudioStateChangedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(3);  
        recognizer.BabbleTimeout = TimeSpan.FromSeconds(2);  
        recognizer.EndSilenceTimeout = TimeSpan.FromSeconds(1);  
        recognizer.EndSilenceTimeoutAmbiguous = TimeSpan.FromSeconds(1.5);  
  
        Console.WriteLine("BabbleTimeout: {0}", recognizer.BabbleTimeout);  
        Console.WriteLine("InitialSilenceTimeout: {0}", recognizer.InitialSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeout: {0}", recognizer.EndSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeoutAmbiguous: {0}", recognizer.EndSilenceTimeoutAmbiguous);  
        Console.WriteLine();  
  
        // Start asynchronous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Single);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Create a grammar and build it into a Grammar object.   
    static Grammar CreateServicesGrammar(string grammarName)  
    {  
  
      // Create a grammar for finding services in different cities.  
      Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
      Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
      GrammarBuilder findServices = new GrammarBuilder("Find");  
      findServices.Append(services);  
      findServices.Append("near");  
      findServices.Append(cities);  
  
      // Create a Grammar object from the GrammarBuilder..  
      Grammar servicesGrammar = new Grammar(findServices);  
      servicesGrammar.Name = ("FindServices");  
      return servicesGrammar;  
    }  
  
    // Handle the AudioStateChanged event.  
    static void AudioStateChangedHandler(  
      object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("AudioStateChanged ({0}): {1}",  
        DateTime.Now.ToString("mm:ss.f"), e.AudioState);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("RecognizeCompleted ({0}):",  
        DateTime.Now.ToString("mm:ss.f"));  
  
      string resultText;  
      if (e.Result != null) { resultText = e.Result.Text; }  
      else { resultText = "<null>"; }  
  
      Console.WriteLine(  
        " BabbleTimeout: {0}; InitialSilenceTimeout: {1}; Result text: {2}",  
        e.BabbleTimeout, e.InitialSilenceTimeout, resultText);  
      if (e.Error != null)  
      {  
        Console.WriteLine(" Exception message: ", e.Error.Message);  
      }  
  
      // Start the next asynchronous recognition operation.  
      ((SpeechRecognitionEngine)sender).RecognizeAsync(RecognizeMode.Single);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Questa proprietà è impostata su un valore minore di 0 secondi.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <Member MemberName="InstalledRecognizers">
      <MemberSignature Language="C#" Value="public static System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt; InstalledRecognizers ();" />
      <MemberSignature Language="ILAsm" Value=".method public static hidebysig class System.Collections.ObjectModel.ReadOnlyCollection`1&lt;class System.Speech.Recognition.RecognizerInfo&gt; InstalledRecognizers() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers" />
      <MemberSignature Language="VB.NET" Value="Public Shared Function InstalledRecognizers () As ReadOnlyCollection(Of RecognizerInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; static System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::RecognizerInfo ^&gt; ^ InstalledRecognizers();" />
      <MemberSignature Language="F#" Value="static member InstalledRecognizers : unit -&gt; System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt;" Usage="System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt;</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Restituisce informazioni per tutti sistemi di riconoscimento vocale installati nel sistema corrente.</summary>
        <returns>Raccolta di sola lettura di oggetti <see cref="T:System.Speech.Recognition.RecognizerInfo" /> che descrive i riconoscitori installati.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Per ottenere informazioni sul riconoscimento corrente, usare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo%2A> proprietà.  
  
   
  
## Examples  
 Nell'esempio seguente mostra parte di un'applicazione console che illustra il riconoscimento vocale di base. L'esempio Usa la raccolta restituita dal <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> metodo per trovare un riconoscimento vocale che supporta la lingua inglese.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Select a speech recognizer that supports English.  
      RecognizerInfo info = null;  
      foreach (RecognizerInfo ri in SpeechRecognitionEngine.InstalledRecognizers())  
      {  
        if (ri.Culture.TwoLetterISOLanguageName.Equals("en"))  
        {  
          info = ri;  
          break;  
        }  
      }  
      if (info == null) return;  
  
      // Create the selected recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(info))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammar">
      <MemberSignature Language="C#" Value="public void LoadGrammar (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void LoadGrammar(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void LoadGrammar(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.LoadGrammar : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognitionEngine.LoadGrammar grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">L'oggetto di grammatica da caricare.</param>
        <summary>Carica un oggetto <see cref="T:System.Speech.Recognition.Grammar" /> in modo sincrono.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il riconoscimento genera un'eccezione se il <xref:System.Speech.Recognition.Grammar> oggetto è già caricata, viene caricato in modo asincrono o non è riuscito a caricare in qualsiasi sistema di riconoscimento. Non è possibile caricare lo stesso <xref:System.Speech.Recognition.Grammar> oggetto in più istanze di <xref:System.Speech.Recognition.SpeechRecognitionEngine>. In alternativa, creare una nuova <xref:System.Speech.Recognition.Grammar> per ciascun <xref:System.Speech.Recognition.SpeechRecognitionEngine> istanza.  
  
 Se il riconoscimento è in esecuzione, le applicazioni devono utilizzare <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> sospendere il motore di riconoscimento vocale prima il caricamento, scaricamento, abilitazione o la disabilitazione di una grammatica.  
  
 Quando si carica una grammatica, si è abilitato per impostazione predefinita. Per disabilitare una grammatica di caricamento, usare il <xref:System.Speech.Recognition.Grammar.Enabled%2A> proprietà.  
  
 Per caricare un <xref:System.Speech.Recognition.Grammar> oggetto in modo asincrono, usare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> (metodo).  
  
   
  
## Examples  
 Nell'esempio seguente mostra parte di un'applicazione console che illustra il riconoscimento vocale di base. Nell'esempio viene creato un <xref:System.Speech.Recognition.DictationGrammar> e li carica in un riconoscimento vocale.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="Grammar" /> è <see langword="null" />.</exception>
        <exception cref="T:System.InvalidOperationException">
          <paramref name="Grammar" /> non è uno stato valido.</exception>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammarAsync">
      <MemberSignature Language="C#" Value="public void LoadGrammarAsync (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void LoadGrammarAsync(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void LoadGrammarAsync(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.LoadGrammarAsync : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognitionEngine.LoadGrammarAsync grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">La grammatica di riconoscimento vocale da caricare.</param>
        <summary>Carica in modo asincrono una grammatica di riconoscimento vocale.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Quando il riconoscimento viene completato il caricamento una <xref:System.Speech.Recognition.Grammar> dell'oggetto, genera un <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> evento. Il riconoscimento genera un'eccezione se il <xref:System.Speech.Recognition.Grammar> oggetto è già caricata, viene caricato in modo asincrono o non è riuscito a caricare in qualsiasi sistema di riconoscimento. Non è possibile caricare lo stesso <xref:System.Speech.Recognition.Grammar> oggetto in più istanze di <xref:System.Speech.Recognition.SpeechRecognitionEngine>. In alternativa, creare una nuova <xref:System.Speech.Recognition.Grammar> per ciascun <xref:System.Speech.Recognition.SpeechRecognitionEngine> istanza.  
  
 Se il riconoscimento è in esecuzione, le applicazioni devono utilizzare <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> sospendere il motore di riconoscimento vocale prima il caricamento, scaricamento, abilitazione o la disabilitazione di una grammatica.  
  
 Quando si carica una grammatica, si è abilitato per impostazione predefinita. Per disabilitare una grammatica di caricamento, usare il <xref:System.Speech.Recognition.Grammar.Enabled%2A> proprietà.  
  
 Per caricare in modo sincrono una grammatica di riconoscimento vocale, usare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> (metodo).  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="Grammar" /> è <see langword="null" />.</exception>
        <exception cref="T:System.InvalidOperationException">
          <paramref name="Grammar" /> non è uno stato valido.</exception>
        <exception cref="T:System.OperationCanceledException">Operazione asincrona annullata.</exception>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammarCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; LoadGrammarCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; LoadGrammarCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event LoadGrammarCompleted As EventHandler(Of LoadGrammarCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::LoadGrammarCompletedEventArgs ^&gt; ^ LoadGrammarCompleted;" />
      <MemberSignature Language="F#" Value="member this.LoadGrammarCompleted : EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; " Usage="member this.LoadGrammarCompleted : System.EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Generato quando <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> termina il caricamento asincrono di un oggetto <see cref="T:System.Speech.Recognition.Grammar" />.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il riconoscimento <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> metodo avvia un'operazione asincrona. Il <xref:System.Speech.Recognition.SpeechRecognitionEngine> genera questo evento quando viene completato l'operazione. Per ottenere il <xref:System.Speech.Recognition.Grammar> oggetto che il riconoscimento è stato caricato, usare il <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs.Grammar%2A> proprietà del controllo associato <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs>. Per ottenere l'oggetto corrente <xref:System.Speech.Recognition.Grammar> oggetti è stata caricata la sospensione del riconoscimento, usare il riconoscimento <xref:System.Speech.Recognition.SpeechRecognitionEngine.Grammars%2A> proprietà.  
  
 Se il riconoscimento è in esecuzione, le applicazioni devono utilizzare <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> sospendere il motore di riconoscimento vocale prima il caricamento, scaricamento, abilitazione o la disabilitazione di una grammatica.  
  
 Quando si crea un delegato <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted>, si identifica il metodo che gestirà l'evento. Per associare l'evento al gestore eventi in uso, aggiungere all'evento un'istanza del delegato. Il gestore eventi viene chiamato ogni volta che si verifica l'evento, a meno che non venga rimosso il delegato. Per altre informazioni sui delegati del gestore eventi, vedere [eventi e delegati](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Nell'esempio seguente crea un sistema di riconoscimento vocale in-process e quindi crea due tipi di grammatiche per il riconoscimento delle parole specifiche e per accettare la dettatura gratuita. Nell'esempio si costruisce una <xref:System.Speech.Recognition.Grammar> oggetto da ognuna delle grammatiche di riconoscimento vocale completati, quindi carica in modo asincrono il <xref:System.Speech.Recognition.Grammar> gli oggetti per il <xref:System.Speech.Recognition.SpeechRecognitionEngine> istanza. Gestori per il riconoscimento <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> e <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventi scrive nella console il nome del <xref:System.Speech.Recognition.Grammar> oggetto utilizzato per eseguire il riconoscimento e il testo del risultato del riconoscimento, rispettivamente.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and set its input.  
      recognizer = new SpeechRecognitionEngine();  
      recognizer.SetInputToDefaultAudioDevice();  
  
      // Add a handler for the LoadGrammarCompleted event.  
      recognizer.LoadGrammarCompleted +=  
        new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
      // Add a handler for the SpeechRecognized event.  
      recognizer.SpeechRecognized +=  
        new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
      // Create the "yesno" grammar.  
      Choices yesChoices = new Choices(new string[] { "yes", "yup", "yeah" });  
      SemanticResultValue yesValue =  
          new SemanticResultValue(yesChoices, (bool)true);  
      Choices noChoices = new Choices(new string[] { "no", "nope", "neah" });  
      SemanticResultValue noValue =  
          new SemanticResultValue(noChoices, (bool)false);  
      SemanticResultKey yesNoKey =  
          new SemanticResultKey("yesno", new Choices(new GrammarBuilder[] { yesValue, noValue }));  
      Grammar yesnoGrammar = new Grammar(yesNoKey);  
      yesnoGrammar.Name = "yesNo";  
  
      // Create the "done" grammar.  
      Grammar doneGrammar =  
        new Grammar(new Choices(new string[] { "done", "exit", "quit", "stop" }));  
      doneGrammar.Name = "Done";  
  
      // Create a dictation grammar.  
      Grammar dictation = new DictationGrammar();  
      dictation.Name = "Dictation";  
  
      // Load grammars to the recognizer.  
      recognizer.LoadGrammarAsync(yesnoGrammar);  
      recognizer.LoadGrammarAsync(doneGrammar);  
      recognizer.LoadGrammarAsync(dictation);  
  
      // Start asynchronous, continuous recognition.  
      recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
      // Keep the console window open.  
      Console.ReadLine();  
    }  
  
    // Handle the LoadGrammarCompleted event.   
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      string grammarName = e.Grammar.Name;  
      bool grammarLoaded = e.Grammar.Loaded;  
  
      if (e.Error != null)  
      {  
        Console.WriteLine("LoadGrammar for {0} failed with a {1}.",  
        grammarName, e.Error.GetType().Name);  
  
        // Add exception handling code here.  
      }  
  
      Console.WriteLine("Grammar {0} {1} loaded.",  
      grammarName, (grammarLoaded) ? "is" : "is not");  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Grammar({0}): {1}", e.Result.Grammar.Name, e.Result.Text);  
  
      // Add event handler code here.  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.LoadGrammarCompletedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.Grammars" />
      </Docs>
    </Member>
    <Member MemberName="MaxAlternates">
      <MemberSignature Language="C#" Value="public int MaxAlternates { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance int32 MaxAlternates" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" />
      <MemberSignature Language="VB.NET" Value="Public Property MaxAlternates As Integer" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property int MaxAlternates { int get(); void set(int value); };" />
      <MemberSignature Language="F#" Value="member this.MaxAlternates : int with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Int32</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ottiene o imposta il numero massimo di risultati del riconoscimento alternativi che <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> restituisce per ogni operazione di riconoscimento.</summary>
        <value>Il numero di risultati alternativi da restituire.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> proprietà del <xref:System.Speech.Recognition.RecognitionResult> classe contiene la raccolta di <xref:System.Speech.Recognition.RecognizedPhrase> gli oggetti che rappresentano le interpretazioni possibili dell'input.  
  
 Il valore predefinito per <xref:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates%2A> è 10.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">La proprietà <see cref="P:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" /> è impostata su un valore minore di 0.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
      </Docs>
    </Member>
    <Member MemberName="QueryRecognizerSetting">
      <MemberSignature Language="C#" Value="public object QueryRecognizerSetting (string settingName);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance object QueryRecognizerSetting(string settingName) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Function QueryRecognizerSetting (settingName As String) As Object" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Object ^ QueryRecognizerSetting(System::String ^ settingName);" />
      <MemberSignature Language="F#" Value="member this.QueryRecognizerSetting : string -&gt; obj" Usage="speechRecognitionEngine.QueryRecognizerSetting settingName" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Object</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="settingName">Nome dell'impostazione da restituire.</param>
        <summary>Restituisce i valori delle impostazioni per il riconoscimento.</summary>
        <returns>Valore dell'impostazione.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Stringa, numero intero a 64 bit o i dati degli indirizzi della memoria, possono contenere le impostazioni di sistema di riconoscimento. La tabella seguente descrive le impostazioni definite per un'API di riconoscimento vocale Microsoft (SAPI)-riconoscimento conforme. Le seguenti impostazioni devono avere lo stesso intervallo per ogni riconoscimento che supporta l'impostazione. Un sistema di riconoscimento conformi SAPI non è necessaria per supportare queste impostazioni e può supportare altre impostazioni.  
  
|nome|Descrizione|  
|----------|-----------------|  
|`ResourceUsage`|Specifica l'utilizzo della CPU del motore di riconoscimento. L'intervallo è compreso tra 0 e 100. Il valore predefinito è 50.|  
|`ResponseSpeed`|Indica la lunghezza di silenzio alla fine dell'input ambiguo prima del riconoscimento vocale viene completata un'operazione di riconoscimento. L'intervallo va da 0 a 10.000 millisecondi (ms). Questa impostazione corrisponde al riconoscimento <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> proprietà.  Default = 150 ms.|  
|`ComplexResponseSpeed`|Indica la lunghezza di silenzio alla fine dell'input ambiguo prima del riconoscimento vocale viene completata un'operazione di riconoscimento. L'intervallo va da 0 a 10.000 ms. Questa impostazione corrisponde al riconoscimento <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> proprietà. Default = 500 ms.|  
|`AdaptationOn`|Indica se l'adattamento del modello acustico è ON (valore = `1`) o OFF (valore = `0`). Il valore predefinito è `1` (via).|  
|`PersistedBackgroundAdaptation`|Indica se adattamento dello sfondo è impostata su ON (valore = `1`) o OFF (valore = `0`), e viene mantenuta l'impostazione del Registro di sistema. Il valore predefinito è `1` (via).|  
  
 Per aggiornare un'impostazione per il riconoscimento, usare uno del <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> metodi.  
  
   
  
## Examples  
 Nell'esempio seguente fa parte di un'applicazione console che restituisca i valori per un numero di impostazioni definiti per il riconoscimento che supporta le impostazioni locali en-US. L'esempio genera l'output seguente.  
  
```  
Settings for recognizer MS-1033-80-DESK:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 150  
  ComplexResponseSpeed           = 500  
  AdaptationOn                   = 1  
  PersistedBackgroundAdaptation  = 1  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace RecognizerSettings  
{  
  class Program  
  {  
    static readonly string[] settings = new string[] {  
      "ResourceUsage",  
      "ResponseSpeed",  
      "ComplexResponseSpeed",  
      "AdaptationOn",  
      "PersistedBackgroundAdaptation"  
    };  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        Console.WriteLine("Settings for recognizer {0}:",  
          recognizer.RecognizerInfo.Name);  
        Console.WriteLine();  
  
        foreach (string setting in settings)  
        {  
          try  
          {  
            object value = recognizer.QueryRecognizerSetting(setting);  
            Console.WriteLine("  {0,-30} = {1}", setting, value);  
          }  
          catch  
          {  
            Console.WriteLine("  {0,-30} is not supported by this recognizer.",  
              setting);  
          }  
        }  
      }  
      Console.WriteLine();  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="settingName" /> è <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="settingName" /> è la stringa vuota ("").</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">Il riconoscimento non ha un'impostazione con tale nome.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.Int32)" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
      </Docs>
    </Member>
    <MemberGroup MemberName="Recognize">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Avvia un'operazione sincrona di riconoscimento vocale.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Questi metodi eseguono un'operazione di riconoscimento sincrono, singolo. Il riconoscimento esegue questa operazione su relative grammatiche di riconoscimento vocale caricato e abilitato.  
  
 Durante una chiamata a questo metodo, il riconoscimento può generare gli eventi seguenti:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Generato quando il riconoscimento rileva un input identificabile come funzione vocale.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Generato quando l'input viene creata una corrispondenza ambigua con una delle grammatiche attive.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Generato quando il riconoscimento completa un'operazione di riconoscimento.  
  
 Il riconoscimento non genera il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> eventi quando si usa uno del <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> metodi.  
  
 Il <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> metodi restituiscono una <xref:System.Speech.Recognition.RecognitionResult> oggetto, o `null` se l'operazione non riesce o il riconoscimento non è abilitato.  
  
 Un'operazione sincrona di riconoscimento può non riuscire per i motivi seguenti:  
  
-   Vocale non è stato rilevato prima della scadono gli intervalli di timeout per il <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> delle proprietà, o per il `initialSilenceTimeout` parametro del <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> (metodo).  
  
-   Il motore di riconoscimento vocale rileva ma non trova corrispondenze in una qualsiasi delle caricato e abilitato <xref:System.Speech.Recognition.Grammar> oggetti.  
  
 Per modificare il riconoscimento gestisce il modo in cui la tempistica di riconoscimento vocale o inattività rispetto al riconoscimento, usare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> proprietà.  
  
 Il <xref:System.Speech.Recognition.SpeechRecognitionEngine> deve avere almeno un <xref:System.Speech.Recognition.Grammar> oggetto caricato prima di eseguire il riconoscimento. Per caricare una grammatica di riconoscimento vocale, usare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> (metodo).  
  
 Per eseguire il riconoscimento asincrono, usare uno del <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> metodi.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="Recognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult Recognize ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult Recognize() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
      <MemberSignature Language="VB.NET" Value="Public Function Recognize () As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ Recognize();" />
      <MemberSignature Language="F#" Value="member this.Recognize : unit -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.Recognize " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Effettua un'operazione sincrona di riconoscimento vocale.</summary>
        <returns>Il risultato di riconoscimento per l'input, o <see langword="null" /> se l'operazione non riesce o il riconoscimento non è abilitato.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Questo metodo esegue un'operazione di riconoscimento singolo. Il riconoscimento esegue questa operazione su relative grammatiche di riconoscimento vocale caricato e abilitato.  
  
 Durante una chiamata a questo metodo, il riconoscimento può generare gli eventi seguenti:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Generato quando il riconoscimento rileva un input identificabile come funzione vocale.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Generato quando l'input viene creata una corrispondenza ambigua con una delle grammatiche attive.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Generato quando il riconoscimento completa un'operazione di riconoscimento.  
  
 Il riconoscimento non genera il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> eventi quando si usa questo metodo.  
  
 Il <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize> metodo restituisce un <xref:System.Speech.Recognition.RecognitionResult> oggetto, o `null` se l'operazione non è riuscita.  
  
 Un'operazione sincrona di riconoscimento può non riuscire per i motivi seguenti:  
  
-   Riconoscimento vocale non è stato rilevato prima della scadono gli intervalli di timeout per il <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> proprietà.  
  
-   Il motore di riconoscimento vocale rileva ma non trova corrispondenze in una qualsiasi delle caricato e abilitato <xref:System.Speech.Recognition.Grammar> oggetti.  
  
 Per eseguire il riconoscimento asincrono, usare uno del <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> metodi.  
  
   
  
## Examples  
 Nell'esempio seguente mostra parte di un'applicazione console che illustra il riconoscimento vocale di base. Nell'esempio viene creato un <xref:System.Speech.Recognition.DictationGrammar>, li carica in un sistema di riconoscimento vocale in-process ed esegue un'operazione di riconoscimento.  
  
```  
  
using System;  
using System.Speech.Recognition;  
  
namespace SynchronousRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Modify the initial silence time-out value.  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(5);  
  
        // Start synchronous speech recognition.  
        RecognitionResult result = recognizer.Recognize();  
  
        if (result != null)  
        {  
          Console.WriteLine("Recognized text = {0}", result.Text);  
        }  
        else  
        {  
          Console.WriteLine("No recognition result available.");  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to continue...");  
      Console.ReadKey();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="Recognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult Recognize (TimeSpan initialSilenceTimeout);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult Recognize(valuetype System.TimeSpan initialSilenceTimeout) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize(System.TimeSpan)" />
      <MemberSignature Language="VB.NET" Value="Public Function Recognize (initialSilenceTimeout As TimeSpan) As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ Recognize(TimeSpan initialSilenceTimeout);" />
      <MemberSignature Language="F#" Value="member this.Recognize : TimeSpan -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.Recognize initialSilenceTimeout" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="initialSilenceTimeout" Type="System.TimeSpan" />
      </Parameters>
      <Docs>
        <param name="initialSilenceTimeout">L'intervallo di tempo in cui un riconoscimento vocale accetta l'input contenente solo il silenzio prima di finalizzare il riconoscimento.</param>
        <summary>Effettua un'operazione sincrona di riconoscimento vocale con un periodo di timeout di inattività iniziale specificato.</summary>
        <returns>Il risultato di riconoscimento per l'input, o <see langword="null" /> se l'operazione non riesce o il riconoscimento non è abilitato.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Se il motore di riconoscimento vocale rileva vocale entro l'intervallo di tempo specificato da `initialSilenceTimeout` argomento, <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%28System.TimeSpan%29> esegue un'operazione di riconoscimento singolo e quindi termina.  Il `initialSilenceTimeout` parametro sostituisce il riconoscimento <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> proprietà.  
  
 Durante una chiamata a questo metodo, il riconoscimento può generare gli eventi seguenti:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Generato quando il riconoscimento rileva un input identificabile come funzione vocale.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Generato quando l'input viene creata una corrispondenza ambigua con una delle grammatiche attive.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Generato quando il riconoscimento completa un'operazione di riconoscimento.  
  
 Il riconoscimento non genera il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> eventi quando si usa questo metodo.  
  
 Il <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize> metodo restituisce un <xref:System.Speech.Recognition.RecognitionResult> oggetto, o `null` se l'operazione non è riuscita.  
  
 Un'operazione sincrona di riconoscimento può non riuscire per i motivi seguenti:  
  
-   Riconoscimento vocale non è stato rilevato prima della scadono gli intervalli di timeout per il <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> o per il `initialSilenceTimeout` parametro.  
  
-   Il motore di riconoscimento vocale rileva ma non trova corrispondenze in una qualsiasi delle caricato e abilitato <xref:System.Speech.Recognition.Grammar> oggetti.  
  
 Per eseguire il riconoscimento asincrono, usare uno del <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> metodi.  
  
   
  
## Examples  
 Nell'esempio seguente mostra parte di un'applicazione console che illustra il riconoscimento vocale di base. Nell'esempio viene creato un <xref:System.Speech.Recognition.DictationGrammar>, li carica in un sistema di riconoscimento vocale in-process ed esegue un'operazione di riconoscimento.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SynchronousRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start synchronous speech recognition.  
        RecognitionResult result = recognizer.Recognize(TimeSpan.FromSeconds(5));  
  
        if (result != null)  
        {  
          Console.WriteLine("Recognized text = {0}", result.Text);  
        }  
        else  
        {  
          Console.WriteLine("No recognition result available.");  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to continue...");  
      Console.ReadKey();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      </Docs>
    </Member>
    <MemberGroup MemberName="RecognizeAsync">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Avvia un'operazione asincrona di riconoscimento vocale.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Questi metodi eseguono uno o più operazioni di riconoscimento asincrona. Il riconoscimento esegue ogni operazione su relative grammatiche di riconoscimento vocale caricato e abilitato.  
  
 Durante una chiamata a questo metodo, il riconoscimento può generare gli eventi seguenti:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Generato quando il riconoscimento rileva un input identificabile come funzione vocale.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Generato quando l'input viene creata una corrispondenza ambigua con una delle grammatiche attive.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Generato quando il riconoscimento completa un'operazione di riconoscimento.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. Eccezione generata quando un <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> al termine dell'operazione.  
  
 Per recuperare il risultato di un'operazione di riconoscimento asincrona, associare un gestore eventi per il riconoscimento <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> evento. Il riconoscimento genera questo evento ogni volta che viene completato correttamente un'operazione di riconoscimento sincrono o asincrono. Se il riconoscimento non è riuscito, il <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> proprietà sul <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> oggetto, che è possibile accedere nel gestore per il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> evento, saranno `null`.  
  
 Un'operazione di riconoscimento asincrona può non riuscire per i motivi seguenti:  
  
-   Riconoscimento vocale non è stato rilevato prima della scadono gli intervalli di timeout per il <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> proprietà.  
  
-   Il motore di riconoscimento vocale rileva ma non trova corrispondenze in una qualsiasi delle caricato e abilitato <xref:System.Speech.Recognition.Grammar> oggetti.  
  
-   Il <xref:System.Speech.Recognition.SpeechRecognitionEngine> deve avere almeno un <xref:System.Speech.Recognition.Grammar> oggetto caricato prima di eseguire il riconoscimento. Per caricare una grammatica di riconoscimento vocale, usare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> (metodo).  
  
-   Per modificare il riconoscimento gestisce il modo in cui la tempistica di riconoscimento vocale o inattività rispetto al riconoscimento, usare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> proprietà.  
  
-   Per eseguire il riconoscimento sincrono, usare uno del <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> metodi.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="RecognizeAsync">
      <MemberSignature Language="C#" Value="public void RecognizeAsync ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsync() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsync ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsync();" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsync : unit -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsync " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0">
          <AttributeName>System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Effettua un'unica operazione asincrona di riconoscimento vocale.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Questo metodo esegue un'operazione di riconoscimento asincrona, single. Il riconoscimento esegue l'operazione su relative grammatiche di riconoscimento vocale caricato e abilitato.  
  
 Durante una chiamata a questo metodo, il riconoscimento può generare gli eventi seguenti:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Generato quando il riconoscimento rileva un input identificabile come funzione vocale.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Generato quando l'input viene creata una corrispondenza ambigua con una delle grammatiche attive.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Generato quando il riconoscimento completa un'operazione di riconoscimento.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. Eccezione generata quando un <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> al termine dell'operazione.  
  
 Per recuperare il risultato di un'operazione di riconoscimento asincrona, associare un gestore eventi per il riconoscimento <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> evento. Il riconoscimento genera questo evento ogni volta che viene completato correttamente un'operazione di riconoscimento sincrono o asincrono. Se il riconoscimento non è riuscito, il <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> proprietà sul <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> oggetto, che è possibile accedere nel gestore per il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> evento, saranno `null`.  
  
 Per eseguire il riconoscimento sincrono, usare uno del <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> metodi.  
  
   
  
## Examples  
 Nell'esempio seguente mostra parte di un'applicazione console che illustra il riconoscimento vocale asincrono base. Nell'esempio viene creato un <xref:System.Speech.Recognition.DictationGrammar>, li carica in un sistema di riconoscimento vocale in-process ed esegue un'operazione di riconoscimento asincrona. I gestori eventi sono inclusi per illustrare gli eventi che il riconoscimento genera durante l'operazione.  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create a grammar for choosing cities for a flight.  
        Choices cities = new Choices(new string[]   
        { "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I want to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Construct a Grammar object and load it to the recognizer.  
        Grammar cityChooser = new Grammar(gb);  
        cityChooser.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(cityChooser);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer and start an asynchronous  
        // recognition operation.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        completed = false;  
        Console.WriteLine("Starting asynchronous recognition...");  
        recognizer.RecognizeAsync();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
        Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsync">
      <MemberSignature Language="C#" Value="public void RecognizeAsync (System.Speech.Recognition.RecognizeMode mode);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsync(valuetype System.Speech.Recognition.RecognizeMode mode) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync(System.Speech.Recognition.RecognizeMode)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsync (mode As RecognizeMode)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsync(System::Speech::Recognition::RecognizeMode mode);" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsync : System.Speech.Recognition.RecognizeMode -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsync mode" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="mode" Type="System.Speech.Recognition.RecognizeMode" />
      </Parameters>
      <Docs>
        <param name="mode">Indica se eseguire una o più operazioni di riconoscimento.</param>
        <summary>Effettua una o più operazioni asincrone di riconoscimento vocale.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Se `mode` viene <xref:System.Speech.Recognition.RecognizeMode.Multiple>, il riconoscimento continua a eseguire operazioni di riconoscimento asincrona finché il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> viene chiamato il metodo.  
  
 Durante una chiamata a questo metodo, il riconoscimento può generare gli eventi seguenti:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Generato quando il riconoscimento rileva un input identificabile come funzione vocale.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Generato quando l'input viene creata una corrispondenza ambigua con una delle grammatiche attive.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Generato quando il riconoscimento completa un'operazione di riconoscimento.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. Eccezione generata quando un <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> al termine dell'operazione.  
  
 Per recuperare il risultato di un'operazione di riconoscimento asincrona, associare un gestore eventi per il riconoscimento <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> evento. Il riconoscimento genera questo evento ogni volta che viene completato correttamente un'operazione di riconoscimento sincrono o asincrono. Se il riconoscimento non è riuscito, il <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> proprietà sul <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> oggetto, che è possibile accedere nel gestore per il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> evento, saranno `null`.  
  
 Un'operazione di riconoscimento asincrona può non riuscire per i motivi seguenti:  
  
-   Riconoscimento vocale non è stato rilevato prima della scadono gli intervalli di timeout per il <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> proprietà.  
  
-   Il motore di riconoscimento vocale rileva ma non trova corrispondenze in una qualsiasi delle caricato e abilitato <xref:System.Speech.Recognition.Grammar> oggetti.  
  
 Per eseguire il riconoscimento sincrono, usare uno del <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> metodi.  
  
   
  
## Examples  
 Nell'esempio seguente mostra parte di un'applicazione console che illustra il riconoscimento vocale asincrono base. Nell'esempio viene creato un <xref:System.Speech.Recognition.DictationGrammar>, li carica in un sistema di riconoscimento vocale in-process ed esegue più operazioni di riconoscimento asincrona. Le operazioni asincrone vengono annullate dopo 30 secondi. I gestori eventi sono inclusi per illustrare gli eventi che il riconoscimento genera durante l'operazione.  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create a grammar for choosing cities for a flight.  
        Choices cities = new Choices(new string[] { "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I want to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Construct a Grammar object and load it to the recognizer.  
        Grammar cityChooser = new Grammar(gb);  
        cityChooser.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(cityChooser);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer and start asynchronous  
        // recognition.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        completed = false;  
        Console.WriteLine("Starting asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 30 seconds, and then cancel asynchronous recognition.  
        Thread.Sleep(TimeSpan.FromSeconds(30));  
        recognizer.RecognizeAsyncCancel();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
        Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsyncCancel">
      <MemberSignature Language="C#" Value="public void RecognizeAsyncCancel ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsyncCancel() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsyncCancel ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsyncCancel();" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsyncCancel : unit -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsyncCancel " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Termina il riconoscimento asincrono senza attendere il completamento dell'operazione di riconoscimento corrente.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Questo metodo completa immediatamente il riconoscimento asincrono. Se l'operazione di riconoscimento asincrona corrente riceve input, l'input viene troncato e il completamento dell'operazione con l'input esistente. Il riconoscimento genera il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> evento quando un'operazione asincrona viene annullata e imposta il <xref:System.ComponentModel.AsyncCompletedEventArgs.Cancelled%2A> proprietà del <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> per `true`. Questo metodo annulla le operazioni asincrone avviate dal <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> e <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> metodi.  
  
 Per arrestare il riconoscimento asincrono senza troncamento di input, usare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> (metodo).  
  
   
  
## Examples  
 L'esempio seguente mostra parte di un'applicazione console che illustra l'uso della <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> (metodo). L'esempio crea e carica una grammatica di riconoscimento vocale, avvia un'operazione di riconoscimento asincrona continui e quindi sospende il 2 secondi prima di annullare l'operazione. Il riconoscimento riceve un input dal file, c:\temp\audioinput\sample.wav. I gestori eventi sono inclusi per illustrare gli eventi che il riconoscimento genera durante l'operazione.  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Begin asynchronous recognition from pre-recorded input.  
        recognizer.SetInputToWaveFile(@"c:\temp\audioinput\sample.wav");  
  
        completed = false;  
        Console.WriteLine("Begin continuing asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 2 seconds and then cancel the recognition operation.  
        Thread.Sleep(TimeSpan.FromSeconds(2));  
        recognizer.RecognizeAsyncCancel();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine(" - asynchronous operation canceled.");  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsyncStop">
      <MemberSignature Language="C#" Value="public void RecognizeAsyncStop ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsyncStop() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsyncStop ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsyncStop();" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsyncStop : unit -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsyncStop " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Arresta il riconoscimento asincrono dopo che l'operazione di riconoscimento corrente è stata completata.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Questo metodo consente di finalizzare il riconoscimento asincrono senza troncamento di input. Se l'operazione di riconoscimento asincrona corrente riceve input, il riconoscimento continua accettare input fino a quando non viene completata l'operazione di riconoscimento corrente. Il riconoscimento genera il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> evento quando un'operazione asincrona viene arrestata e imposta il <xref:System.ComponentModel.AsyncCompletedEventArgs.Cancelled%2A> proprietà del <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> per `true`. Questo metodo arresta operazioni asincrone avviate dal <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> e <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> metodi.  
  
 Per annullare immediatamente il riconoscimento asincrono con solo l'input esistente, usare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> (metodo).  
  
   
  
## Examples  
 L'esempio seguente mostra parte di un'applicazione console che illustra l'uso della <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> (metodo). L'esempio crea e carica una grammatica di riconoscimento vocale, avvia un'operazione di riconoscimento asincrona continui e quindi sospende il 2 secondi prima che venga interrotta l'operazione. Il riconoscimento riceve un input dal file, c:\temp\audioinput\sample.wav. I gestori eventi sono inclusi per illustrare gli eventi che il riconoscimento genera durante l'operazione.  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Begin asynchronous recognition from pre-recorded input.  
        recognizer.SetInputToWaveFile(@"c:\temp\audioinput\sample.wav");  
  
        completed = false;  
        Console.WriteLine("Begin continuing asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 2 seconds and then stop the recognition operation.  
        Thread.Sleep(TimeSpan.FromSeconds(2));  
        recognizer.RecognizeAsyncStop();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine(" - asynchronous operation canceled.");  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt; RecognizeCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.RecognizeCompletedEventArgs&gt; RecognizeCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event RecognizeCompleted As EventHandler(Of RecognizeCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::RecognizeCompletedEventArgs ^&gt; ^ RecognizeCompleted;" />
      <MemberSignature Language="F#" Value="member this.RecognizeCompleted : EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt; " Usage="member this.RecognizeCompleted : System.EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Generato quando <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> completa un'operazione di riconoscimento asincrona.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il <xref:System.Speech.Recognition.SpeechRecognitionEngine> dell'oggetto <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> metodo avvia un'operazione di riconoscimento asincrona. Quando il riconoscimento Finalizza l'operazione asincrona, genera questo evento.  
  
 Tramite il gestore per il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> evento, è possibile accedere la <xref:System.Speech.Recognition.RecognitionResult> nel <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> oggetto. Se non è riuscito, riconoscimento <xref:System.Speech.Recognition.RecognitionResult> saranno `null`. Per determinare se un timeout o un'interruzione nell'input audio causato il riconoscimento esito negativo, è possibile accedere alle proprietà per <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.BabbleTimeout%2A>, o <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InputStreamEnded%2A>.  
  
 Per ulteriori informazioni, vedere la classe <xref:System.Speech.Recognition.RecognizeCompletedEventArgs>.  
  
 Per ottenere informazioni dettagliate sui candidati ideali rifiutata il riconoscimento, collegare un gestore per il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> evento.  
  
 Quando si crea un delegato <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>, si identifica il metodo che gestirà l'evento. Per associare l'evento al gestore eventi in uso, aggiungere all'evento un'istanza del delegato. Il gestore eventi viene chiamato ogni volta che si verifica l'evento, a meno che non venga rimosso il delegato. Per altre informazioni sui delegati del gestore eventi, vedere [eventi e delegati](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Nell'esempio seguente riconosce le frasi, ad esempio "Visualizzare l'elenco degli artisti nella categoria jazz" o "Visualizzare gospel album". L'esempio Usa un gestore per il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> evento per visualizzare informazioni sui risultati del riconoscimento nella console.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display");  
        mediaMenu.Append("the list of", 0, 1);  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the", 0, 1);  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category.", 0, 1);  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(recognizer_RecognizeCompleted);  
        recognizer.LoadGrammarCompleted +=   
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void recognizer_RecognizeCompleted(object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted, error occurred during recognition: {0}", e.Error);  
        return;  
      }  
  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted: BabbleTimeout({0}), InitialSilenceTimeout({1}).",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted: AudioPosition({0}), InputStreamEnded({1}).",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
  
      if (e.Result != null)  
      {  
        Console.WriteLine("RecognizeCompleted:");  
        Console.WriteLine("  Grammar: " + e.Result.Grammar.Name);  
        Console.WriteLine("  Recognized text: " + e.Result.Text);  
        Console.WriteLine("  Confidence score: " + e.Result.Confidence);  
        Console.WriteLine("  Audio position: " + e.AudioPosition);  
      }  
  
      else  
      {  
        Console.WriteLine("RecognizeCompleted: No result.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded:  " + e.Grammar.Name);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognizeCompletedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerAudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan RecognizerAudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan RecognizerAudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property RecognizerAudioPosition As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan RecognizerAudioPosition { TimeSpan get(); };" />
      <MemberSignature Language="F#" Value="member this.RecognizerAudioPosition : TimeSpan" Usage="System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ottiene la posizione corrente di <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> nell'input audio in fase di elaborazione.</summary>
        <value>La posizione del riconoscimento nell'input audio in fase di elaborazione.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 La posizione audio è specifica per ogni riconoscimento vocale. Il valore zero di un flusso di input viene stabilito quando è abilitato.  
  
 Il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> riferimenti a proprietà di <xref:System.Speech.Recognition.SpeechRecognitionEngine> posizione dell'oggetto all'interno di relativo input audio. Al contrario, il <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> proprietà fa riferimento la posizione del dispositivo di input nel relativo flusso audio generato. Queste posizioni possono essere diverse. Ad esempio, se ha ricevuto il riconoscimento di input per i quali non ha ancora generato un risultato del riconoscimento, il valore della <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> proprietà è minore del valore del <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> proprietà.  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerInfo">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognizerInfo RecognizerInfo { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.Recognition.RecognizerInfo RecognizerInfo" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property RecognizerInfo As RecognizerInfo" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::RecognizerInfo ^ RecognizerInfo { System::Speech::Recognition::RecognizerInfo ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.RecognizerInfo : System.Speech.Recognition.RecognizerInfo" Usage="System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognizerInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ottiene informazioni sull'istanza corrente di <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>Informazioni sul riconoscimento vocale corrente.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Per ottenere informazioni su tutti i sistemi di riconoscimento vocale installate per il sistema corrente, usare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> (metodo).  
  
   
  
## Examples  
 Nell'esempio seguente ottiene un elenco parziale dei dati per il motore di riconoscimento vocale in-process corrente. Per ulteriori informazioni, vedere <xref:System.Speech.Recognition.RecognizerInfo>.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace RecognitionEngine  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer = new SpeechRecognitionEngine())  
      {  
        Console.WriteLine("Information for the current speech recognition engine:");  
        Console.WriteLine("  Name: {0}", recognizer.RecognizerInfo.Name);  
        Console.WriteLine("  Culture: {0}", recognizer.RecognizerInfo.Culture.ToString());  
        Console.WriteLine("  Description: {0}", recognizer.RecognizerInfo.Description);  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerUpdateReached">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; RecognizerUpdateReached;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; RecognizerUpdateReached" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
      <MemberSignature Language="VB.NET" Value="Public Event RecognizerUpdateReached As EventHandler(Of RecognizerUpdateReachedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::RecognizerUpdateReachedEventArgs ^&gt; ^ RecognizerUpdateReached;" />
      <MemberSignature Language="F#" Value="member this.RecognizerUpdateReached : EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; " Usage="member this.RecognizerUpdateReached : System.EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Generato quando un oggetto <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> in esecuzione viene sospeso per accettare le modifiche.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Le applicazioni devono utilizzare <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> sospendere un'istanza in esecuzione <xref:System.Speech.Recognition.SpeechRecognitionEngine> prima di modificare le impostazioni o dai relativi <xref:System.Speech.Recognition.Grammar> oggetti. Il <xref:System.Speech.Recognition.SpeechRecognitionEngine> genera questo evento quando è pronto per accettare le modifiche.  
  
 Ad esempio, se il <xref:System.Speech.Recognition.SpeechRecognitionEngine> è in pausa, è possibile caricare, scaricare, abilitare e disabilitare <xref:System.Speech.Recognition.Grammar> gli oggetti e modificare i valori per il <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> proprietà. Per altre informazioni, vedere il metodo <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A>.  
  
 Quando si crea un delegato <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached>, si identifica il metodo che gestirà l'evento. Per associare l'evento al gestore eventi in uso, aggiungere all'evento un'istanza del delegato. Il gestore eventi viene chiamato ogni volta che si verifica l'evento, a meno che non venga rimosso il delegato. Per altre informazioni sui delegati del gestore eventi, vedere [eventi e delegati](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 L'esempio seguente illustra un'applicazione console che carica e Scarica <xref:System.Speech.Recognition.Grammar> oggetti. L'applicazione usa il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> metodo per richiedere il motore di riconoscimento vocale per mettere in pausa per la ricezione di un aggiornamento. L'applicazione quindi carica o scarica un <xref:System.Speech.Recognition.Grammar> oggetto.  
  
 In ogni aggiornamento, un gestore per <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> eventi scrive il nome e lo stato di attualmente caricati <xref:System.Speech.Recognition.Grammar> gli oggetti nella console. Come le grammatiche vengono caricate e scaricate, l'applicazione prima di tutto riconosce i nomi di animali, i nomi di animali farm e i nomi dei frutti e quindi solo i nomi dei frutti.  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Collections.Generic;  
using System.Threading;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and configure its input.  
      using (recognizer = new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create the first grammar - Farm.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
  
        // Create the second grammar - Fruit.  
        Choices fruit = new Choices(new string[] { "apples", "peaches", "oranges" });  
        GrammarBuilder favorite = new GrammarBuilder(fruit);  
        Grammar favoriteFruit = new Grammar(favorite);  
        favoriteFruit.Name = "Fruit";  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizerUpdateReached +=  
          new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the Farm grammar.  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
        Console.WriteLine("Starting asynchronous, continuous recognition");  
        Console.WriteLine("  Farm grammar is loaded and enabled.");  
  
        // Pause to recognize farm animals.  
        Thread.Sleep(7000);  
        Console.WriteLine();  
  
        // Request an update and load the Fruit grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.LoadGrammarAsync(favoriteFruit);  
        Thread.Sleep(7000);  
  
        // Request an update and unload the Farm grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.UnloadGrammar(farmAnimals);  
        Thread.Sleep(7000);  
      }  
  
      // Keep the console window open.  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // At the update, get the names and enabled status of the currently loaded grammars.  
    public static void recognizer_RecognizerUpdateReached(  
      object sender, RecognizerUpdateReachedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Update reached:");  
      Thread.Sleep(1000);  
  
      string qualifier;  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      foreach (Grammar g in grammars)  
      {  
        qualifier = (g.Enabled) ? "enabled" : "disabled";  
        Console.WriteLine("  {0} grammar is loaded and {1}.",  
        g.Name, qualifier);  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("    Speech recognized: " + e.Result.Text);  
    }  
  
    // Write a message to the console when recognition fails.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("    Recognition attempt failed");  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognizerUpdateReachedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate" />
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
      </Docs>
    </Member>
    <MemberGroup MemberName="RequestRecognizerUpdate">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Richiede la sospensione del riconoscimento per aggiornarne lo stato.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Utilizzare questo metodo per sincronizzare le modifiche al sistema di riconoscimento. Ad esempio, se si carica o scarica una grammatica di riconoscimento vocale mentre il sistema di riconoscimento sta elaborando input, usare questo metodo e <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> sincronizzare il comportamento dell'applicazione con lo stato del motore di riconoscimento dell'evento.  
  
 Quando questo metodo viene chiamato, la sospensione del riconoscimento o completa di operazioni asincrone e genera un <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> evento. Oggetto <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> gestore eventi può quindi modificare lo stato del motore di riconoscimento tra operazioni di riconoscimento. Quando si gestiscono <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> eventi, il riconoscimento viene sospeso fino a quando non restituisce il gestore dell'evento.  
  
> [!NOTE]
>  Se l'input al riconoscimento viene modificato prima il riconoscimento genera il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> evento, la richiesta viene eliminata.  
  
 Quando viene chiamato questo metodo:  
  
-   Se il riconoscimento non elabora input, il riconoscimento genera immediatamente il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> evento.  
  
-   Se il riconoscimento Elabora input costituito da inattività o radiazione di fondo, la sospensione del riconoscimento l'operazione di riconoscimento e genera il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> evento.  
  
-   Se il riconoscimento Elabora input non costituiti da inattività o radiazione di fondo, il riconoscimento viene completata l'operazione di riconoscimento e genera quindi il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> evento.  
  
 Mentre il riconoscimento gestisce il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> evento:  
  
-   Il riconoscimento non elabora input e il valore della <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> proprietà rimane invariato.  
  
-   Il riconoscimento continua a raccogliere input e il valore della <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> proprietà può essere modificata.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate();" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : unit -&gt; unit" Usage="speechRecognitionEngine.RequestRecognizerUpdate " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Richiede la sospensione del riconoscimento per aggiornarne lo stato.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Quando il riconoscimento genera il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> evento, il <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> proprietà delle <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> è `null`.  
  
 Per fornire un token utente, usare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> (metodo). Per specificare un offset di posizione audio, usare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> (metodo).  
  
   
  
## Examples  
 L'esempio seguente illustra un'applicazione console che carica e Scarica <xref:System.Speech.Recognition.Grammar> oggetti. L'applicazione usa il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> metodo per richiedere il motore di riconoscimento vocale per mettere in pausa per la ricezione di un aggiornamento. L'applicazione quindi carica o scarica un <xref:System.Speech.Recognition.Grammar> oggetto.  
  
 In ogni aggiornamento, un gestore per <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> eventi scrive il nome e lo stato di attualmente caricati <xref:System.Speech.Recognition.Grammar> gli oggetti nella console. Come le grammatiche vengono caricate e scaricate, l'applicazione prima di tutto riconosce i nomi di animali, i nomi di animali farm e i nomi dei frutti e quindi solo i nomi dei frutti.  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Collections.Generic;  
using System.Threading;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and configure its input.  
      using (recognizer = new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create the first grammar - Farm.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
  
        // Create the second grammar - Fruit.  
        Choices fruit = new Choices(new string[] { "apples", "peaches", "oranges" });  
        GrammarBuilder favorite = new GrammarBuilder(fruit);  
        Grammar favoriteFruit = new Grammar(favorite);  
        favoriteFruit.Name = "Fruit";  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizerUpdateReached +=  
          new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the Farm grammar.  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
        Console.WriteLine("Starting asynchronous, continuous recognition");  
        Console.WriteLine("  Farm grammar is loaded and enabled.");  
  
        // Pause to recognize farm animals.  
        Thread.Sleep(7000);  
        Console.WriteLine();  
  
        // Request an update and load the Fruit grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.LoadGrammarAsync(favoriteFruit);  
        Thread.Sleep(7000);  
  
        // Request an update and unload the Farm grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.UnloadGrammar(farmAnimals);  
        Thread.Sleep(7000);  
      }  
  
      // Keep the console window open.  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // At the update, get the names and enabled status of the currently loaded grammars.  
    public static void recognizer_RecognizerUpdateReached(  
      object sender, RecognizerUpdateReachedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Update reached:");  
      Thread.Sleep(1000);  
  
      string qualifier;  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      foreach (Grammar g in grammars)  
      {  
        qualifier = (g.Enabled) ? "enabled" : "disabled";  
        Console.WriteLine("  {0} grammar is loaded and {1}.",  
        g.Name, qualifier);  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("    Speech recognized: " + e.Result.Text);  
    }  
  
    // Write a message to the console when recognition fails.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("    Recognition attempt failed");  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate (object userToken);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate(object userToken) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate(System.Object)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate (userToken As Object)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate(System::Object ^ userToken);" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : obj -&gt; unit" Usage="speechRecognitionEngine.RequestRecognizerUpdate userToken" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="userToken" Type="System.Object" />
      </Parameters>
      <Docs>
        <param name="userToken">Informazioni definite dall'utente che contengono informazioni per l'operazione.</param>
        <summary>Richiede la sospensione del riconoscimento per aggiornarne lo stato e fornire un token utente per l'evento associato.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Quando il riconoscimento genera il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> evento, il <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> proprietà del <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> contiene il valore della `userToken` parametro.  
  
 Per specificare un offset di posizione audio, usare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> (metodo).  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate (object userToken, TimeSpan audioPositionAheadToRaiseUpdate);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate(object userToken, valuetype System.TimeSpan audioPositionAheadToRaiseUpdate) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate(System.Object,System.TimeSpan)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate (userToken As Object, audioPositionAheadToRaiseUpdate As TimeSpan)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate(System::Object ^ userToken, TimeSpan audioPositionAheadToRaiseUpdate);" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : obj * TimeSpan -&gt; unit" Usage="speechRecognitionEngine.RequestRecognizerUpdate (userToken, audioPositionAheadToRaiseUpdate)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="userToken" Type="System.Object" />
        <Parameter Name="audioPositionAheadToRaiseUpdate" Type="System.TimeSpan" />
      </Parameters>
      <Docs>
        <param name="userToken">Informazioni definite dall'utente che contengono informazioni per l'operazione.</param>
        <param name="audioPositionAheadToRaiseUpdate">L'offset dall'oggetto <see cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" /> corrente per ritardare la richiesta.</param>
        <summary>Richiede la sospensione del riconoscimento per aggiornarne lo stato e fornire un offset e un token utente per l'evento associato.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il riconoscimento non avvierà la richiesta di aggiornamento per il riconoscimento fino al riconoscimento <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> corrente è uguale <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> plus `audioPositionAheadToRaiseUpdate`.  
  
 Quando il riconoscimento genera il <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> evento, il <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> proprietà del <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> contiene il valore della `userToken` parametro.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToAudioStream">
      <MemberSignature Language="C#" Value="public void SetInputToAudioStream (System.IO.Stream audioSource, System.Speech.AudioFormat.SpeechAudioFormatInfo audioFormat);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToAudioStream(class System.IO.Stream audioSource, class System.Speech.AudioFormat.SpeechAudioFormatInfo audioFormat) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToAudioStream (audioSource As Stream, audioFormat As SpeechAudioFormatInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToAudioStream(System::IO::Stream ^ audioSource, System::Speech::AudioFormat::SpeechAudioFormatInfo ^ audioFormat);" />
      <MemberSignature Language="F#" Value="member this.SetInputToAudioStream : System.IO.Stream * System.Speech.AudioFormat.SpeechAudioFormatInfo -&gt; unit" Usage="speechRecognitionEngine.SetInputToAudioStream (audioSource, audioFormat)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="audioSource" Type="System.IO.Stream" />
        <Parameter Name="audioFormat" Type="System.Speech.AudioFormat.SpeechAudioFormatInfo" />
      </Parameters>
      <Docs>
        <param name="audioSource">Flusso di input audio.</param>
        <param name="audioFormat">Il formato dell'input audio.</param>
        <summary>Configura l'oggetto <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> per ricevere l'input da un flusso audio.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Se il riconoscimento raggiunge la fine del flusso di input durante un'operazione di riconoscimento, Finalizza l'operazione di riconoscimento con l'input disponibile. Qualsiasi operazione di riconoscimento successive possono generare un'eccezione, a meno che non si aggiorna l'input al riconoscimento.  
  
   
  
## Examples  
 Nell'esempio seguente mostra parte di un'applicazione console che illustra il riconoscimento vocale di base. L'esempio utilizza l'input da un file audio, example.wav, che contiene le frasi, "test o test uno due tre" e "vendita cooper", separati da una pausa. L'esempio genera l'output seguente.  
  
```  
  
Starting asynchronous recognition...  
  Recognized text =  Testing testing 123  
  Recognized text =  Mr. Cooper  
  End of stream encountered.  
Done.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.IO;  
using System.Speech.AudioFormat;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace InputExamples  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Create and load a grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Configure the input to the recognizer.  
        recognizer.SetInputToAudioStream(  
          File.OpenRead(@"c:\temp\audioinput\example.wav"),  
          new SpeechAudioFormatInfo(  
            44100, AudioBitsPerSample.Sixteen, AudioChannel.Mono));  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Perform recognition of the whole file.  
        Console.WriteLine("Starting asynchronous recognition...");  
        completed = false;  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine("  Error encountered, {0}: {1}",  
          e.Error.GetType().Name, e.Error.Message);  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine("  End of stream encountered.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToDefaultAudioDevice">
      <MemberSignature Language="C#" Value="public void SetInputToDefaultAudioDevice ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToDefaultAudioDevice() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToDefaultAudioDevice ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToDefaultAudioDevice();" />
      <MemberSignature Language="F#" Value="member this.SetInputToDefaultAudioDevice : unit -&gt; unit" Usage="speechRecognitionEngine.SetInputToDefaultAudioDevice " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Configura l'oggetto <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> per ricevere l'input dal dispositivo audio predefinito.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Examples  
 Nell'esempio seguente mostra parte di un'applicazione console che illustra il riconoscimento vocale di base. L'esempio Usa l'output dal dispositivo audio predefinito, consente di eseguire più operazioni di riconoscimento asincrona e viene chiuso quando un utente utters la frase "exit".  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace DefaultInput  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition has finished.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Create and load the exit grammar.  
        Grammar exitGrammar = new Grammar(new GrammarBuilder("exit"));  
        exitGrammar.Name = "Exit Grammar";  
        recognizer.LoadGrammar(exitGrammar);  
  
        // Create and load the dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers to the recognizer.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Begin asynchronous recognition.  
        Console.WriteLine("Starting recognition...");  
        completed = false;  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait for recognition to finish.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("  Speech recognized:");  
      string grammarName = "<not available>";  
      if (e.Result.Grammar.Name != null &&  
        !e.Result.Grammar.Name.Equals(string.Empty))  
      {  
        grammarName = e.Result.Grammar.Name;  
      }  
      Console.WriteLine("    {0,-17} - {1}",  
        grammarName, e.Result.Text);  
  
      if (grammarName.Equals("Exit Grammar"))  
      {  
        ((SpeechRecognitionEngine)sender).RecognizeAsyncCancel();  
      }  
    }  
  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("  Recognition completed.");  
      completed = true;  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToNull">
      <MemberSignature Language="C#" Value="public void SetInputToNull ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToNull() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToNull ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToNull();" />
      <MemberSignature Language="F#" Value="member this.SetInputToNull : unit -&gt; unit" Usage="speechRecognitionEngine.SetInputToNull " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Disabilita l'input del riconoscimento vocale.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Configurare il <xref:System.Speech.Recognition.SpeechRecognitionEngine> oggetto per nessun input quando si usano i <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> e <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> metodi, o quando si acquisisce un motore di riconoscimento temporaneamente non in linea.  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToWaveFile">
      <MemberSignature Language="C#" Value="public void SetInputToWaveFile (string path);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToWaveFile(string path) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToWaveFile (path As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToWaveFile(System::String ^ path);" />
      <MemberSignature Language="F#" Value="member this.SetInputToWaveFile : string -&gt; unit" Usage="speechRecognitionEngine.SetInputToWaveFile path" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="path" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="path">Percorso del file da utilizzare come input.</param>
        <summary>Configura l'oggetto <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> per ricevere l'input da un file in formato audio Waveform (.wav).</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Se il riconoscimento raggiunge la fine del file di input durante un'operazione di riconoscimento, Finalizza l'operazione di riconoscimento con l'input disponibile. Qualsiasi operazione di riconoscimento successive possono generare un'eccezione, a meno che non si aggiorna l'input al riconoscimento.  
  
   
  
## Examples  
 Nell'esempio seguente esegue il riconoscimento di audio in un file wav e scrive il testo riconosciuto nella console.  
  
```  
using System;  
using System.IO;  
using System.Speech.Recognition;  
using System.Speech.AudioFormat;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static bool completed;  
  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create and load a grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Configure the input to the recognizer.  
recognizer.SetInputToWaveFile(@"c:\temp\SampleWAVInput.wav");  
  
        // Attach event handlers for the results of recognition.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizeCompleted +=   
          new EventHandler<RecognizeCompletedEventArgs>(recognizer_RecognizeCompleted);  
  
        // Perform recognition on the entire file.  
        Console.WriteLine("Starting asynchronous recognition...");  
        completed = false;  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        while (!completed)  
        {  
          Console.ReadLine();  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void recognizer_RecognizeCompleted(object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine("  Error encountered, {0}: {1}",  
        e.Error.GetType().Name, e.Error.Message);  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine("  End of stream encountered.");  
      }  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToWaveStream">
      <MemberSignature Language="C#" Value="public void SetInputToWaveStream (System.IO.Stream audioSource);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToWaveStream(class System.IO.Stream audioSource) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToWaveStream (audioSource As Stream)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToWaveStream(System::IO::Stream ^ audioSource);" />
      <MemberSignature Language="F#" Value="member this.SetInputToWaveStream : System.IO.Stream -&gt; unit" Usage="speechRecognitionEngine.SetInputToWaveStream audioSource" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="audioSource" Type="System.IO.Stream" />
      </Parameters>
      <Docs>
        <param name="audioSource">Flusso contenente i dati audio.</param>
        <summary>Configura l'oggetto <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> per ricevere l'input da un flusso che contiene i dati in formato audio Waveform (.wav).</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Se il riconoscimento raggiunge la fine del flusso di input durante un'operazione di riconoscimento, Finalizza l'operazione di riconoscimento con l'input disponibile. Qualsiasi operazione di riconoscimento successive possono generare un'eccezione, a meno che non si aggiorna l'input al riconoscimento.  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SpeechDetected">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; SpeechDetected;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechDetectedEventArgs&gt; SpeechDetected" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechDetected As EventHandler(Of SpeechDetectedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechDetectedEventArgs ^&gt; ^ SpeechDetected;" />
      <MemberSignature Language="F#" Value="member this.SpeechDetected : EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; " Usage="member this.SpeechDetected : System.EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Generato quando <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> rileva un input identificabile come funzione vocale.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ogni riconoscimento vocale è disponibile un algoritmo per distinguere tra silenzio e riconoscimento vocale. Quando la <xref:System.Speech.Recognition.SpeechRecognitionEngine> esegue un'operazione di riconoscimento vocale, genera il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> evento quando l'algoritmo identifica l'input come funzione vocale. Il <xref:System.Speech.Recognition.SpeechDetectedEventArgs.AudioPosition%2A> proprietà del controllo associato <xref:System.Speech.Recognition.SpeechDetectedEventArgs> oggetto indica una posizione nel flusso di input in cui il riconoscimento rilevato il riconoscimento vocale. Il <xref:System.Speech.Recognition.SpeechRecognitionEngine> genera il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> evento prima che venga generato uno qualsiasi delle <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>, o <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> gli eventi.  
  
 Per altre informazioni vedere la <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> metodi.  
  
 Quando si crea un delegato <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, si identifica il metodo che gestirà l'evento. Per associare l'evento al gestore eventi in uso, aggiungere all'evento un'istanza del delegato. Il gestore eventi viene chiamato ogni volta che si verifica l'evento, a meno che non venga rimosso il delegato. Per altre informazioni sui delegati del gestore eventi, vedere [eventi e delegati](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Nell'esempio seguente fa parte di un'applicazione console per la scelta di origine e destinazione città per un volo. L'applicazione riconosca le frasi, ad esempio "Voglio entrata da Miami a Chicago."  L'esempio Usa la <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> evento al report il <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> viene rilevato il parlato ogni ora.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create a grammar.  
        Choices cities = new Choices(new string[] {   
          "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I would like to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Create a Grammar object and load it to the recognizer.  
        Grammar g = new Grammar(gb);  
        g.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(g);  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechDetected event.  
    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine("  Speech detected at AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("  Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="SpeechHypothesized">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; SpeechHypothesized;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; SpeechHypothesized" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event SpeechHypothesized As EventHandler(Of SpeechHypothesizedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechHypothesizedEventArgs ^&gt; ^ SpeechHypothesized;" />
      <MemberSignature Language="F#" Value="member this.SpeechHypothesized : EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; " Usage="member this.SpeechHypothesized : System.EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Generato quando <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> ha riconosciuto una o più parole che possono essere componenti di più frasi complete in una grammatica.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il <xref:System.Speech.Recognition.SpeechRecognitionEngine> genera numerosi <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> eventi perché tenta di identificare una frase di input. È possibile accedere al testo di frasi parzialmente riconosciute nel <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> proprietà del <xref:System.Speech.Recognition.SpeechHypothesizedEventArgs> oggetto nel gestore per il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> evento. In genere, la gestione di questi eventi è utile solo per il debug.  
  
 <xref:System.Speech.Recognition.SpeechHypothesizedEventArgs> deriva da <xref:System.Speech.Recognition.RecognitionEventArgs>.  
  
 Per altre informazioni vedere la <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> proprietà e i <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> metodi.  
  
 Quando si crea un delegato <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, si identifica il metodo che gestirà l'evento. Per associare l'evento al gestore eventi in uso, aggiungere all'evento un'istanza del delegato. Il gestore eventi viene chiamato ogni volta che si verifica l'evento, a meno che non venga rimosso il delegato. Per altre informazioni sui delegati del gestore eventi, vedere [eventi e delegati](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Nell'esempio seguente riconosce le frasi, ad esempio "Visualizza l'elenco degli artisti nella categoria jazz". Nell'esempio viene usato il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> evento per visualizzare i frammenti incompleto frase nella console di come vengono riconosciute.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create a grammar.  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display the list of");  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the");  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category.");  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(recognizer_SpeechHypothesized);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void recognizer_SpeechHypothesized(object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine("Speech hypothesized: " + e.Result.Text);  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
      Console.WriteLine();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine();   
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="T:System.Speech.Recognition.RecognitionEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="SpeechRecognitionRejected">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; SpeechRecognitionRejected;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; SpeechRecognitionRejected" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechRecognitionRejected As EventHandler(Of SpeechRecognitionRejectedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechRecognitionRejectedEventArgs ^&gt; ^ SpeechRecognitionRejected;" />
      <MemberSignature Language="F#" Value="member this.SpeechRecognitionRejected : EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; " Usage="member this.SpeechRecognitionRejected : System.EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Generato quando <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> riceve input che non corrisponde ad alcun oggetto <see cref="T:System.Speech.Recognition.Grammar" /> caricato e abilitato.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 L'evento viene generato il riconoscimento, se determina che l'input non corrisponde con fiducia sufficiente qualsiasi caricato e abilitato <xref:System.Speech.Recognition.Grammar> oggetti. Il <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> proprietà del <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> contiene rifiutato <xref:System.Speech.Recognition.RecognitionResult> oggetto. È possibile usare il gestore per il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> eventi per recuperare il riconoscimento <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> che sono stati rifiutati e i relativi <xref:System.Speech.Recognition.RecognizedPhrase.Confidence%2A> punteggi.  
  
 Se l'applicazione usa una <xref:System.Speech.Recognition.SpeechRecognitionEngine> istanza, è possibile modificare il livello di confidenza quali sintesi e riconoscimento vocale è accettato o rifiutato con uno degli input di <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> metodi. È possibile modificare come il riconoscimento vocale risponde a non a funzionalità di input usando la <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> proprietà.  
  
 Quando si crea un delegato <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, si identifica il metodo che gestirà l'evento. Per associare l'evento al gestore eventi in uso, aggiungere all'evento un'istanza del delegato. Il gestore eventi viene chiamato ogni volta che si verifica l'evento, a meno che non venga rimosso il delegato. Per altre informazioni sui delegati del gestore eventi, vedere [eventi e delegati](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Nell'esempio seguente riconosce le frasi, ad esempio "Visualizzare l'elenco degli artisti nella categoria jazz" o "Visualizzare gospel album". L'esempio Usa un gestore per il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> eventi per visualizzare una notifica nella console quando il riconoscimento vocale di input non corrisponde al contenuto della grammatica con sufficienti <xref:System.Speech.Recognition.RecognizedPhrase.Confidence%2A> per produrre un riconoscimento corretto. Il gestore visualizza anche del risultato del riconoscimento <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> che sono stati rifiutati a causa di punteggi di confidenza basso in.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create a grammar.  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display");  
        mediaMenu.Append("the list of", 0, 1);  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the", 0, 1);  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category", 0, 1);  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("Speech input was rejected.");  
      foreach (RecognizedPhrase phrase in e.Result.Alternates)  
      {  
      Console.WriteLine("  Rejected phrase: " + phrase.Text);  
      Console.WriteLine("  Confidence score: " + phrase.Confidence);  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
      Console.WriteLine("  Confidence score: " + e.Result.Confidence);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="SpeechRecognized">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; SpeechRecognized;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechRecognizedEventArgs&gt; SpeechRecognized" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechRecognized As EventHandler(Of SpeechRecognizedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechRecognizedEventArgs ^&gt; ^ SpeechRecognized;" />
      <MemberSignature Language="F#" Value="member this.SpeechRecognized : EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; " Usage="member this.SpeechRecognized : System.EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Generato quando <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> riceve input che corrisponde a un oggetto <see cref="T:System.Speech.Recognition.Grammar" /> caricato e abilitato.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 È possibile avviare un'operazione di riconoscimento usando uno del <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> metodi. Il riconoscimento genera il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> evento se determina che input corrisponda a uno dei relativi caricato <xref:System.Speech.Recognition.Grammar> oggetti con un livello sufficiente di confidenza costituire il riconoscimento. Il <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> proprietà del <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> contiene accettato <xref:System.Speech.Recognition.RecognitionResult> oggetto. Gestori dei <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> gli eventi possono ottenere la frase riconosciuta, nonché un elenco di riconoscimento <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> con punteggi di confidenza inferiore.  
  
 Se l'applicazione usa una <xref:System.Speech.Recognition.SpeechRecognitionEngine> istanza, è possibile modificare il livello di confidenza quali sintesi e riconoscimento vocale è accettato o rifiutato con uno degli input di <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> metodi.  È possibile modificare come il riconoscimento vocale risponde a non a funzionalità di input usando la <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> proprietà.  
  
 Quando il riconoscimento riceve input che corrisponde a una grammatica, il <xref:System.Speech.Recognition.Grammar> possono generare oggetti relativo <xref:System.Speech.Recognition.Grammar.SpeechRecognized> evento. Il <xref:System.Speech.Recognition.Grammar> dell'oggetto <xref:System.Speech.Recognition.Grammar.SpeechRecognized> evento viene generato prima del riconoscimento vocale <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> evento. Qualsiasi attività specifiche a una grammatica particolare devono essere sempre eseguite da un gestore per il <xref:System.Speech.Recognition.Grammar.SpeechRecognized> evento.  
  
 Quando si crea un delegato <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>, si identifica il metodo che gestirà l'evento. Per associare l'evento al gestore eventi in uso, aggiungere all'evento un'istanza del delegato. Il gestore eventi viene chiamato ogni volta che si verifica l'evento, a meno che non venga rimosso il delegato. Per altre informazioni sui delegati del gestore eventi, vedere [eventi e delegati](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Nell'esempio seguente fa parte di un'applicazione console che crea la grammatica di riconoscimento vocale, costrutti una <xref:System.Speech.Recognition.Grammar> dell'oggetto e viene caricato il <xref:System.Speech.Recognition.SpeechRecognitionEngine> per eseguire il riconoscimento. Nell'esempio viene illustrato l'input vocale di un <xref:System.Speech.Recognition.SpeechRecognitionEngine>, i risultati di riconoscimento associati e gli eventi associati generati dal riconoscimento vocale.  
  
 Parlata input, ad esempio "I want to entrata da Chicago a Miami" attiverà un <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> evento. Pronuncia della frase "Pilotare me da Houston a Chicago" non attiverà una <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventi.  
  
 L'esempio Usa un gestore per il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> evento da visualizzare è stato riconosciuto frasi e la semantica contengono nella console.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer = new SpeechRecognitionEngine())  
      {  
  
        // Create SemanticResultValue objects that contain cities and airport codes.  
        SemanticResultValue chicago = new SemanticResultValue("Chicago", "ORD");  
        SemanticResultValue boston = new SemanticResultValue("Boston", "BOS");  
        SemanticResultValue miami = new SemanticResultValue("Miami", "MIA");  
        SemanticResultValue dallas = new SemanticResultValue("Dallas", "DFW");  
  
        // Create a Choices object and add the SemanticResultValue objects, using  
        // implicit conversion from SemanticResultValue to GrammarBuilder  
        Choices cities = new Choices();  
        cities.Add(new Choices(new GrammarBuilder[] { chicago, boston, miami, dallas }));  
  
        // Build the phrase and add SemanticResultKeys.  
        GrammarBuilder chooseCities = new GrammarBuilder();  
        chooseCities.Append("I want to fly from");  
        chooseCities.Append(new SemanticResultKey("origin", cities));  
        chooseCities.Append("to");  
        chooseCities.Append(new SemanticResultKey("destination", cities));  
  
        // Build a Grammar object from the GrammarBuilder.  
        Grammar bookFlight = new Grammar(chooseCities);  
        bookFlight.Name = "Book Flight";  
  
        // Add a handler for the LoadGrammarCompleted event.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Add a handler for the SpeechRecognized event.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(bookFlight);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
      Console.WriteLine();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized:  " + e.Result.Text);  
      Console.WriteLine();  
      Console.WriteLine("Semantic results:");  
      Console.WriteLine("  The flight origin is " + e.Result.Semantics["origin"].Value);  
      Console.WriteLine("  The flight destination is " + e.Result.Semantics["destination"].Value);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="T:System.Speech.Recognition.RecognitionEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="UnloadAllGrammars">
      <MemberSignature Language="C#" Value="public void UnloadAllGrammars ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UnloadAllGrammars() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
      <MemberSignature Language="VB.NET" Value="Public Sub UnloadAllGrammars ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UnloadAllGrammars();" />
      <MemberSignature Language="F#" Value="member this.UnloadAllGrammars : unit -&gt; unit" Usage="speechRecognitionEngine.UnloadAllGrammars " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Scarica tutti gli oggetti <see cref="T:System.Speech.Recognition.Grammar" /> dal riconoscimento.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Se il sistema di riconoscimento sta attualmente caricando un <xref:System.Speech.Recognition.Grammar> in modo asincrono, questo metodo attende fino a quando il <xref:System.Speech.Recognition.Grammar> viene caricato, prima che venga scaricato tutte le <xref:System.Speech.Recognition.Grammar> oggetti dal <xref:System.Speech.Recognition.SpeechRecognitionEngine> istanza.  
  
 Per scaricare una grammatica specifica, utilizzare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar%2A> (metodo).  
  
   
  
## Examples  
 Nell'esempio seguente mostra parte di un'applicazione console che illustra il caricamento sincrono e lo scaricamento delle grammatiche di riconoscimento vocale.  
  
```  
Loading grammars...  
Loaded grammars:  
 - Grammar1  
 - Grammar2  
 - Grammar3  
  
Unloading Grammar1...  
Loaded grammars:  
 - Grammar2  
 - Grammar3  
  
Unloading all grammars...  
No grammars loaded.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Collections.Generic;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace UnloadGrammars  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        Console.WriteLine("Loading grammars...");  
  
        // Create and load a number of grammars.  
        Grammar grammar1 = new Grammar(new GrammarBuilder("first grammar"));  
        grammar1.Name = "Grammar1";  
        recognizer.LoadGrammar(grammar1);  
  
        Grammar grammar2 = new Grammar(new GrammarBuilder("second grammar"));  
        grammar2.Name = "Grammar2";  
        recognizer.LoadGrammar(grammar2);  
  
        Grammar grammar3 = new Grammar(new GrammarBuilder("third grammar"));  
        grammar3.Name = "Grammar3";  
        recognizer.LoadGrammar(grammar3);  
  
        // List the recognizer's loaded grammars.  
        ListGrammars(recognizer);  
  
        // Unload one grammar and list the loaded grammars.  
        Console.WriteLine("Unloading Grammar1...");  
        recognizer.UnloadGrammar(grammar1);  
        ListGrammars(recognizer);  
  
        // Unload all grammars and list the loaded grammars.  
        Console.WriteLine("Unloading all grammars...");  
        recognizer.UnloadAllGrammars();  
        ListGrammars(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListGrammars(SpeechRecognitionEngine recognizer)  
    {  
      // Make a copy of the recognizer's grammar collection.  
      List<Grammar> loadedGrammars = new List<Grammar>(recognizer.Grammars);  
  
      if (loadedGrammars.Count > 0)  
      {  
        Console.WriteLine("Loaded grammars:");  
        foreach (Grammar g in recognizer.Grammars)  
        {  
          Console.WriteLine(" - {0}", g.Name);  
        }  
      }  
      else  
      {  
        Console.WriteLine("No grammars loaded.");  
      }  
      Console.WriteLine();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="UnloadGrammar">
      <MemberSignature Language="C#" Value="public void UnloadGrammar (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UnloadGrammar(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UnloadGrammar(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.UnloadGrammar : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognitionEngine.UnloadGrammar grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">L'oggetto di grammatica di cui annullare il caricamento.</param>
        <summary>Scarica un oggetto <see cref="T:System.Speech.Recognition.Grammar" /> specificato dall'istanza <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Se il riconoscimento è in esecuzione, le applicazioni devono utilizzare <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> sospendere la <xref:System.Speech.Recognition.SpeechRecognitionEngine> istanza prima del caricamento, scaricamento, abilitazione o disabilitazione di un <xref:System.Speech.Recognition.Grammar> oggetto. Per scaricare tutti i <xref:System.Speech.Recognition.Grammar> oggetti, usare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars%2A> (metodo).  
  
   
  
## Examples  
 Nell'esempio seguente mostra parte di un'applicazione console che illustra il caricamento sincrono e lo scaricamento delle grammatiche di riconoscimento vocale.  
  
```  
Loading grammars...  
Loaded grammars:  
 - Grammar1  
 - Grammar2  
 - Grammar3  
  
Unloading Grammar1...  
Loaded grammars:  
 - Grammar2  
 - Grammar3  
  
Unloading all grammars...  
No grammars loaded.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Collections.Generic;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace UnloadGrammars  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        Console.WriteLine("Loading grammars...");  
  
        // Create and load a number of grammars.  
        Grammar grammar1 = new Grammar(new GrammarBuilder("first grammar"));  
        grammar1.Name = "Grammar1";  
        recognizer.LoadGrammar(grammar1);  
  
        Grammar grammar2 = new Grammar(new GrammarBuilder("second grammar"));  
        grammar2.Name = "Grammar2";  
        recognizer.LoadGrammar(grammar2);  
  
        Grammar grammar3 = new Grammar(new GrammarBuilder("third grammar"));  
        grammar3.Name = "Grammar3";  
        recognizer.LoadGrammar(grammar3);  
  
        // List the recognizer's loaded grammars.  
        ListGrammars(recognizer);  
  
        // Unload one grammar and list the loaded grammars.  
        Console.WriteLine("Unloading Grammar1...");  
        recognizer.UnloadGrammar(grammar1);  
        ListGrammars(recognizer);  
  
        // Unload all grammars and list the loaded grammars.  
        Console.WriteLine("Unloading all grammars...");  
        recognizer.UnloadAllGrammars();  
        ListGrammars(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListGrammars(SpeechRecognitionEngine recognizer)  
    {  
      // Make a copy of the recognizer's grammar collection.  
      List<Grammar> loadedGrammars = new List<Grammar>(recognizer.Grammars);  
  
      if (loadedGrammars.Count > 0)  
      {  
        Console.WriteLine("Loaded grammars:");  
        foreach (Grammar g in recognizer.Grammars)  
        {  
          Console.WriteLine(" - {0}", g.Name);  
        }  
      }  
      else  
      {  
        Console.WriteLine("No grammars loaded.");  
      }  
      Console.WriteLine();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="Grammar" /> è <see langword="null" />.</exception>
        <exception cref="T:System.InvalidOperationException">La grammatica non viene caricata nel sistema di riconoscimento o il sistema di riconoscimento sta al momento caricando la grammatica in modo asincrono.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
      </Docs>
    </Member>
    <MemberGroup MemberName="UpdateRecognizerSetting">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Aggiorna il valore di un'impostazione per il riconoscimento.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Stringa, numero intero a 64 bit o i dati degli indirizzi della memoria, possono contenere le impostazioni di sistema di riconoscimento. La tabella seguente descrive le impostazioni definite per un'API di riconoscimento vocale Microsoft (SAPI)-riconoscimento conforme. Le seguenti impostazioni devono avere lo stesso intervallo per ogni riconoscimento che supporta l'impostazione. Un sistema di riconoscimento conformi SAPI non è necessaria per supportare queste impostazioni e può supportare altre impostazioni.  
  
|nome|Descrizione|  
|----------|-----------------|  
|`ResourceUsage`|Specifica l'utilizzo della CPU del motore di riconoscimento. L'intervallo è compreso tra 0 e 100. Il valore predefinito è 50.|  
|`ResponseSpeed`|Indica la lunghezza di silenzio alla fine dell'input ambiguo prima del riconoscimento vocale viene completata un'operazione di riconoscimento. L'intervallo va da 0 a 10.000 millisecondi (ms). Questa impostazione corrisponde al riconoscimento <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> proprietà. Default = 150 ms.|  
|`ComplexResponseSpeed`|Indica la lunghezza di inattività espresso in millisecondi (ms) alla fine dell'input ambiguo prima del riconoscimento vocale viene completata un'operazione di riconoscimento. L'intervallo va da 0 a 10.000 ms. Questa impostazione corrisponde al riconoscimento <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> proprietà. Default = 500 ms.|  
|`AdaptationOn`|Indica se l'adattamento del modello acustico è ON (valore = `1`) o OFF (valore = `0`). Il valore predefinito è `1` (via).|  
|`PersistedBackgroundAdaptation`|Indica se adattamento dello sfondo è impostata su ON (valore = `1`) o OFF (valore = `0`), e viene mantenuta l'impostazione del Registro di sistema. Il valore predefinito è `1` (via).|  
  
 Per restituire una delle impostazioni del motore di riconoscimento, usare il <xref:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting%2A> (metodo).  
  
 Ad eccezione di `PersistedBackgroundAdaptation`, i valori delle proprietà impostati utilizzando il <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> metodi rimangono valide solo per l'istanza corrente di <xref:System.Speech.Recognition.SpeechRecognitionEngine>, dopo cui ripristinare le impostazioni predefinite.  
  
 È possibile modificare come il riconoscimento vocale risponde a non a funzionalità di input usando la <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> proprietà.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="UpdateRecognizerSetting">
      <MemberSignature Language="C#" Value="public void UpdateRecognizerSetting (string settingName, int updatedValue);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UpdateRecognizerSetting(string settingName, int32 updatedValue) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.Int32)" />
      <MemberSignature Language="VB.NET" Value="Public Sub UpdateRecognizerSetting (settingName As String, updatedValue As Integer)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UpdateRecognizerSetting(System::String ^ settingName, int updatedValue);" />
      <MemberSignature Language="F#" Value="member this.UpdateRecognizerSetting : string * int -&gt; unit" Usage="speechRecognitionEngine.UpdateRecognizerSetting (settingName, updatedValue)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
        <Parameter Name="updatedValue" Type="System.Int32" />
      </Parameters>
      <Docs>
        <param name="settingName">Nome dell'impostazione da aggiornare.</param>
        <param name="updatedValue">Il nuovo valore per l'impostazione.</param>
        <summary>Aggiorna l'impostazione specificata per <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> con il valore Integer specificato.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ad eccezione di `PersistedBackgroundAdaptation`, i valori delle proprietà impostati utilizzando il <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> metodo rimangono valide solo per l'istanza corrente di <xref:System.Speech.Recognition.SpeechRecognitionEngine>, dopo cui ripristinare le impostazioni predefinite. Vedere <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> per le descrizioni delle impostazioni supportate.  
  
   
  
## Examples  
 Nell'esempio seguente fa parte di un'applicazione console che restituisca i valori per un numero di impostazioni definiti per il riconoscimento che supporta le impostazioni locali en-US. L'esempio aggiorna le impostazioni a livello di confidenza e successivamente sottoposto a query la sospensione del riconoscimento per controllare i valori aggiornati. L'esempio genera l'output seguente.  
  
```  
Settings for recognizer MS-1033-80-DESK:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 150  
  ComplexResponseSpeed           = 500  
  AdaptationOn                   = 1  
  PersistedBackgroundAdaptation  = 1  
  
Updated settings:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 200  
  ComplexResponseSpeed           = 300  
  AdaptationOn                   = 0  
  PersistedBackgroundAdaptation  = 0  
  
Press any key to exit...  
```  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace RecognizerSettings  
{  
  class Program  
  {  
    static readonly string[] settings = new string[] {  
      "ResourceUsage",  
      "ResponseSpeed",  
      "ComplexResponseSpeed",  
      "AdaptationOn",  
      "PersistedBackgroundAdaptation",  
    };  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        Console.WriteLine("Settings for recognizer {0}:",  
          recognizer.RecognizerInfo.Name);  
        Console.WriteLine();  
  
        // List the current settings.  
        ListSettings(recognizer);  
  
        // Change some of the settings.  
        recognizer.UpdateRecognizerSetting("ResponseSpeed", 200);  
        recognizer.UpdateRecognizerSetting("ComplexResponseSpeed", 300);  
        recognizer.UpdateRecognizerSetting("AdaptationOn", 1);  
        recognizer.UpdateRecognizerSetting("PersistedBackgroundAdaptation", 0);  
  
        Console.WriteLine("Updated settings:");  
        Console.WriteLine();  
  
        // List the updated settings.  
        ListSettings(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListSettings(SpeechRecognitionEngine recognizer)  
    {  
      foreach (string setting in settings)  
      {  
        try  
        {  
          object value = recognizer.QueryRecognizerSetting(setting);  
          Console.WriteLine("  {0,-30} = {1}", setting, value);  
        }  
        catch  
        {  
          Console.WriteLine("  {0,-30} is not supported by this recognizer.",  
            setting);  
        }  
      }  
      Console.WriteLine();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="settingName" /> è <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="settingName" /> è la stringa vuota ("").</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">Il riconoscimento non ha un'impostazione con tale nome.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="UpdateRecognizerSetting">
      <MemberSignature Language="C#" Value="public void UpdateRecognizerSetting (string settingName, string updatedValue);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UpdateRecognizerSetting(string settingName, string updatedValue) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub UpdateRecognizerSetting (settingName As String, updatedValue As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UpdateRecognizerSetting(System::String ^ settingName, System::String ^ updatedValue);" />
      <MemberSignature Language="F#" Value="member this.UpdateRecognizerSetting : string * string -&gt; unit" Usage="speechRecognitionEngine.UpdateRecognizerSetting (settingName, updatedValue)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
        <Parameter Name="updatedValue" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="settingName">Nome dell'impostazione da aggiornare.</param>
        <param name="updatedValue">Il nuovo valore per l'impostazione.</param>
        <summary>Aggiorna l'impostazione specificata del motore di riconoscimento vocale con il valore di stringa specificato.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ad eccezione di `PersistedBackgroundAdaptation`, i valori delle proprietà impostati utilizzando il <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> metodo rimangono valide solo per l'istanza corrente di <xref:System.Speech.Recognition.SpeechRecognitionEngine>, dopo cui ripristinare le impostazioni predefinite. Vedere <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> per le descrizioni delle impostazioni supportate.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="settingName" /> è <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="settingName" /> è la stringa vuota ("").</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">Il riconoscimento non ha un'impostazione con tale nome.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      </Docs>
    </Member>
  </Members>
</Type>
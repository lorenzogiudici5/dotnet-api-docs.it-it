<Type Name="SpeechRecognizer" FullName="System.Speech.Recognition.SpeechRecognizer">
  <Metadata>
    <Meta Name="ms.openlocfilehash" Value="bc77257cd77c3fc2c078698df4cc6e968d3bf09a" />
    <Meta Name="ms.sourcegitcommit" Value="d31dc2ede16f6f7bc64e90d9f897ff54c4e3869b" />
    <Meta Name="ms.translationtype" Value="HT" />
    <Meta Name="ms.contentlocale" Value="it-IT" />
    <Meta Name="ms.lasthandoff" Value="04/03/2018" />
    <Meta Name="ms.locfileid" Value="30531235" />
  </Metadata>
  <TypeSignature Language="C#" Value="public class SpeechRecognizer : IDisposable" />
  <TypeSignature Language="ILAsm" Value=".class public auto ansi beforefieldinit SpeechRecognizer extends System.Object implements class System.IDisposable" />
  <TypeSignature Language="DocId" Value="T:System.Speech.Recognition.SpeechRecognizer" />
  <TypeSignature Language="VB.NET" Value="Public Class SpeechRecognizer&#xA;Implements IDisposable" />
  <TypeSignature Language="C++ CLI" Value="public ref class SpeechRecognizer : IDisposable" />
  <AssemblyInfo>
    <AssemblyName>System.Speech</AssemblyName>
    <AssemblyVersion>4.0.0.0</AssemblyVersion>
  </AssemblyInfo>
  <Base>
    <BaseTypeName>System.Object</BaseTypeName>
  </Base>
  <Interfaces>
    <Interface>
      <InterfaceName>System.IDisposable</InterfaceName>
    </Interface>
  </Interfaces>
  <Docs>
    <summary>Fornisce l'accesso al servizio condiviso di riconoscimento vocale disponibile sul desktop di Windows.</summary>
    <remarks>
      <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Applicazioni utilizzano il riconoscimento condiviso per accedere a Windows di riconoscimento vocale. Utilizzare il <xref:System.Speech.Recognition.SpeechRecognizer> oggetto da aggiungere all'utente di Windows vocale.  
  
 Questa classe è possibile controllare vari aspetti del processo di riconoscimento vocale:  
  
-   Per gestire le grammatiche riconoscimento vocale, utilizzare il <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammar%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars%2A>, e <xref:System.Speech.Recognition.SpeechRecognizer.Grammars%2A>.  
  
-   Per ottenere informazioni sul riconoscimento vocale corrente di operazioni di riconoscimento, sottoscrivere il <xref:System.Speech.Recognition.SpeechRecognizer>del <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected>, e <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> eventi.  
  
-   Per visualizzare o modificare il numero di risultati alternativi restituisce il riconoscimento, utilizzare il <xref:System.Speech.Recognition.SpeechRecognizer.MaxAlternates%2A> proprietà. Il riconoscimento restituisce i risultati del riconoscimento in un <xref:System.Speech.Recognition.RecognitionResult> oggetto.  
  
-   Per accedere o controllare lo stato di riconoscimento condiviso, utilizzare il <xref:System.Speech.Recognition.SpeechRecognizer.AudioLevel%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.AudioState%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.Enabled%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A>, e <xref:System.Speech.Recognition.SpeechRecognizer.State%2A> proprietà e <xref:System.Speech.Recognition.SpeechRecognizer.AudioLevelUpdated>, <xref:System.Speech.Recognition.SpeechRecognizer.AudioSignalProblemOccurred>, <xref:System.Speech.Recognition.SpeechRecognizer.AudioStateChanged>, e <xref:System.Speech.Recognition.SpeechRecognizer.StateChanged> eventi.  
  
-   Per sincronizzare le modifiche al sistema di riconoscimento, utilizzare il <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> metodo. Il riconoscimento condiviso utilizza più di un thread per eseguire attività.  
  
-   Per emulare l'input per il riconoscimento condiviso, utilizzare il <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A> e <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> metodi.  
  
 La configurazione del riconoscimento vocale Windows viene gestita dall'uso del **proprietà di riconoscimento vocale** nella finestra di dialogo di **Pannello di controllo**. Questa interfaccia viene utilizzata per selezionare il motore di riconoscimento vocale desktop predefinito e lingua, il dispositivo audio di input e il comportamento di sospensione del riconoscimento vocale. Se viene modificata la configurazione di Windows il riconoscimento vocale mentre è in esecuzione l'applicazione, (ad esempio, se il riconoscimento vocale è disabilitato o modificato la lingua di input), la modifica interesserà tutti <xref:System.Speech.Recognition.SpeechRecognizer> oggetti.  
  
 Per creare un riconoscimento in-process che sia indipendente dal riconoscimento vocale Windows, utilizzare la <xref:System.Speech.Recognition.SpeechRecognitionEngine> classe.  
  
> [!NOTE]
>  Chiamare sempre il metodo <xref:System.Speech.Recognition.SpeechRecognizer.Dispose%2A> prima di rilasciare l'ultimo riferimento a riconoscimento vocale. In caso contrario, le risorse utilizzate non vengono liberate finché il garbage collector chiama l'oggetto di riconoscimento `Finalize` metodo.  
  
   
  
## Examples  
 Nell'esempio seguente fa parte di un'applicazione console che viene caricata una grammatica di riconoscimento vocale e input emulata asincrona, i risultati di riconoscimento associati e gli eventi associati generati dal riconoscimento vocale.  Se il riconoscimento vocale Windows non è in esecuzione, quindi avviare l'applicazione verrà avviata il riconoscimento vocale Windows. Se il riconoscimento vocale Windows il **Sleeping** stato, quindi <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> restituisce sempre null.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SharedRecognizer  
{  
  class Program  
  {  
  
    // Indicate whether the asynchronous emulate recognition  
    // operation has completed.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
  
      // Initialize an instance of the shared recognizer.  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
        recognizer.LoadGrammar(testGrammar);  
  
        // Attach event handlers for recognition events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        completed = false;  
  
        // Start asynchronous emulated recognition.   
        // This matches the grammar and generates a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing testing");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        completed = false;  
  
        // Start asynchronous emulated recognition.  
        // This does not match the grammar or generate a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing one two three");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null)  
      {  
        Console.WriteLine("Recognition result = {0}",  
          e.Result.Text ?? "<no text>");  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }  
  
    // Handle the SpeechRecognizeCompleted event.  
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      if (e.Result == null)  
      {  
        Console.WriteLine("No result generated.");  
      }  
  
      // Indicate the asynchronous operation is complete.  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
    </remarks>
    <altmember cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />
    <altmember cref="T:System.Speech.Recognition.Grammar" />
    <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
  </Docs>
  <Members>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognizer ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.#ctor" />
      <MemberSignature Language="VB.NET" Value="Public Sub New ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognizer();" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters />
      <Docs>
        <summary>Inizializza una nuova istanza della classe <see cref="T:System.Speech.Recognition.SpeechRecognizer" />.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ogni <xref:System.Speech.Recognition.SpeechRecognizer> oggetto gestisce un set separato di grammatiche riconoscimento vocale.  
  
   
  
## Examples  
 Nell'esempio seguente fa parte di un'applicazione console che viene caricata una grammatica di riconoscimento vocale e input emulata asincrona, i risultati di riconoscimento associati e gli eventi associati generati dal riconoscimento vocale. Se il riconoscimento vocale Windows non è in esecuzione, quindi avviare l'applicazione verrà avviata il riconoscimento vocale Windows. Se il riconoscimento vocale Windows il **Sleeping** stato, quindi <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> restituisce sempre null.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SharedRecognizer  
{  
  class Program  
  {  
  
    // Indicate whether the asynchronous emulate recognition  
    // operation has completed.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
  
      // Initialize an instance of the shared recognizer.  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
        recognizer.LoadGrammar(testGrammar);  
  
        // Attach event handlers for recognition events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        completed = false;  
  
        // Start asynchronous emulated recognition.   
        // This matches the grammar and generates a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing testing");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        completed = false;  
  
        // Start asynchronous emulated recognition.  
        // This does not match the grammar or generate a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing one two three");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null)  
      {  
        Console.WriteLine("Recognition result = {0}",  
          e.Result.Text ?? "<no text>");  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }  
  
    // Handle the SpeechRecognizeCompleted event.  
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      if (e.Result == null)  
      {  
        Console.WriteLine("No result generated.");  
      }  
  
      // Indicate the asynchronous operation is complete.  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
      </Docs>
    </Member>
    <Member MemberName="AudioFormat">
      <MemberSignature Language="C#" Value="public System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.AudioFormat" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioFormat As SpeechAudioFormatInfo" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::AudioFormat::SpeechAudioFormatInfo ^ AudioFormat { System::Speech::AudioFormat::SpeechAudioFormatInfo ^ get(); };" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.AudioFormat.SpeechAudioFormatInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ottiene il formato dell'audio ricevuto dal riconoscimento vocale.</summary>
        <value>Il formato di input audio per il riconoscimento vocale o <see langword="null" /> se l'input al riconoscimento non è configurato.</value>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <Member MemberName="AudioLevel">
      <MemberSignature Language="C#" Value="public int AudioLevel { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance int32 AudioLevel" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.AudioLevel" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioLevel As Integer" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property int AudioLevel { int get(); };" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Int32</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ottiene il livello dell'audio ricevuto dal riconoscimento vocale.</summary>
        <value>Livello audio dell'input del riconoscimento vocale, da 0 a 100.</value>
        <remarks>To be added.</remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.AudioLevelUpdated" />
      </Docs>
    </Member>
    <Member MemberName="AudioLevelUpdated">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; AudioLevelUpdated;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; AudioLevelUpdated" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.AudioLevelUpdated" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioLevelUpdated As EventHandler(Of AudioLevelUpdatedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioLevelUpdatedEventArgs ^&gt; ^ AudioLevelUpdated;" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Viene generato quando il riconoscimento condiviso segnala il livello del relativo input audio.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il riconoscimento ha generato l'evento più volte al secondo. La frequenza con cui l'evento viene generato varia a seconda del computer in cui è in esecuzione l'applicazione.  
  
 Per ottenere il livello audio al momento dell'evento, utilizzare il <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs.AudioLevel%2A> proprietà dell'oggetto associato <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs>. Per ottenere il livello corrente audio di input per il riconoscimento, usare il riconoscimento <xref:System.Speech.Recognition.SpeechRecognizer.AudioLevel%2A> proprietà.  
  
 Quando si crea un delegato per un `AudioLevelUpdated` evento, si identifica il metodo che gestirà l'evento. Per associare l'evento al gestore eventi in uso, aggiungere all'evento un'istanza del delegato. Il gestore eventi viene chiamato ogni volta che si verifica l'evento, a meno che non venga rimosso il delegato. Per ulteriori informazioni sui delegati del gestore eventi, vedere [eventi e delegati](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Nell'esempio seguente aggiunge un gestore per il `AudioLevelUpdated` evento da un <xref:System.Speech.Recognition.SpeechRecognizer> oggetto. Il gestore restituisce il nuovo livello audio nella console.  
  
```csharp  
private SpeechRecognizer recognizer;  
  
// Initialize the SpeechRecognizer object.   
private void Initialize()  
{  
  recognizer = new SpeechRecognizer();  
  
  // Add an event handler for the AudioLevelUpdated event.  
  recognizer.AudioLevelUpdated +=   
    new EventHandler<AudioLevelUpdatedEventArgs>(recognizer_AudioLevelUpdated);  
  
  // Add other initialization code here.  
  
}  
  
// Write the audio level to the console when the AudioLevelUpdated event is raised.  
void recognizer_AudioLevelUpdated(object sender, AudioLevelUpdatedEventArgs e)  
{  
  Console.WriteLine("The audio level is now: {0}.", e.AudioLevel);  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioLevelUpdatedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioLevel" />
      </Docs>
    </Member>
    <Member MemberName="AudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan AudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan AudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioPosition As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan AudioPosition { TimeSpan get(); };" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ottiene la posizione corrente nel flusso audio generato dal dispositivo che fornisce l'input al riconoscimento vocale.</summary>
        <value>La posizione corrente nel flusso di input audio del riconoscimento vocale attraverso il quale è stato ricevuto l'input.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il riconoscimento condiviso riceve l'input mentre è in esecuzione il riconoscimento vocale desktop.  
  
 Il `AudioPosition` proprietà fa riferimento la posizione del dispositivo di input nel proprio flusso audio generato. Al contrario, il <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> proprietà fa riferimento la posizione del riconoscimento nell'elaborazione dell'input audio. Queste posizioni possono essere diverse.  Ad esempio, se ha ricevuto il riconoscimento di input per i quali non ha ancora generato un risultato di riconoscimento, il valore del <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> proprietà è minore del valore del <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> proprietà.  
  
   
  
## Examples  
 Nell'esempio seguente, il riconoscimento vocale condiviso Usa una grammatica dettatura per associare input vocale. Un gestore per il <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected> evento scrive nella console di <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A>, e <xref:System.Speech.Recognition.SpeechRecognizer.AudioLevel%2A> quando il riconoscimento vocale ha rilevato vocale relativi input.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognizer recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize a shared speech recognition engine.  
      recognizer = new SpeechRecognizer();  
  
      // Add handlers for events.  
      recognizer.LoadGrammarCompleted +=   
        new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
      recognizer.SpeechRecognized +=   
        new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
      recognizer.StateChanged +=   
        new EventHandler<StateChangedEventArgs>(recognizer_StateChanged);  
      recognizer.SpeechDetected +=   
        new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  
  
      // Create a dictation grammar.  
      Grammar dictation = new DictationGrammar();  
      dictation.Name = "Dictation";  
  
      // Load the grammar object to the recognizer.  
      recognizer.LoadGrammarAsync(dictation);  
  
      // Keep the console window open.  
      Console.ReadLine();  
    }  
  
    // Gather information about detected speech and write it to the console.  
    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Speech detected:");  
      Console.WriteLine("  Audio level: " + recognizer.AudioLevel);  
      Console.WriteLine("  Audio position: " + recognizer.AudioPosition);  
      Console.WriteLine("  Recognizer audio position: " + recognizer.RecognizerAudioPosition);  
    }  
  
    // Write the text of the recognition result to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {   
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
  
      // Add event handler code here.  
    }  
  
    // Write the name of the loaded grammar to the console.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Put the shared speech recognizer into "listening" mode.  
    static void recognizer_StateChanged(object sender, StateChangedEventArgs e)  
    {  
      if (e.RecognizerState != RecognizerState.Stopped)  
      {  
        recognizer.EmulateRecognizeAsync("Start listening");  
      }  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="AudioSignalProblemOccurred">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; AudioSignalProblemOccurred;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; AudioSignalProblemOccurred" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.AudioSignalProblemOccurred" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioSignalProblemOccurred As EventHandler(Of AudioSignalProblemOccurredEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioSignalProblemOccurredEventArgs ^&gt; ^ AudioSignalProblemOccurred;" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Viene generato quando il riconoscimento rileva un problema nel segnale audio.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Per ottenere si è verificato il problema, utilizzare il <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs.AudioSignalProblem%2A> proprietà dell'oggetto associato <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs>.  
  
 Quando si crea un delegato per un `AudioSignalProblemOccurred` evento, si identifica il metodo che gestirà l'evento. Per associare l'evento al gestore eventi in uso, aggiungere all'evento un'istanza del delegato. Il gestore eventi viene chiamato ogni volta che si verifica l'evento, a meno che non venga rimosso il delegato. Per ulteriori informazioni sui delegati del gestore eventi, vedere [eventi e delegati](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 L'esempio seguente definisce un gestore eventi che raccoglie informazioni su un `AudioSignalProblemOccurred` evento.  
  
```  
private SpeechRecognizer recognizer;  
  
// Initialize the speech recognition engine.  
private void Initialize()  
{  
  recognizer = new SpeechRecognizer();  
  
  // Add a handler for the AudioSignalProblemOccurred event.  
  recognizer.AudioSignalProblemOccurred +=   
    new EventHandler<AudioSignalProblemOccurredEventArgs>(  
      recognizer_AudioSignalProblemOccurred);  
}  
  
// Gather information when the AudioSignalProblemOccurred event is raised.  
void recognizer_AudioSignalProblemOccurred(object sender, AudioSignalProblemOccurredEventArgs e)  
{  
  StringBuilder details = new StringBuilder();  
  
  details.AppendLine("Audio signal problem information:");  
  details.AppendFormat(  
    " Audio level:               {0}" + Environment.NewLine +  
    " Audio position:            {1}" + Environment.NewLine +  
    " Audio signal problem:      {2}" + Environment.NewLine +  
    " Recognition engine audio position: {3}" + Environment.NewLine,  
    e.AudioLevel, e.AudioPosition,  e.AudioSignalProblem,  
    e.recoEngineAudioPosition);  
  
  // Insert additional event handler code here.  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioSignalProblem" />
        <altmember cref="T:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="AudioState">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.AudioState AudioState { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.Speech.Recognition.AudioState AudioState" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.AudioState" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioState As AudioState" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::AudioState AudioState { System::Speech::Recognition::AudioState get(); };" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.AudioState</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ottiene lo stato dell'audio ricevuto dal riconoscimento vocale.</summary>
        <value>Lo stato dell'input audio per il riconoscimento vocale.</value>
        <remarks>To be added.</remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.AudioStateChanged" />
      </Docs>
    </Member>
    <Member MemberName="AudioStateChanged">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; AudioStateChanged;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioStateChangedEventArgs&gt; AudioStateChanged" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.AudioStateChanged" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioStateChanged As EventHandler(Of AudioStateChangedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioStateChangedEventArgs ^&gt; ^ AudioStateChanged;" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Viene generato in seguito alla modifica dello stato nell'audio ricevuto dal riconoscimento.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Per ottenere lo stato audio al momento dell'evento, utilizzare il <xref:System.Speech.Recognition.AudioStateChangedEventArgs.AudioState%2A> proprietà dell'oggetto associato <xref:System.Speech.Recognition.AudioStateChangedEventArgs>. Per ottenere lo stato corrente di audio di input per il riconoscimento, usare il riconoscimento <xref:System.Speech.Recognition.SpeechRecognizer.AudioState%2A> proprietà. Per ulteriori informazioni sullo stato audio, vedere il <xref:System.Speech.Recognition.AudioState> enumerazione.  
  
 Quando si crea un delegato per un `AudioStateChanged` evento, si identifica il metodo che gestirà l'evento. Per associare l'evento al gestore eventi in uso, aggiungere all'evento un'istanza del delegato. Il gestore eventi viene chiamato ogni volta che si verifica l'evento, a meno che non venga rimosso il delegato. Per ulteriori informazioni sui delegati del gestore eventi, vedere [eventi e delegati](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Nell'esempio seguente viene utilizzato un gestore per il `AudioStateChanged` evento da scrivere il riconoscimento del nuovo <xref:System.Speech.Recognition.SpeechRecognizer.AudioState%2A> nella console ogni volta che le modifiche utilizzando un membro del <xref:System.Speech.Recognition.AudioState> enumerazione.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognizer recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize a shared speech recognition engine.  
      recognizer = new SpeechRecognizer();  
  
        // Create and load a grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(recognizer_AudioStateChanged);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.StateChanged +=  
          new EventHandler<StateChangedEventArgs>(recognizer_StateChanged);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
  
    // Handle the AudioStateChanged event.  
    static void recognizer_AudioStateChanged(object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("The new audio state is: " + e.AudioState);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine();  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
        Console.WriteLine();  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Done.");  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Put the recognizer into Listening mode.  
    static void recognizer_StateChanged(object sender, StateChangedEventArgs e)  
    {  
      if (e.RecognizerState != RecognizerState.Stopped)  
      {  
        Console.WriteLine();  
        recognizer.EmulateRecognizeAsync("Start listening");  
      }  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioState" />
        <altmember cref="T:System.Speech.Recognition.AudioStateChangedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioState" />
      </Docs>
    </Member>
    <MemberGroup MemberName="Dispose">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Elimina l'oggetto <see cref="T:System.Speech.Recognition.SpeechRecognizer" />.</summary>
      </Docs>
    </MemberGroup>
    <Member MemberName="Dispose">
      <MemberSignature Language="C#" Value="public void Dispose ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig newslot virtual instance void Dispose() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.Dispose" />
      <MemberSignature Language="VB.NET" Value="Public Sub Dispose ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; virtual void Dispose();" />
      <MemberType>Method</MemberType>
      <Implements>
        <InterfaceMember>M:System.IDisposable.Dispose</InterfaceMember>
      </Implements>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Elimina l'oggetto <see cref="T:System.Speech.Recognition.SpeechRecognizer" />.</summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <Member MemberName="Dispose">
      <MemberSignature Language="C#" Value="protected virtual void Dispose (bool disposing);" />
      <MemberSignature Language="ILAsm" Value=".method familyhidebysig newslot virtual instance void Dispose(bool disposing) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.Dispose(System.Boolean)" />
      <MemberSignature Language="VB.NET" Value="Protected Overridable Sub Dispose (disposing As Boolean)" />
      <MemberSignature Language="C++ CLI" Value="protected:&#xA; virtual void Dispose(bool disposing);" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="disposing" Type="System.Boolean" />
      </Parameters>
      <Docs>
        <param name="disposing">
          <see langword="true" /> per rilasciare sia le risorse gestite sia quelle non gestite; <see langword="false" /> per rilasciare solo le risorse non gestite.</param>
        <summary>Elimina l'oggetto <see cref="T:System.Speech.Recognition.SpeechRecognizer" /> e rilascia le risorse usate durante la sessione.</summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <MemberGroup MemberName="EmulateRecognize">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Emula l'input al riconoscimento vocale condiviso, utilizzando il testo anziché l'audio per il riconoscimento vocale sincrono.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Questi metodi richiedono l'input audio di sistema. Può essere utile quando si desidera testare o il debug di un'applicazione o grammatica.  
  
> [!NOTE]
>  Se il riconoscimento vocale Windows il **Sleeping** stato, quindi questi metodi restituiscono `null`.  
  
 Genera il riconoscimento condiviso il <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected>, e <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> eventi come se l'operazione di riconoscimento non è emulata. Il riconoscimento delle nuove righe e gli spazi vuoti aggiuntivi vengono ignorati e considera i segni di punteggiatura come valore letterale input.  
  
> [!NOTE]
>  Il <xref:System.Speech.Recognition.RecognitionResult> oggetto generato dal riconoscimento condiviso in risposta all'input emulata ha un valore di `null` per relativo <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> proprietà.  
  
 Per emulare un riconoscimento asincrono, utilizzare il <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> metodo.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(string inputText) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Function EmulateRecognize (inputText As String) As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(System::String ^ inputText);" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="inputText">Input per l'operazione di riconoscimento.</param>
        <summary>Emula l'input di una frase al riconoscimento vocale condiviso, utilizzando il testo anziché l'audio per il riconoscimento vocale sincrono.</summary>
        <returns>Il risultato dell'operazione di riconoscimento o <see langword="null" />, se l'operazione non riesce o il riconoscimento vocale di Windows è nello stato **Sospeso**.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il riconoscimento forniti con Vista e Windows 7 Ignora maiuscole / minuscole e caratteri di larghezza quando l'applicazione delle regole di sintassi per la frase di input. Per ulteriori informazioni su questo tipo di confronto, vedere il <xref:System.Globalization.CompareOptions> valori di enumerazione <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> e <xref:System.Globalization.CompareOptions.IgnoreWidth>. Il riconoscimento anche Ignora le nuove righe e lo spazio vuoto aggiuntivo e considera la punteggiatura come input letterale.  
  
   
  
## Examples  
 Nell'esempio seguente viene caricata una grammatica di esempio per il riconoscimento condiviso ed emula input per il riconoscimento. Se il riconoscimento vocale Windows non è in esecuzione, quindi avviare l'applicazione verrà avviata il riconoscimento vocale Windows. Se il riconoscimento vocale Windows il **Sleeping** stato, quindi <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A> restituisce sempre null.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SharedRecognizer  
{  
  class Program  
  {  
  
    static void Main(string[] args)  
    {  
      // Initialize an instance of the shared recognizer.  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
  
        recognizer.LoadGrammar(testGrammar);  
  
        RecognitionResult result;  
  
        // This EmulateRecognize call matches the grammar and returns a  
        // recognition result.  
        result = recognizer.EmulateRecognize("testing testing");  
        OutputResult(result);  
  
        // This EmulateRecognize call does not match the grammar and   
        // returns null.  
        result = recognizer.EmulateRecognize("testing one two three");  
        OutputResult(result);  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Output information about a recognition result to the console.  
    private static void OutputResult(RecognitionResult result)  
    {  
      if (result != null)  
      {  
        Console.WriteLine("Recognition result = {0}",  
          result.Text ?? "<no text>");  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(class System.Speech.Recognition.RecognizedWordUnit[] wordUnits, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(cli::array &lt;System::Speech::Recognition::RecognizedWordUnit ^&gt; ^ wordUnits, System::Globalization::CompareOptions compareOptions);" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="wordUnits" Type="System.Speech.Recognition.RecognizedWordUnit[]" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="wordUnits">Matrice di unità di parole che contiene l'input per l'operazione di riconoscimento.</param>
        <param name="compareOptions">Combinazione bit per bit dei valori di enumerazione che descrivono il tipo di confronto da utilizzare per l'operazione di riconoscimento emulato.</param>
        <summary>Emula l'input di parole specifiche al riconoscimento vocale condiviso, utilizzando il testo anziché l'audio per il riconoscimento vocale sincrono, e specifica come il riconoscimento gestisce il confronto Unicode tra le parole e le grammatiche di riconoscimento vocale caricate.</summary>
        <returns>Il risultato dell'operazione di riconoscimento o <see langword="null" />, se l'operazione non riesce o il riconoscimento vocale di Windows è nello stato **Sospeso**.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Questo metodo crea un <xref:System.Speech.Recognition.RecognitionResult> oggetto utilizzando le informazioni fornite nel `wordUnits` parametro.  
  
 Usa il riconoscimento di `compareOptions` quando si applica regole grammaticali per la frase di input. Il riconoscimento forniti con Vista e Windows 7 Ignora maiuscole / minuscole se il <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> o <xref:System.Globalization.CompareOptions.IgnoreCase> valore è presente. Il riconoscimento Ignora sempre la larghezza del carattere e mai Ignora Katakana / Hiragana. Il riconoscimento anche ignorare le nuove righe e lo spazio vuoto aggiuntivo e considera i segni di punteggiatura come input letterale. Per ulteriori informazioni sulla larghezza del carattere e il tipo Kana, vedere il <xref:System.Globalization.CompareOptions> enumerazione.  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(string inputText, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize(System.String,System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(System::String ^ inputText, System::Globalization::CompareOptions compareOptions);" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="inputText">Frase di Input per l'operazione di riconoscimento.</param>
        <param name="compareOptions">Combinazione bit per bit dei valori di enumerazione che descrivono il tipo di confronto da utilizzare per l'operazione di riconoscimento emulato.</param>
        <summary>Emula l'input di una frase al riconoscimento vocale condiviso, utilizzando il testo anziché l'audio per il riconoscimento vocale sincrono, e specifica come il riconoscimento gestisce il confronto Unicode tra la frase e le grammatiche di riconoscimento vocale caricate.</summary>
        <returns>Il risultato dell'operazione di riconoscimento o <see langword="null" />, se l'operazione non riesce o il riconoscimento vocale di Windows è nello stato **Sospeso**.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Usa il riconoscimento di `compareOptions` quando si applica regole grammaticali per la frase di input. Il riconoscimento forniti con Vista e Windows 7 Ignora maiuscole / minuscole se il <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> o <xref:System.Globalization.CompareOptions.IgnoreCase> valore è presente. Il riconoscimento Ignora sempre la larghezza del carattere e mai Ignora Katakana / Hiragana. Il riconoscimento anche ignorare le nuove righe e lo spazio vuoto aggiuntivo e considera i segni di punteggiatura come input letterale. Per ulteriori informazioni sulla larghezza del carattere e il tipo Kana, vedere il <xref:System.Globalization.CompareOptions> enumerazione.  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <MemberGroup MemberName="EmulateRecognizeAsync">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Emula l'input al riconoscimento vocale condiviso, utilizzando il testo anziché l'audio per il riconoscimento vocale asincrono.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Questi metodi richiedono l'input audio di sistema. Può essere utile quando si desidera testare o il debug di un'applicazione o grammatica.  
  
 Genera il riconoscimento condiviso il <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected>, e <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> eventi come se l'operazione di riconoscimento non è emulata. Al termine dell'operazione di riconoscimento asincrono il riconoscimento, genera il <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted> evento. Il riconoscimento delle nuove righe e gli spazi vuoti aggiuntivi vengono ignorati e considera i segni di punteggiatura come valore letterale input.  
  
> [!NOTE]
>  Se il riconoscimento vocale Windows è nel **Sleeping** stato, quindi il riconoscimento condiviso non elaborare l'input e non genera il <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected> e gli eventi correlati, ma ancora genera il <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted> evento.  
  
> [!NOTE]
>  Il <xref:System.Speech.Recognition.RecognitionResult> oggetto generato dal riconoscimento condiviso in risposta all'input emulata ha un valore di `null` per relativo <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> proprietà.  
  
 Per emulare un riconoscimento sincrono, utilizzare il <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A> metodo.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (string inputText);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(string inputText) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub EmulateRecognizeAsync (inputText As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(System::String ^ inputText);" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="inputText">Input per l'operazione di riconoscimento.</param>
        <summary>Emula l'input di una frase al riconoscimento vocale condiviso, utilizzando il testo anziché l'audio per il riconoscimento vocale asincrono.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il riconoscimento forniti con Vista e Windows 7 Ignora maiuscole / minuscole e caratteri di larghezza quando l'applicazione delle regole di sintassi per la frase di input. Per ulteriori informazioni su questo tipo di confronto, vedere il <xref:System.Globalization.CompareOptions> valori di enumerazione <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> e <xref:System.Globalization.CompareOptions.IgnoreWidth>. Il riconoscimento anche Ignora le nuove righe e lo spazio vuoto aggiuntivo e considera la punteggiatura come input letterale.  
  
   
  
## Examples  
 Nell'esempio seguente fa parte di un'applicazione console che viene caricata una grammatica di riconoscimento vocale e input emulata asincrona, i risultati di riconoscimento associati e gli eventi associati generati dal riconoscimento vocale. Se il riconoscimento vocale Windows non è in esecuzione, quindi avviare l'applicazione verrà avviata il riconoscimento vocale Windows. Se il riconoscimento vocale Windows il **Sleeping** stato, quindi <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> restituisce sempre null.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SharedRecognizer  
{  
  class Program  
  {  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Initialize an instance of the shared recognizer.  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
  
        recognizer.LoadGrammar(testGrammar);  
  
        // Attach event handlers for recognition events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call generates a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing testing");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call does not match the grammar   
        // or generate a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing one two three");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null)  
      {  
        Console.WriteLine("Recognition result = {0}",  
          e.Result.Text ?? "<no text>");  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }  
  
    // Handle the EmulateRecognizeCompleted event.   
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      if (e.Result == null)  
      {  
        Console.WriteLine("No result generated.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(class System.Speech.Recognition.RecognizedWordUnit[] wordUnits, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(cli::array &lt;System::Speech::Recognition::RecognizedWordUnit ^&gt; ^ wordUnits, System::Globalization::CompareOptions compareOptions);" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="wordUnits" Type="System.Speech.Recognition.RecognizedWordUnit[]" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="wordUnits">Matrice di unità di parole che contiene l'input per l'operazione di riconoscimento.</param>
        <param name="compareOptions">Combinazione bit per bit dei valori di enumerazione che descrivono il tipo di confronto da utilizzare per l'operazione di riconoscimento emulato.</param>
        <summary>Emula l'input di parole specifiche al riconoscimento vocale condiviso, utilizzando il testo anziché l'audio per il riconoscimento vocale asincrono, e specifica come il riconoscimento gestisce il confronto Unicode tra le parole e le grammatiche di riconoscimento vocale caricate.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Questo metodo crea un <xref:System.Speech.Recognition.RecognitionResult> oggetto utilizzando le informazioni fornite nel `wordUnits` parametro.  
  
 Usa il riconoscimento di `compareOptions` quando si applica regole grammaticali per la frase di input. Il riconoscimento forniti con Vista e Windows 7 Ignora maiuscole / minuscole se il <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> o <xref:System.Globalization.CompareOptions.IgnoreCase> valore è presente. Il riconoscimento Ignora sempre la larghezza del carattere e mai Ignora Katakana / Hiragana. Il riconoscimento anche ignorare le nuove righe e lo spazio vuoto aggiuntivo e considera i segni di punteggiatura come input letterale. Per ulteriori informazioni sulla larghezza del carattere e il tipo Kana, vedere il <xref:System.Globalization.CompareOptions> enumerazione.  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (string inputText, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(string inputText, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.String,System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(System::String ^ inputText, System::Globalization::CompareOptions compareOptions);" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="inputText">Frase di Input per l'operazione di riconoscimento.</param>
        <param name="compareOptions">Combinazione bit per bit dei valori di enumerazione che descrivono il tipo di confronto da utilizzare per l'operazione di riconoscimento emulato.</param>
        <summary>Emula l'input di una frase al riconoscimento vocale condiviso, utilizzando il testo anziché l'audio per il riconoscimento vocale asincrono, e specifica come il riconoscimento gestisce il confronto Unicode tra la frase e le grammatiche di riconoscimento vocale caricate.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Usa il riconoscimento di `compareOptions` quando si applica regole grammaticali per la frase di input. Il riconoscimento forniti con Vista e Windows 7 Ignora maiuscole / minuscole se il <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> o <xref:System.Globalization.CompareOptions.IgnoreCase> valore è presente. Il riconoscimento Ignora sempre la larghezza del carattere e mai Ignora Katakana / Hiragana. Il riconoscimento anche ignorare le nuove righe e lo spazio vuoto aggiuntivo e considera i segni di punteggiatura come input letterale. Per ulteriori informazioni sulla larghezza del carattere e il tipo Kana, vedere il <xref:System.Globalization.CompareOptions> enumerazione.  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; EmulateRecognizeCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; EmulateRecognizeCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event EmulateRecognizeCompleted As EventHandler(Of EmulateRecognizeCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::EmulateRecognizeCompletedEventArgs ^&gt; ^ EmulateRecognizeCompleted;" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Viene generato quando il riconoscimento condiviso completa un'operazione di riconoscimento asincrona per l'input emulato.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ogni <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> metodo inizia un'operazione asincrona di riconoscimento. Genera il riconoscimento di `EmulateRecognizeCompleted` eventi quando completa l'operazione asincrona.  
  
 L'operazione di riconoscimento asincrono può generare il <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected>, e <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> eventi. Il <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted> evento è l'ultimo evento di questo tipo che il riconoscimento genera per l'operazione specificata.  
  
 Quando si crea un delegato per un `EmulateRecognizeCompleted` evento, si identifica il metodo che gestirà l'evento. Per associare l'evento al gestore eventi in uso, aggiungere all'evento un'istanza del delegato. Il gestore eventi viene chiamato ogni volta che si verifica l'evento, a meno che non venga rimosso il delegato. Per ulteriori informazioni sui delegati del gestore eventi, vedere [eventi e delegati](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Nell'esempio seguente fa parte di un'applicazione console che viene caricata una grammatica di riconoscimento vocale e input emulata asincrona, i risultati di riconoscimento associati e gli eventi associati generati dal riconoscimento vocale. Se il riconoscimento vocale Windows non è in esecuzione, quindi avviare l'applicazione verrà avviata il riconoscimento vocale Windows. Se il riconoscimento vocale Windows il **Sleeping** modalità, quindi <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> restituisce sempre null.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SharedRecognizer  
{  
  class Program  
  {  
    // Indicate whether the asynchronous emulate recognition  
    // operation has completed.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
  
      // Initialize an instance of the shared recognizer.  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
        recognizer.LoadGrammar(testGrammar);  
  
        // Attach event handlers for recognition events.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=   
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call generates a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing testing");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call does not match the grammar  
        // or generate a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing one two three");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null)  
      {  
        Console.WriteLine("Recognition result = {0}",  
          e.Result.Text ?? "<no text>");  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }  
  
    // Handle the EmulateRecognizeCompleted event.  
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      if (e.Result == null)  
      {  
        Console.WriteLine("No result generated.");  
      }  
  
      // Indicate the asynchronous operation is complete.  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="Enabled">
      <MemberSignature Language="C#" Value="public bool Enabled { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance bool Enabled" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.Enabled" />
      <MemberSignature Language="VB.NET" Value="Public Property Enabled As Boolean" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property bool Enabled { bool get(); void set(bool value); };" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Boolean</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ottiene o imposta un valore che indica se l'oggetto <see cref="T:System.Speech.Recognition.SpeechRecognizer" /> è pronto per l'elaborazione vocale.</summary>
        <value>
          <see langword="true" /> se l'oggetto <see cref="T:System.Speech.Recognition.SpeechRecognizer" /> esegue il riconoscimento vocale; in caso contrario, <see langword="false" />.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Le modifiche a questa proprietà non influiscono le altre istanze del <xref:System.Speech.Recognition.SpeechRecognizer> classe.  
  
 Per impostazione predefinita, il valore di <xref:System.Speech.Recognition.SpeechRecognizer.Enabled%2A> proprietà `true` per un'istanza appena creata un'istanza di <xref:System.Speech.Recognition.SpeechRecognizer>. Anche se il riconoscimento è disabilitato, nessuna delle grammatiche di riconoscimento del riconoscimento vocale sono disponibile per le operazioni di riconoscimento. Impostazione del riconoscimento <xref:System.Speech.Recognition.SpeechRecognizer.Enabled%2A> proprietà ha effetto sul riconoscimento <xref:System.Speech.Recognition.SpeechRecognizer.State%2A> proprietà.  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition" />
      </Docs>
    </Member>
    <Member MemberName="Grammars">
      <MemberSignature Language="C#" Value="public System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt; Grammars { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Collections.ObjectModel.ReadOnlyCollection`1&lt;class System.Speech.Recognition.Grammar&gt; Grammars" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.Grammars" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property Grammars As ReadOnlyCollection(Of Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::Grammar ^&gt; ^ Grammars { System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::Grammar ^&gt; ^ get(); };" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ottiene una raccolta di oggetti <see cref="T:System.Speech.Recognition.Grammar" /> caricati in questa istanza di <see cref="T:System.Speech.Recognition.SpeechRecognizer" />.</summary>
        <value>Raccolta di oggetti <see cref="T:System.Speech.Recognition.Grammar" /> che l'applicazione ha caricato nell'istanza corrente del riconoscitore condiviso.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Questa proprietà non restituisce alcun riconoscimento vocale grammatiche riconoscimento caricate da un'altra applicazione.  
  
   
  
## Examples  
 Nell'esempio seguente genera informazioni nella console per ogni grammatica di riconoscimento vocale caricato il riconoscimento vocale condiviso.  
  
```csharp  
  
using System;  
using System.Collections.Generic;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SharedRecognizer  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
        Grammar sampleGrammar = new Grammar(new GrammarBuilder("sample phrase"));  
        sampleGrammar.Name = "Sample Grammar";  
        recognizer.LoadGrammar(sampleGrammar);  
  
        OutputGrammarList(recognizer);  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void OutputGrammarList(SpeechRecognizer recognizer)  
    {  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      if (grammars.Count > 0)  
      {  
        Console.WriteLine("Loaded grammars:");  
        foreach (Grammar g in grammars)  
        {  
          Console.WriteLine("  Grammar: {0}",  
            (g.Name != null) ? g.Name : "<no name>");  
        }  
      }  
      else  
      {  
        Console.WriteLine("No grammars loaded.");  
      }  
    }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammar">
      <MemberSignature Language="C#" Value="public void LoadGrammar (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void LoadGrammar(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammar(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void LoadGrammar(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">La grammatica di riconoscimento vocale da caricare.</param>
        <summary>Carica una grammatica di riconoscimento vocale.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il riconoscimento condiviso genera un'eccezione se la grammatica di riconoscimento vocale è già stata caricata, viene caricata in modo asincrono o non è riuscito a caricare qualsiasi tipo di riconoscimento. Se il riconoscimento è in esecuzione, le applicazioni devono utilizzare <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> per sospendere il motore di riconoscimento vocale prima di caricamento, scaricamento, abilitazione o disabilitazione di una grammatica.  
  
 Per caricare una grammatica di riconoscimento vocale in modo asincrono, utilizzare il <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync%2A> metodo.  
  
   
  
## Examples  
 Nell'esempio seguente fa parte di un'applicazione console che viene caricata una grammatica di riconoscimento vocale e input emulata asincrona, i risultati di riconoscimento associati e gli eventi associati generati dal riconoscimento vocale. Se il riconoscimento vocale Windows non è in esecuzione, quindi avviare l'applicazione verrà avviata il riconoscimento vocale Windows. Se il riconoscimento vocale Windows il **Sleeping** stato, quindi <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> restituisce sempre null.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SharedRecognizer  
{  
  class Program  
  {  
    // Indicate whether the asynchronous emulate recognition  
    // operation has completed.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Initialize an instance of the shared recognizer.  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
  
        recognizer.LoadGrammar(testGrammar);  
  
        // Attach event handlers for recognition events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call generates a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing testing");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call does not match the grammar   
        // or generate a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing one two three");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null)  
      {  
        Console.WriteLine("Recognition result = {0}",  
          e.Result.Text ?? "<no text>");  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }   
  
    // Handle the EmulateRecognizeCompleted event.   
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      if (e.Result == null)  
      {  
        Console.WriteLine("No result generated.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammarAsync">
      <MemberSignature Language="C#" Value="public void LoadGrammarAsync (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void LoadGrammarAsync(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void LoadGrammarAsync(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">La grammatica di riconoscimento vocale da caricare.</param>
        <summary>Carica in modo asincrono una grammatica di riconoscimento vocale.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Al termine dell'operazione asincrona il riconoscimento, genera un <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarCompleted> evento. Il riconoscimento genera un'eccezione se la grammatica di riconoscimento vocale è già stata caricata, viene caricata in modo asincrono o non è riuscito a caricare qualsiasi tipo di riconoscimento. Se il riconoscimento è in esecuzione, le applicazioni devono utilizzare <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> per sospendere il motore di riconoscimento vocale prima di caricamento, scaricamento, abilitazione o disabilitazione di una grammatica.  
  
 Per caricare una grammatica di riconoscimento vocale in modo sincrono, utilizzare il <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammar%2A> metodo.  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.LoadGrammarCompleted" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammarCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; LoadGrammarCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; LoadGrammarCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.LoadGrammarCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event LoadGrammarCompleted As EventHandler(Of LoadGrammarCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::LoadGrammarCompletedEventArgs ^&gt; ^ LoadGrammarCompleted;" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Viene generato quando il riconoscimento termina il caricamento asincrono di una grammatica del riconoscimento vocale.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il riconoscimento <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync%2A> metodo inizia un'operazione asincrona. Genera il riconoscimento di `LoadGrammarCompleted` evento dopo il completamento dell'operazione. Per ottenere il <xref:System.Speech.Recognition.Grammar> oggetto caricato il riconoscimento, usare il <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs.Grammar%2A> proprietà dell'oggetto associato <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs>. Per ottenere l'oggetto corrente <xref:System.Speech.Recognition.Grammar> oggetti ha caricato il riconoscimento, usare il riconoscimento <xref:System.Speech.Recognition.SpeechRecognizer.Grammars%2A> proprietà.  
  
 Quando si crea un delegato per un `LoadGrammarCompleted` evento, si identifica il metodo che gestirà l'evento. Per associare l'evento al gestore eventi in uso, aggiungere all'evento un'istanza del delegato. Il gestore eventi viene chiamato ogni volta che si verifica l'evento, a meno che non venga rimosso il delegato. Per ulteriori informazioni sui delegati del gestore eventi, vedere [eventi e delegati](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Nell'esempio seguente viene creato un riconoscimento vocale condiviso e quindi crea due tipi di grammatiche per il riconoscimento delle parole specifiche e per l'accettazione di dettatura disponibile. Nell'esempio viene caricato in modo asincrono tutte le grammatiche create per il riconoscimento. Gestori per il riconoscimento <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarCompleted> e <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> eventi scrive nella console il nome della grammatica che è stato utilizzato per eseguire il riconoscimento e il testo del risultato del riconoscimento, rispettivamente.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognizer recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize a shared speech recognition engine.  
      recognizer = new SpeechRecognizer();  
  
        // Add a handler for the LoadGrammarCompleted event.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Add a handler for the SpeechRecognized event.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Add a handler for the StateChanged event.  
        recognizer.StateChanged +=  
          new EventHandler<StateChangedEventArgs>(recognizer_StateChanged);  
  
        // Create "yesno" grammar.  
        Choices yesChoices = new Choices(new string[] { "yes", "yup", "yeah}" });  
        SemanticResultValue yesValue =  
            new SemanticResultValue(yesChoices, (bool)true);  
        Choices noChoices = new Choices(new string[] { "no", "nope", "neah" });  
        SemanticResultValue noValue =  
            new SemanticResultValue(noChoices, (bool)false);  
        SemanticResultKey yesNoKey =  
            new SemanticResultKey("yesno", new Choices(new GrammarBuilder[] { yesValue, noValue }));  
        Grammar yesnoGrammar = new Grammar(yesNoKey);  
        yesnoGrammar.Name = "yesNo";  
  
        // Create "done" grammar.  
        Grammar doneGrammar =  
          new Grammar(new Choices(new string[] { "done", "exit", "quit", "stop" }));  
        doneGrammar.Name = "Done";  
  
        // Create dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation";  
  
        // Load grammars to the recognizer.  
        recognizer.LoadGrammarAsync(yesnoGrammar);  
        recognizer.LoadGrammarAsync(doneGrammar);  
        recognizer.LoadGrammarAsync(dictation);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Grammar({0}): {1}", e.Result.Grammar.Name, e.Result.Text);  
  
      // Add event handler code here.  
    }  
  
    // Handle the LoadGrammarCompleted event.   
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      string grammarName = e.Grammar.Name;  
      bool grammarLoaded = e.Grammar.Loaded;  
  
      if (e.Error != null)  
      {  
        Console.WriteLine("LoadGrammar for {0} failed with a {1}.",  
        grammarName, e.Error.GetType().Name);  
  
        // Add exception handling code here.  
      }  
  
      Console.WriteLine("Grammar {0} {1} loaded.",  
      grammarName, (grammarLoaded) ? "is" : "is not");  
    }  
  
    // Put the shared speech recognizer into "listening" mode.   
    static void recognizer_StateChanged(object sender, StateChangedEventArgs e)  
    {  
      if (e.RecognizerState != RecognizerState.Stopped)  
      {  
        recognizer.EmulateRecognizeAsync("Start listening");  
      }  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.LoadGrammarCompletedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.Grammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="MaxAlternates">
      <MemberSignature Language="C#" Value="public int MaxAlternates { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance int32 MaxAlternates" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.MaxAlternates" />
      <MemberSignature Language="VB.NET" Value="Public Property MaxAlternates As Integer" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property int MaxAlternates { int get(); void set(int value); };" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Int32</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ottiene o imposta il numero massimo di risultati del riconoscimento alternativi che il riconoscimento condiviso restituisce per ogni operazione di riconoscimento.</summary>
        <value>Il numero massimo di risultati alternativi che il riconoscimento vocale restituisce per ogni operazione di riconoscimento.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> proprietà del <xref:System.Speech.Recognition.RecognitionResult> classe contiene la raccolta di <xref:System.Speech.Recognition.RecognizedPhrase> gli oggetti che rappresentano altri interpretazioni candidato dell'input.  
  
 Il valore predefinito per <xref:System.Speech.Recognition.SpeechRecognizer.MaxAlternates%2A> è 10.  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.RecognitionResult.Alternates" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="PauseRecognizerOnRecognition">
      <MemberSignature Language="C#" Value="public bool PauseRecognizerOnRecognition { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance bool PauseRecognizerOnRecognition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition" />
      <MemberSignature Language="VB.NET" Value="Public Property PauseRecognizerOnRecognition As Boolean" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property bool PauseRecognizerOnRecognition { bool get(); void set(bool value); };" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Boolean</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ottiene o imposta un valore che indica se il riconoscimento condiviso sospende le operazioni di riconoscimento quando un'applicazione gestisce un evento <see cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />.</summary>
        <value>
          <see langword="true" /> se il riconoscimento condiviso attende l'input del processo mentre un'applicazione gestisce l'evento <see cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" /> ; in caso contrario, <see langword="false" />.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Impostare questa proprietà su `true`, se all'interno di <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> gestore eventi dell'applicazione è necessario modificare lo stato del servizio di riconoscimento vocale o modificare le grammatiche di riconoscimento vocale caricato o attivato prima che il servizio di riconoscimento vocale processi più input.  
  
> [!NOTE]
>  L'impostazione di <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> proprietà `true` , ognuna <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> gestore dell'evento in ogni applicazione di bloccare il servizio di riconoscimento vocale Windows.  
  
 Per sincronizzare le modifiche per il riconoscimento condiviso con lo stato dell'applicazione, utilizzare il <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> metodo.  
  
 Quando <xref:System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition%2A> è `true`, durante l'esecuzione del <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> gestore del servizio di riconoscimento vocale sospende e memorizza nel buffer di input audio di nuovo quando arriva. Una volta il <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> il gestore eventi, il riconoscimento servizio riprende il riconoscimento e inizia l'elaborazione delle informazioni dal buffer di input.  
  
 Per abilitare o disabilitare il servizio di riconoscimento vocale, utilizzare il <xref:System.Speech.Recognition.SpeechRecognizer.Enabled%2A> proprietà.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.Enabled" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerAudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan RecognizerAudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan RecognizerAudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property RecognizerAudioPosition As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan RecognizerAudioPosition { TimeSpan get(); };" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ottiene la posizione corrente del riconoscimento nell'input audio in fase di elaborazione.</summary>
        <value>La posizione del riconoscimento nell'input audio in fase di elaborazione.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il `RecognizerAudioPosition` proprietà fa riferimento la posizione del riconoscimento l'elaborazione del proprio input audio. Al contrario, il <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> proprietà fa riferimento la posizione del dispositivo di input nel proprio flusso audio generato. Queste posizioni possono essere diverse. Ad esempio, se ha ricevuto il riconoscimento di input per i quali non ha ancora generato un risultato di riconoscimento, il valore del <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> proprietà è minore del valore del <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> proprietà.  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerInfo">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognizerInfo RecognizerInfo { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.Recognition.RecognizerInfo RecognizerInfo" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.RecognizerInfo" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property RecognizerInfo As RecognizerInfo" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::RecognizerInfo ^ RecognizerInfo { System::Speech::Recognition::RecognizerInfo ^ get(); };" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognizerInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ottiene informazioni sul riconoscimento vocale condiviso.</summary>
        <value>Informazioni sul riconoscimento vocale condiviso.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Questa proprietà restituisce informazioni su riconoscimento vocale in uso dal riconoscimento vocale Windows.  
  
   
  
## Examples  
 Nell'esempio seguente invia informazioni il riconoscimento condiviso nella console.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SharedRecognizer  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
        Console.WriteLine("Recognizer information for the shared recognizer:");  
        Console.WriteLine("  Name: {0}", recognizer.RecognizerInfo.Name);  
        Console.WriteLine("  Culture: {0}", recognizer.RecognizerInfo.Culture.ToString());  
        Console.WriteLine("  Description: {0}", recognizer.RecognizerInfo.Description);  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognizerInfo" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.State" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerUpdateReached">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; RecognizerUpdateReached;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; RecognizerUpdateReached" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached" />
      <MemberSignature Language="VB.NET" Value="Public Event RecognizerUpdateReached As EventHandler(Of RecognizerUpdateReachedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::RecognizerUpdateReachedEventArgs ^&gt; ^ RecognizerUpdateReached;" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Viene generato quando il riconoscimento viene sospeso per sincronizzare il riconoscimento e altre operazioni.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Le applicazioni devono utilizzare <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> sospendere un'istanza in esecuzione di <xref:System.Speech.Recognition.SpeechRecognizer> prima di modificare il relativo <xref:System.Speech.Recognition.Grammar> oggetti. Ad esempio, mentre il <xref:System.Speech.Recognition.SpeechRecognizer> è sospesa, è possibile caricare, scaricare, abilitare e disabilitare <xref:System.Speech.Recognition.Grammar> oggetti. Il <xref:System.Speech.Recognition.SpeechRecognizer> genera questo evento quando è pronto per accettare le modifiche.  
  
 Quando si crea un delegato per un <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> evento, si identifica il metodo che gestirà l'evento. Per associare l'evento al gestore eventi in uso, aggiungere all'evento un'istanza del delegato. Il gestore eventi viene chiamato ogni volta che si verifica l'evento, a meno che non venga rimosso il delegato. Per ulteriori informazioni sui delegati del gestore eventi, vedere [eventi e delegati](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 L'esempio seguente illustra un'applicazione console che carica e Scarica <xref:System.Speech.Recognition.Grammar> oggetti. L'applicazione utilizza il <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> metodo per richiedere il motore di riconoscimento vocale per mettere in pausa per la ricezione di un aggiornamento. L'applicazione quindi carica o scarica un <xref:System.Speech.Recognition.Grammar> oggetto.  
  
 A ogni aggiornamento, un gestore per <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> evento scrive il nome e lo stato di attualmente caricato <xref:System.Speech.Recognition.Grammar> oggetti nella console. Come le grammatiche vengono caricate e scaricate, l'applicazione vengono innanzitutto i nomi di animali, i nomi di animali e i nomi di frutti e quindi solo i nomi di frutta.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
using System.Collections.Generic;  
using System.Threading;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognizer recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize a shared speech recognition engine.  
      recognizer = new SpeechRecognizer();  
  
      // Create the first grammar - Farm.  
      Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
      GrammarBuilder farm = new GrammarBuilder(animals);  
      Grammar farmAnimals = new Grammar(farm);  
      farmAnimals.Name = "Farm";  
  
      // Create the second grammar - Fruit.  
      Choices fruit = new Choices(new string[] { "apples", "peaches", "oranges" });  
      GrammarBuilder favorite = new GrammarBuilder(fruit);  
      Grammar favoriteFruit = new Grammar(favorite);  
      favoriteFruit.Name = "Fruit";  
  
      // Attach event handlers.  
      recognizer.SpeechRecognized +=  
        new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
      recognizer.RecognizerUpdateReached +=  
        new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  
      recognizer.StateChanged +=   
        new EventHandler<StateChangedEventArgs>(recognizer_StateChanged);  
  
      // Load the Farm grammar.  
      recognizer.LoadGrammar(farmAnimals);  
      Console.WriteLine("Grammar Farm is loaded");  
  
      // Pause to recognize farm animals.  
      Thread.Sleep(7000);  
      Console.WriteLine();  
  
      // Request an update and load the Fruit grammar.  
      recognizer.RequestRecognizerUpdate();  
      recognizer.LoadGrammarAsync(favoriteFruit);  
      Thread.Sleep(5000);  
  
      // Request an update and unload the Farm grammar.  
      recognizer.RequestRecognizerUpdate();  
      recognizer.UnloadGrammar(farmAnimals);  
      Thread.Sleep(5000);  
  
      // Keep the console window open.  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Put the shared speech recognizer into "listening" mode.  
    static void recognizer_StateChanged(object sender, StateChangedEventArgs e)  
    {  
      if (e.RecognizerState != RecognizerState.Stopped)  
      {  
        recognizer.EmulateRecognizeAsync("Start listening");  
      }  
    }  
  
    // At the update, get the names and enabled status of the currently loaded grammars.  
    public static void recognizer_RecognizerUpdateReached(  
      object sender, RecognizerUpdateReachedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Update reached:");  
      Thread.Sleep(1000);  
  
      string qualifier;  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      foreach (Grammar g in grammars)  
      {  
        qualifier = (g.Enabled) ? "enabled" : "disabled";  
        Console.WriteLine("  Grammar {0} is loaded and is {1}.",  
        g.Name, qualifier);  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("  Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate" />
        <altmember cref="T:System.Speech.Recognition.RecognizerUpdateReachedEventArgs" />
      </Docs>
    </Member>
    <MemberGroup MemberName="RequestRecognizerUpdate">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Richiede la sospensione del riconoscimento condiviso e l'aggiornamento dello stato.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Utilizzare questo metodo per sincronizzare le modifiche per il riconoscimento condiviso. Ad esempio, se si carica o scarica una grammatica di riconoscimento vocale mentre il riconoscimento per elaborare l'input, utilizzare questo metodo e <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> eventi per la sincronizzazione del comportamento dell'applicazione con lo stato del sistema di riconoscimento.  
  
 Quando questo metodo viene chiamato, il riconoscimento viene sospeso o completamento di operazioni asincrone e genera un <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> evento. Oggetto <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> gestore eventi può quindi modificare lo stato del sistema di riconoscimento tra operazioni di riconoscimento.  
  
 Quando viene chiamato questo metodo:  
  
-   Se il riconoscimento non è l'elaborazione di input, viene generato immediatamente il riconoscimento di <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> evento.  
  
-   Se il riconoscimento è l'elaborazione di input che consiste di inattività o rumore di fondo, il riconoscimento sospende l'operazione di riconoscimento e genera il <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> evento.  
  
-   Se il riconoscimento è l'elaborazione di input che non è costituito inattività o rumore di fondo, il riconoscimento completa l'operazione di riconoscimento e quindi genera il <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> evento.  
  
 Mentre il riconoscimento sta gestendo il <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> evento:  
  
-   Il riconoscimento non elabora l'input e il valore della <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> proprietà rimane invariato.  
  
-   Il riconoscimento continua a raccogliere input e il valore di <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> proprietà può essere modificata.  
  
 Per modificare la modalità di operazioni di riconoscimento viene sospeso il riconoscimento condiviso, mentre un'applicazione gestisce un <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> evento, utilizzare il <xref:System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition%2A> proprietà.  
  
   
  
## Examples  
 L'esempio seguente illustra un'applicazione console che carica e Scarica <xref:System.Speech.Recognition.Grammar> oggetti. L'applicazione utilizza il <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> metodo per richiedere il motore di riconoscimento vocale per mettere in pausa per la ricezione di un aggiornamento. L'applicazione quindi carica o scarica un <xref:System.Speech.Recognition.Grammar> oggetto.  
  
 A ogni aggiornamento, un gestore per <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> evento scrive il nome e lo stato di attualmente caricato <xref:System.Speech.Recognition.Grammar> oggetti nella console. Come le grammatiche vengono caricate e scaricate, l'applicazione vengono innanzitutto i nomi di animali, i nomi di animali e i nomi di frutti e quindi solo i nomi di frutta.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
using System.Collections.Generic;  
using System.Threading;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognizer recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and configure its input.  
      recognizer = new SpeechRecognizer();  
  
      // Create the first grammar - Farm.  
      Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
      GrammarBuilder farm = new GrammarBuilder(animals);  
      Grammar farmAnimals = new Grammar(farm);  
      farmAnimals.Name = "Farm";  
  
      // Create the second grammar - Fruit.  
      Choices fruit = new Choices(new string[] { "apples", "peaches", "oranges" });  
      GrammarBuilder favorite = new GrammarBuilder(fruit);  
      Grammar favoriteFruit = new Grammar(favorite);  
      favoriteFruit.Name = "Fruit";  
  
      // Attach event handlers.  
      recognizer.SpeechRecognized +=  
        new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
      recognizer.RecognizerUpdateReached +=  
        new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  
  
      // Check to see if recognizer is loaded, wait if it is not loaded.  
      if (recognizer.State != RecognizerState.Listening)  
      {  
        Thread.Sleep(5000);  
  
        // Put recognizer in listening state.  
        recognizer.EmulateRecognizeAsync("Start listening");  
      }  
  
      // Load the Farm grammar.  
      recognizer.LoadGrammar(farmAnimals);  
      Console.WriteLine("Grammar Farm is loaded");  
  
      // Pause to recognize farm animals.  
      Thread.Sleep(7000);  
      Console.WriteLine();  
  
      // Request an update and load the Fruit grammar.  
      recognizer.RequestRecognizerUpdate();  
      recognizer.LoadGrammarAsync(favoriteFruit);  
      Thread.Sleep(5000);  
  
      // Request an update and unload the Farm grammar.  
      recognizer.RequestRecognizerUpdate();  
      recognizer.UnloadGrammar(farmAnimals);  
      Thread.Sleep(5000);  
  
      // Keep the console window open.  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    public static void recognizer_RecognizerUpdateReached(object sender, RecognizerUpdateReachedEventArgs e)  
    {  
      // At the update, get the names and enabled status of the currently loaded grammars.  
      Console.WriteLine();  
      Console.WriteLine("Update reached:");  
      Thread.Sleep(1000);  
      string qualifier;  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      foreach (Grammar g in grammars)  
      {  
        qualifier = (g.Enabled) ? "enabled" : "disabled";  
        Console.WriteLine("  Grammar {0} is loaded and is {1}.",  
        g.Name, qualifier);  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("  Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached" />
      </Docs>
    </MemberGroup>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate();" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Richiede la sospensione del riconoscimento condiviso e l'aggiornamento dello stato.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Quando viene generato il riconoscimento di <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> evento, il <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> proprietà del <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> è `null`.  
  
 Per fornire un token dell'utente, utilizzare il <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> o <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> metodo. Per specificare un offset di posizione audio, usare il <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> metodo.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate (object userToken);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate(object userToken) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate(System.Object)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate (userToken As Object)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate(System::Object ^ userToken);" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="userToken" Type="System.Object" />
      </Parameters>
      <Docs>
        <param name="userToken">Informazioni definite dall'utente che contengono informazioni per l'operazione.</param>
        <summary>Richiede la sospensione del riconoscimento condiviso, l'aggiornamento dello stato e la fornitura di un token utente per l'evento associato.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Quando viene generato il riconoscimento di <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> evento, il <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> proprietà del <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> contiene il valore del `userToken` parametro.  
  
 Per specificare un offset di posizione audio, usare il <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> metodo.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate (object userToken, TimeSpan audioPositionAheadToRaiseUpdate);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate(object userToken, valuetype System.TimeSpan audioPositionAheadToRaiseUpdate) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate(System.Object,System.TimeSpan)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate (userToken As Object, audioPositionAheadToRaiseUpdate As TimeSpan)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate(System::Object ^ userToken, TimeSpan audioPositionAheadToRaiseUpdate);" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="userToken" Type="System.Object" />
        <Parameter Name="audioPositionAheadToRaiseUpdate" Type="System.TimeSpan" />
      </Parameters>
      <Docs>
        <param name="userToken">Informazioni definite dall'utente che contengono informazioni per l'operazione.</param>
        <param name="audioPositionAheadToRaiseUpdate">L'offset dall'oggetto <see cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" /> corrente per ritardare la richiesta.</param>
        <summary>Richiede la sospensione del riconoscimento condiviso, l'aggiornamento dello stato e la fornitura di un offset e un token utente per l'evento associato.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il riconoscimento non avvierà la richiesta di aggiornamento per il riconoscimento fino a quando il riconoscimento <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> corrente è uguale a <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> sommato al valore del `audioPositionAheadToRaiseUpdate` parametro.  
  
 Quando viene generato il riconoscimento di <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> evento, il <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> proprietà del <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> contiene il valore del `userToken` parametro.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="SpeechDetected">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; SpeechDetected;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechDetectedEventArgs&gt; SpeechDetected" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechDetected As EventHandler(Of SpeechDetectedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechDetectedEventArgs ^&gt; ^ SpeechDetected;" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Viene generato quando il riconoscimento rileva un input identificabile come funzione vocale.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il riconoscimento condiviso può generare questo evento in risposta all'input. Il <xref:System.Speech.Recognition.SpeechDetectedEventArgs.AudioPosition%2A> proprietà dell'oggetto associato <xref:System.Speech.Recognition.SpeechDetectedEventArgs> oggetto indica una posizione nel flusso di input in cui il riconoscimento per rilevata il riconoscimento vocale. Per ulteriori informazioni, vedere il <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> e <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> proprietà e <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A> e <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> metodi.  
  
 Quando si crea un delegato per un <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected> evento, si identifica il metodo che gestirà l'evento. Per associare l'evento al gestore eventi in uso, aggiungere all'evento un'istanza del delegato. Il gestore eventi viene chiamato ogni volta che si verifica l'evento, a meno che non venga rimosso il delegato. Per ulteriori informazioni sui delegati del gestore eventi, vedere [eventi e delegati](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Nell'esempio seguente fa parte di un'applicazione console per la scelta di origine e destinazione città per un volo. L'applicazione riconosce frasi, ad esempio "Desidero entrata da Miami a Chicago."  Nell'esempio viene utilizzato il <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected> evento report il <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> viene rilevato il parlato ogni ora.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize a shared speech recognition engine.  
    {  
      using (SpeechRecognizer recognizer =  
         new SpeechRecognizer())  
      {  
  
        // Create a grammar.  
        Choices cities = new Choices(new string[] {   
          "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I would like to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Create a Grammar object and load it to the recognizer.  
        Grammar g = new Grammar(gb);  
        g.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(g);  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechDetected +=   
          new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechDetected event.  
    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine("Speech detected at AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="SpeechHypothesized">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; SpeechHypothesized;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; SpeechHypothesized" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event SpeechHypothesized As EventHandler(Of SpeechHypothesizedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechHypothesizedEventArgs ^&gt; ^ SpeechHypothesized;" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Viene generato quando il riconoscimento ha riconosciuto le parole che possono essere componenti di più frasi complete in una grammatica.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il riconoscimento condiviso può generare questo evento quando l'input è ambiguo. Ad esempio, per una grammatica di riconoscimento vocale che supporta il riconoscimento di uno "nuovo gioco." o "nuova partita", "nuovo gioco." è un input non ambiguo, e "nuova partita" è un input ambiguo.  
  
 Quando si crea un delegato per un <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized> evento, si identifica il metodo che gestirà l'evento. Per associare l'evento al gestore eventi in uso, aggiungere all'evento un'istanza del delegato. Il gestore eventi viene chiamato ogni volta che si verifica l'evento, a meno che non venga rimosso il delegato. Per ulteriori informazioni sui delegati del gestore eventi, vedere [eventi e delegati](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Nell'esempio seguente riconosce frasi, ad esempio "Visualizza l'elenco degli artisti nella categoria jazz". Nell'esempio viene utilizzato il <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized> evento per visualizzare i frammenti di frase incompleto nella console di come vengono riconosciuti.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize a shared speech recognition engine.  
    {  
      using (SpeechRecognizer recognizer =  
         new SpeechRecognizer())  
      {  
  
        // Create a grammar.  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display the list of");  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the");  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category.");  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechHypothesized +=   
          new EventHandler<SpeechHypothesizedEventArgs>(recognizer_SpeechHypothesized);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void recognizer_SpeechHypothesized(object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine("Speech hypothesized: " + e.Result.Text);  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechHypothesizedEventArgs" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="SpeechRecognitionRejected">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; SpeechRecognitionRejected;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; SpeechRecognitionRejected" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechRecognitionRejected As EventHandler(Of SpeechRecognitionRejectedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechRecognitionRejectedEventArgs ^&gt; ^ SpeechRecognitionRejected;" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Viene generato quando il riconoscimento riceve un input che non corrisponde ad alcuna grammatica di riconoscimento vocale caricata.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Se determina che input non corrisponde con sufficiente sicurezza qualsiasi le grammatiche di riconoscimento vocale caricato, il riconoscimento condiviso genera questo evento. Il <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> proprietà del <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> contiene il rifiutati <xref:System.Speech.Recognition.RecognitionResult> oggetto.  
  
 Le soglie di probabilità per il riconoscimento condiviso, gestito da <xref:System.Speech.Recognition.SpeechRecognizer>, sono associati a un profilo utente e archiviati nel Registro di sistema Windows. Le applicazioni non scrivere le modifiche al Registro di sistema per le proprietà di riconoscimento condiviso.  
  
 Quando si crea un delegato per un <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected> evento, si identifica il metodo che gestirà l'evento. Per associare l'evento al gestore eventi in uso, aggiungere all'evento un'istanza del delegato. Il gestore eventi viene chiamato ogni volta che si verifica l'evento, a meno che non venga rimosso il delegato. Per ulteriori informazioni sui delegati del gestore eventi, vedere [eventi e delegati](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Nell'esempio seguente riconosce frasi, ad esempio "Visualizzare l'elenco degli artisti nella categoria jazz" o "Gospel album visualizzare". Nell'esempio viene utilizzato un gestore per il <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected> evento per visualizzare una notifica nella console quando il riconoscimento vocale per l'input non è possibile associare al contenuto della grammatica confidenza sufficiente per generare un riconoscimento ha esito positivo.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize a shared speech recognition engine.  
    {  
      using (SpeechRecognizer recognizer =  
         new SpeechRecognizer())  
      {  
  
        // Create a grammar.  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display");  
        mediaMenu.Append("the list of", 0, 1);  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the", 0, 1);  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category", 0, 1);  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechRecognitionRejected +=   
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("Speech input was rejected.");  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="SpeechRecognized">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; SpeechRecognized;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechRecognizedEventArgs&gt; SpeechRecognized" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechRecognized As EventHandler(Of SpeechRecognizedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechRecognizedEventArgs ^&gt; ^ SpeechRecognized;" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Viene generato quando il riconoscimento riceve un input che corrisponde a una delle relative grammatiche di riconoscimento vocale.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Genera il riconoscimento di `SpeechRecognized` eventi se si determina con sufficiente sicurezza che input corrisponda a uno le grammatiche di riconoscimento vocale caricati e abilitati. Il <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> proprietà del <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> contiene accettata <xref:System.Speech.Recognition.RecognitionResult> oggetto.  
  
 Le soglie di probabilità per il riconoscimento condiviso, gestito da <xref:System.Speech.Recognition.SpeechRecognizer>, sono associati a un profilo utente e archiviati nel Registro di sistema Windows. Le applicazioni non scrivere le modifiche al Registro di sistema per le proprietà di riconoscimento condiviso.  
  
 Quando il riconoscimento riceve l'input che corrisponde a una grammatica, il <xref:System.Speech.Recognition.Grammar> oggetto può generare il <xref:System.Speech.Recognition.Grammar.SpeechRecognized> evento. Il <xref:System.Speech.Recognition.Grammar> dell'oggetto <xref:System.Speech.Recognition.Grammar.SpeechRecognized> evento viene generato prima del riconoscimento vocale <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> evento.  
  
 Quando si crea un delegato per un <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> evento, si identifica il metodo che gestirà l'evento. Per associare l'evento al gestore eventi in uso, aggiungere all'evento un'istanza del delegato. Il gestore eventi viene chiamato ogni volta che si verifica l'evento, a meno che non venga rimosso il delegato. Per ulteriori informazioni sui delegati del gestore eventi, vedere [eventi e delegati](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Nell'esempio seguente fa parte di un'applicazione console che carica una grammatica di riconoscimento vocale e viene illustrato l'input vocale per il riconoscimento condiviso, i risultati di riconoscimento associati e gli eventi associati generati dal riconoscimento vocale. Se il riconoscimento vocale Windows non è in esecuzione, quindi avviare l'applicazione verrà avviata il riconoscimento vocale Windows.  
  
 Leggere l'input, ad esempio "Desidero entrata da Chicago a Miami" attiverà una <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> evento. Parlando la frase "Entrata me da Houston a Chicago" non attiverà una <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> evento.  
  
 Nell'esempio viene utilizzato un gestore per il <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> evento per visualizzare correttamente riconosciuto frasi e la semantica contengono nella console.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize a shared speech recognition engine.  
    {  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
  
        // Create SemanticResultValue objects that contain cities and airport codes.  
        SemanticResultValue chicago = new SemanticResultValue("Chicago", "ORD");  
        SemanticResultValue boston = new SemanticResultValue("Boston", "BOS");  
        SemanticResultValue miami = new SemanticResultValue("Miami", "MIA");  
        SemanticResultValue dallas = new SemanticResultValue("Dallas", "DFW");  
  
        // Create a Choices object and add the SemanticResultValue objects, using  
        // implicit conversion from SemanticResultValue to GrammarBuilder  
        Choices cities = new Choices();  
        cities.Add(new Choices(new GrammarBuilder[] { chicago, boston, miami, dallas }));  
  
        // Build the phrase and add SemanticResultKeys.  
        GrammarBuilder chooseCities = new GrammarBuilder();  
        chooseCities.Append("I want to fly from");  
        chooseCities.Append(new SemanticResultKey("origin", cities));  
        chooseCities.Append("to");  
        chooseCities.Append(new SemanticResultKey("destination", cities));  
  
        // Build a Grammar object from the GrammarBuilder.  
        Grammar bookFlight = new Grammar(chooseCities);  
        bookFlight.Name = "Book Flight";  
  
        // Add a handler for the LoadGrammarCompleted event.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Add a handler for the SpeechRecognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(bookFlight);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
      Console.WriteLine();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized:  " + e.Result.Text);  
      Console.WriteLine();  
      Console.WriteLine("Semantic results:");  
      Console.WriteLine("  The flight origin is " + e.Result.Semantics["origin"].Value);  
      Console.WriteLine("  The flight destination is " + e.Result.Semantics["destination"].Value);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="T:System.Speech.Recognition.SpeechRecognizedEventArgs" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
      </Docs>
    </Member>
    <Member MemberName="State">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognizerState State { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.Speech.Recognition.RecognizerState State" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.State" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property State As RecognizerState" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::RecognizerState State { System::Speech::Recognition::RecognizerState get(); };" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognizerState</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ottiene lo stato di un oggetto <see cref="T:System.Speech.Recognition.SpeechRecognizer" />.</summary>
        <value>Stato dell'oggetto <see langword="SpeechRecognizer" />.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Questa proprietà di sola lettura indica se il riconoscimento condiviso residente in Windows è nel `Stopped` o `Listening` stato. Per altre informazioni, vedere l'enumerazione <xref:System.Speech.Recognition.RecognizerState>.  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognizerState" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.StateChanged" />
      </Docs>
    </Member>
    <Member MemberName="StateChanged">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.StateChangedEventArgs&gt; StateChanged;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.StateChangedEventArgs&gt; StateChanged" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.StateChanged" />
      <MemberSignature Language="VB.NET" Value="Public Event StateChanged As EventHandler(Of StateChangedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::StateChangedEventArgs ^&gt; ^ StateChanged;" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.StateChangedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Viene generato quando lo stato di esecuzione del motore di riconoscimento della tecnologia Windows Desktop Speech viene modificato.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Il riconoscimento condiviso genera questo evento quando cambia lo stato di riconoscimento vocale Windows per il <xref:System.Speech.Recognition.RecognizerState.Listening> o <xref:System.Speech.Recognition.RecognizerState.Stopped> stato.  
  
 Per ottenere lo stato di sistema condiviso di riconoscimento al momento dell'evento, utilizzare il <xref:System.Speech.Recognition.StateChangedEventArgs.RecognizerState%2A> proprietà dell'oggetto associato <xref:System.Speech.Recognition.StateChangedEventArgs>. Per ottenere lo stato corrente del riconoscimento condiviso, usare il riconoscimento <xref:System.Speech.Recognition.SpeechRecognizer.State%2A> proprietà.  
  
 Quando si crea un delegato per un <xref:System.Speech.Recognition.SpeechRecognizer.StateChanged> evento, si identifica il metodo che gestirà l'evento. Per associare l'evento al gestore eventi in uso, aggiungere all'evento un'istanza del delegato. Il gestore eventi viene chiamato ogni volta che si verifica l'evento, a meno che non venga rimosso il delegato. Per ulteriori informazioni sui delegati del gestore eventi, vedere [eventi e delegati](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Nell'esempio seguente viene creato un riconoscimento vocale condiviso e quindi crea due tipi di grammatiche per il riconoscimento delle parole specifiche e per l'accettazione di dettatura disponibile. Nell'esempio viene caricato in modo asincrono tutte le grammatiche create per il riconoscimento.  Un gestore per il <xref:System.Speech.Recognition.SpeechRecognizer.StateChanged> evento utilizza il <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> metodo per inserire il riconoscimento di Windows in modalità "ascolto".  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognizer recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize a shared speech recognition engine.  
      recognizer = new SpeechRecognizer();  
  
      // Add a handler for the LoadGrammarCompleted event.  
      recognizer.LoadGrammarCompleted += new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
      // Add a handler for the SpeechRecognized event.  
      recognizer.SpeechRecognized += new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
      // Add a handler for the StateChanged event.  
      recognizer.StateChanged += new EventHandler<StateChangedEventArgs>(recognizer_StateChanged);  
  
      // Create "yesno" grammar.  
      Choices yesChoices = new Choices(new string[] { "yes", "yup", "yah}" });  
      SemanticResultValue yesValue =  
          new SemanticResultValue(yesChoices, (bool)true);  
      Choices noChoices = new Choices(new string[] { "no", "nope", "nah" });  
      SemanticResultValue noValue = new SemanticResultValue(noChoices, (bool)false);  
      SemanticResultKey yesNoKey =  
          new SemanticResultKey("yesno", new Choices(new GrammarBuilder[] { yesValue, noValue }));  
      Grammar yesnoGrammar = new Grammar(yesNoKey);  
      yesnoGrammar.Name = "yesNo";  
  
      // Create "done" grammar.  
      Grammar doneGrammar =  
        new Grammar(new Choices(new string[] { "done", "exit", "quit", "stop" }));  
      doneGrammar.Name = "Done";  
  
      // Create dictation grammar.  
      Grammar dictation = new DictationGrammar();  
      dictation.Name = "Dictation";  
  
      // Load grammars to the recognizer.  
      recognizer.LoadGrammarAsync(yesnoGrammar);  
      recognizer.LoadGrammarAsync(doneGrammar);  
      recognizer.LoadGrammarAsync(dictation);  
  
      // Keep the console window open.  
      Console.ReadLine();  
    }  
  
    // Put the shared speech recognizer into "listening" mode.  
    static void  recognizer_StateChanged(object sender, StateChangedEventArgs e)  
    {  
     if (e.RecognizerState != RecognizerState.Stopped)  
      {  
        recognizer.EmulateRecognizeAsync("Start listening");  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void  recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
     Console.WriteLine("Grammar({0}): {1}", e.Result.Grammar.Name, e.Result.Text);  
  
      // Add event handler code here.  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void  recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
     string grammarName = e.Grammar.Name;  
      bool grammarLoaded = e.Grammar.Loaded;  
      if (e.Error != null)  
      {  
        Console.WriteLine("LoadGrammar for {0} failed with a {1}.",  
        grammarName, e.Error.GetType().Name);  
      }  
  
      // Add exception handling code here.  
      Console.WriteLine("Grammar {0} {1} loaded.",  
      grammarName, (grammarLoaded) ? "is" : "is not");  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognizerState" />
        <altmember cref="T:System.Speech.Recognition.StateChangedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.State" />
      </Docs>
    </Member>
    <Member MemberName="UnloadAllGrammars">
      <MemberSignature Language="C#" Value="public void UnloadAllGrammars ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UnloadAllGrammars() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars" />
      <MemberSignature Language="VB.NET" Value="Public Sub UnloadAllGrammars ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UnloadAllGrammars();" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Scarica tutte le grammatiche di riconoscimento vocale dal riconoscimento condiviso.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Se il riconoscimento sta caricando una grammatica in modo asincrono, questo metodo attende fino a quando non viene caricata la grammatica, prima che venga scaricato tutte le grammatiche del riconoscimento.  
  
 Per scaricare una grammatica specifica, utilizzare il <xref:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar%2A> metodo.  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.Grammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="UnloadGrammar">
      <MemberSignature Language="C#" Value="public void UnloadGrammar (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UnloadGrammar(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UnloadGrammar(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">La grammatica di cui annullare il caricamento.</param>
        <summary>Scarica una grammatica di riconoscimento vocale specificata dal riconoscimento condiviso.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Se il riconoscimento è in esecuzione, le applicazioni devono utilizzare <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> per sospendere il motore di riconoscimento vocale prima di caricamento, scaricamento, abilitazione o disabilitazione di una grammatica. Per scaricare tutte le grammatiche, utilizzare il <xref:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars%2A> metodo.  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.Grammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars" />
      </Docs>
    </Member>
  </Members>
</Type>